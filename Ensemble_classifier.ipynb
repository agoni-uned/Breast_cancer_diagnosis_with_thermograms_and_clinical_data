{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d8be5-3808-4580-a7c8-23f550a76896",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gc\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a04d0-592b-4343-bb0d-142c5fdf1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the display option for wide dataframes\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8a826-8a7d-41e5-9a6d-821840b6ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set variables\n",
    "N = 10 # experiment repetitions\n",
    "k = 4  # k for k-fold cross-validation in hyperparameter tuning\n",
    "seed = 42  # seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5548cfe-9bc9-457e-b2b1-6ff40b05b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize variables to store tuned hyperparameters and results\n",
    "trials_params, trials_results = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669beeb-8ada-415d-8c37-ae3fca330263",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selected clinical dtaa classifier and multi-view thermal image classifier\n",
    "selected_img_classifier = 'MultiInputCNN'  #'multiInputCNN'\n",
    "selected_cd_classifier = 'RF'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa2859-bcbe-4885-91a1-5165f9c84a9b",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b889c0d-c56b-4434-ae9c-68839c7c19a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read list of selected patients\n",
    "f = open('Data/selected_patients.txt', 'r')\n",
    "patient_IDs = []\n",
    "for x in f:\n",
    "  patient_IDs.append(int(x.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495e16e2-277a-46a4-b859-439dc283ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load clinical data\n",
    "clinical_data = pd.read_csv('Data/clinical_data.csv', header=0, index_col=0, delimiter=';')\n",
    "clinical_data = clinical_data.loc[patient_IDs]  # Sort the clinical data DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79a636-6304-43ad-ba9e-13474ff43eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load images\n",
    "from PIL import Image\n",
    "\n",
    "front = np.asarray([np.array(Image.open(os.path.join('Data/Images/Frontal/',str(ID).zfill(4) + '_front.jpg'))) for ID in patient_IDs])\n",
    "L90 = np.asarray([np.array(Image.open(os.path.join('Data/Images/L90/',str(ID).zfill(4) + '_L90.jpg'))) for ID in patient_IDs])\n",
    "R90 = np.asarray([np.array(Image.open(os.path.join('Data/Images/R90/',str(ID).zfill(4) + '_R90.jpg'))) for ID in patient_IDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32922de9-6d04-438b-a8dc-764a38b3dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate binary labels\n",
    "diagnosis = clinical_data['vx-diagnosis']\n",
    "labels = np.zeros(len(patient_IDs))\n",
    "labels[diagnosis == 'Sick'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0ea71-5699-4c47-b6a6-01b38ca710be",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b8b66-e591-4d15-a44b-4ed33e24d40d",
   "metadata": {},
   "source": [
    "## Clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02cc9c2-a15b-4014-9b75-47eecb64f6fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Select features\n",
    "selected_features = ['age_at_visit', 'vx-PH-Eating habits', 'vx-PH-Cancer family?', 'vx-MH-Radiotherapy', 'vx-MH-Use of hormone replacement?',\n",
    "                     'age_at_menarche', 'age_at_menopause', 'diabetes', 'nodules']\n",
    "clinical_data = clinical_data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ea761-1fd1-4a20-838c-cd63b8aa1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min-max normalization\n",
    "M = clinical_data.max().values\n",
    "M[M<1] = 1\n",
    "m = clinical_data.min().values\n",
    "\n",
    "clinical_data = (clinical_data-m)/(M-m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00358906-3d51-459e-b8d6-4e8b4205360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to numpy array\n",
    "clinical_data = np.asarray(clinical_data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf7214-69d1-48ba-8946-7ab37c293163",
   "metadata": {},
   "source": [
    "## Thermal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26e38d-4b16-40c4-acff-2e680d8ac4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min-max normalization\n",
    "M = np.concatenate((front, L90, R90)).max()\n",
    "m = np.concatenate((front, L90, R90)).min()\n",
    "\n",
    "front = ((front - m) / (M - m)).astype('float32')\n",
    "L90 = ((L90 - m) / (M - m)).astype('float32')\n",
    "R90 = ((R90 - m) / (M - m)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f34035-c3b1-4a87-808d-f8e5070769d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert 3-channel to 1-channel images\n",
    "front = np.expand_dims(front[:,:,:,0], -1)\n",
    "L90 = np.expand_dims(L90[:,:,:,0], -1)\n",
    "R90 = np.expand_dims(R90[:,:,:,0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f736e79-8dd2-47f6-8025-0d44ec094001",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf0fa5-ecde-4ecc-838f-3ed3d461fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_error(y_test, y_test_pred):\n",
    "    WE = 20\n",
    "\n",
    "    # Number of elements for which y_test > y_test_pred, i.e., y_test = 1 and y_test_pred = 0 (FN)\n",
    "    fn = np.sum(np.greater(y_test, y_test_pred))\n",
    "\n",
    "    # Number of elements for which y_test < y_test_pred, i.e., y_test = 0 and y_test_pred = 1 (FP)\n",
    "    fp = np.sum(np.less(y_test, y_test_pred))\n",
    "\n",
    "    return fn*WE + fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711306ba-7f62-4ae5-bd94-e33ed3d0f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(preds_raw, ground_truth):\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, log_loss, roc_auc_score #, accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "    \n",
    "    # BCE loss\n",
    "    bce = log_loss(ground_truth, preds_raw)\n",
    "\n",
    "    # Confusion matrix\n",
    "    TN, FP, FN, TP = confusion_matrix(ground_truth, np.round(preds_raw)).ravel()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)  # accuracy = accuracy_score(ground_truth, np.round(preds_raw))\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    #gmean = np.sqrt((TP/(TP+FN))*(TN/(TN+FP)))\n",
    "    precision = TP / (TP + FP)  # precision = precision_score(ground_truth, np.round(preds_raw))\n",
    "    F1 = 2 * TP / (2*TP + FP + FN)  # F1 = f1_score(ground_truth, np.round(preds_raw))\n",
    "    \n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(ground_truth, preds_raw)\n",
    "    \n",
    "    # Weighted error    \n",
    "    we = weighted_error(ground_truth, np.round(preds_raw))\n",
    "    \n",
    "    results = {\n",
    "        'BCELoss':bce,\n",
    "        'Accuracy':accuracy,\n",
    "        'TP':TP,\n",
    "        'FP':FP,\n",
    "        'TN':TN,\n",
    "        'FN':FN,\n",
    "        'Sensitivity':sensitivity,\n",
    "        'Specificity':specificity,\n",
    "        #'G-mean':gmean,\n",
    "        'Precision':precision,\n",
    "        'Recall':sensitivity,\n",
    "        'F1':F1,\n",
    "        'ROC_AUC':auc,\n",
    "        'WE':we\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada018b5-8044-459a-8433-43c36ca5b14d",
   "metadata": {},
   "source": [
    "# Weighted Voting (WV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b7a3d-e16e-4ded-a04b-703aff39cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_voting(pred_cd, pred_img, weight = 0.5):\n",
    "    return pred_cd * weight + pred_img * (1 - weight)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "def tune_weight(pred_cd, pred_img, labels, metric=None):\n",
    "    best_weight = 0\n",
    "    best_metric = 0\n",
    "    if metric == \"we\":\n",
    "        best_metric = np.inf\n",
    "            \n",
    "    for weight_cd in range(0, 10000, 1):\n",
    "        weight = weight_cd/10000\n",
    "                \n",
    "        new_pred = weighted_voting(pred_cd, pred_img, weight)\n",
    "        \n",
    "        if metric == \"accuracy\":\n",
    "            accuracy = accuracy_score(labels, np.round(new_pred))\n",
    "            if accuracy > best_metric:\n",
    "                best_metric = accuracy\n",
    "                best_weight = weight\n",
    "        elif metric == \"roc_auc\":\n",
    "            roc_auc = roc_auc_score(labels, new_pred)\n",
    "            if roc_auc > best_metric:\n",
    "                best_metric = roc_auc\n",
    "                best_weight = weight\n",
    "        elif metric == \"we\":\n",
    "            we = weighted_error(labels, np.round(new_pred))\n",
    "            if we < best_metric:\n",
    "                best_metric = we\n",
    "                best_weight = weight\n",
    "        else:\n",
    "            tune_weight(pred_cd, pred_img, labels, \"accuracy\")\n",
    "            tune_weight(pred_cd, pred_img, labels, \"roc_auc\")\n",
    "            tune_weight(pred_cd, pred_img, labels, \"we\")\n",
    "            break\n",
    "            \n",
    "    print(f'Best {metric}: {best_metric} | Best weight: {best_weight}')\n",
    "    return best_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f6c38-4f54-473b-9256-bfeef02fa972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the splitter to split the dataset into 85% train and 15% tests sets N times \n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95499901-f9f4-46ce-b727-2a8dda535fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial, (train_index, test_index) in enumerate(splitter.split(clinical_data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset into train (85%) and test (15%) sets\n",
    "    crossval_cd = clinical_data[train_index]\n",
    "    crossval_front = front[train_index]\n",
    "    crossval_L90 = L90[train_index]\n",
    "    crossval_R90 = R90[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    \n",
    "    test_cd = clinical_data[test_index]\n",
    "    test_front = front[test_index]\n",
    "    test_L90 = L90[test_index]\n",
    "    test_R90 = R90[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Calculate the weight for training to address class imbalance\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    ## Load selected clinical data classifier and image classifier\n",
    "    # Clinical data classifier\n",
    "    with open('Models/'+selected_cd_classifier+'_'+str(trial+1)+'.pkl','rb') as f:\n",
    "        cd_classifier = pickle.load(f)\n",
    "\n",
    "    # Milti-view thermal image classifier\n",
    "    img_classifier = load_model('Models/'+selected_img_classifier+'_'+str(trial+1)+'.h5')\n",
    "\n",
    "    ## Get predictions (inputs)\n",
    "    pred_cd_crossval = cd_classifier.predict_proba(crossval_cd)[:,1]\n",
    "    pred_img_crossval = img_classifier.predict([crossval_front, crossval_L90, crossval_R90])\n",
    "\n",
    "    pred_cd_test = cd_classifier.predict_proba(test_cd)[:,1]\n",
    "    pred_img_test = img_classifier.predict([test_front, test_L90, test_R90])\n",
    "\n",
    "    \n",
    "    ## Tune the weight\n",
    "    ti = time.time()\n",
    "    weight = tune_weight(pred_cd_crossval, pred_img_crossval, crossval_labels, metric='roc_auc')\n",
    "    train_time = time.time() - ti\n",
    "    hours, remainder = divmod(train_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Training took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    ## Store tuned hyperparameters\n",
    "    trials_params.append({'classifier':'WV', 'trial':trial+1, 'weight':weight})\n",
    "\n",
    "    ## Predict\n",
    "    predictions_train = weighted_voting(pred_cd_crossval, pred_img_crossval, weight)\n",
    "    np.save('Predictions/Ensemble_WV_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "    predictions_test = weighted_voting(pred_cd_test, pred_img_test, weight)\n",
    "    np.save('Predictions/Ensemble_WV_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "        \n",
    "    ## Evaluate the model\n",
    "    results_train = evaluate_model(predictions_train, crossval_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "            \n",
    "    results_test = evaluate_model(predictions_test, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append(\n",
    "        {'classifier':'WV',\n",
    "         'trial':trial+1,\n",
    "         'parameters':1,\n",
    "         'trainTime':train_time,\n",
    "         **{'train_'+k:v for k,v in results_train.items()},\n",
    "         **{'test_'+k:v for k,v in results_test.items()}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Clear cache and TensorFlow session to release GPU memory\n",
    "    clear_session()\n",
    "    del img_classifier, cd_classifier, crossval_cd, crossval_front, crossval_L90, crossval_R90, crossval_labels, test_cd, test_front, test_L90, test_R90, test_labels  # Delete the references to objects\n",
    "    del pred_cd_crossval, pred_img_crossval, pred_cd_test, pred_img_test, predictions_train, predictions_test\n",
    "    gc.collect()  # Manually invoke garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "    print(), print(100*'#'), print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9274dd-17b3-49a9-9721-ee4f017a33d5",
   "metadata": {},
   "source": [
    "# Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587166e-d8ed-450c-8384-6f4e50255277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39074821-85c9-4c40-bc9b-4828b89d7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the splitter to split the dataset into 85% train and 15% tests sets N times \n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84323e2f-6c03-4e2c-b859-b5f3027449eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial, (train_index, test_index) in enumerate(splitter.split(clinical_data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset into train (85%) and test (15%) sets\n",
    "    crossval_cd = clinical_data[train_index]\n",
    "    crossval_front = front[train_index]\n",
    "    crossval_L90 = L90[train_index]\n",
    "    crossval_R90 = R90[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    \n",
    "    test_cd = clinical_data[test_index]\n",
    "    test_front = front[test_index]\n",
    "    test_L90 = L90[test_index]\n",
    "    test_R90 = R90[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Calculate the weight for training to address class imbalance\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    ## Load selected clinical data classifier and image classifier\n",
    "    # Clinical data classifier\n",
    "    with open('Models/'+selected_cd_classifier+'_'+str(trial+1)+'.pkl','rb') as f:\n",
    "        cd_classifier = pickle.load(f)\n",
    "\n",
    "    # Milti-view thermal image classifier\n",
    "    img_classifier = load_model('Models/'+selected_img_classifier+'_'+str(trial+1)+'.h5')\n",
    "\n",
    "    ## Get predictions (inputs)\n",
    "    pred_cd_crossval = cd_classifier.predict_proba(crossval_cd)[:,1]\n",
    "    pred_img_crossval = img_classifier.predict([crossval_front, crossval_L90, crossval_R90])\n",
    "    inputs_crossval = np.concatenate((pred_img_crossval.reshape(-1, 1),pred_cd_crossval.reshape(-1, 1)), axis=1)  # Concatenate predictions to generate the input vector\n",
    "\n",
    "    pred_cd_test = cd_classifier.predict_proba(test_cd)[:,1]\n",
    "    pred_img_test = img_classifier.predict([test_front, test_L90, test_R90])\n",
    "    inputs_test = np.concatenate((pred_img_test.reshape(-1, 1),pred_cd_test.reshape(-1, 1)), axis=1)  # Concatenate predictions to generate the input vector\n",
    "\n",
    "\n",
    "    ## Tune hyperparameters with k-fold cross-validation and then train on the entire training set with the tuned hyperparameters\n",
    "    ti = time.time()\n",
    "    estimator = DecisionTreeClassifier(class_weight={0: 1, 1: rate_train})\n",
    "    param_grid = {'ccp_alpha' : np.arange(0, 0.1, 0.01), # Complexity parameter used for Minimal Cost-Complexity Pruning. Default: ccp_alpha = 0.0\n",
    "                  'criterion': ['gini','entropy'],#'log_loss'], # The function to measure the quality of a split. Default: criterion='gini'\n",
    "                  'max_depth' : [None, 1, 5, 10, 15], # The maximum depth of the tree. Default: max_depth=None\n",
    "                  #'max_features': [None, 'sqrt', 'log2'], # The number of features to consider when looking for the best split. Default: max_features=None\n",
    "                  'max_leaf_nodes': [None, 3, 6, 9], # Grow a tree with max_leaf_nodes in best-first fashion. Default: None\n",
    "                  'min_samples_leaf': [1, 2, 3, 4], # The minimum number of samples required to be at a leaf node. Default: min_samples_leaf=1\n",
    "                  'min_samples_split' : [2, 5, 10, 15], # The minimum number of samples required to split an internal node. Default: min_samples_split=2\n",
    "                  'min_weight_fraction_leaf' : np.arange(0.0, 0.5, 0.05), # The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Default: min_weight_fraction_leaf=0\n",
    "                  #'splitter': ['best','random'] # The strategy used to choose the split at each node. Default: splitter='best'\n",
    "                }\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring='roc_auc', cv=k)\n",
    "    classifier.fit(inputs_crossval, crossval_labels)\n",
    "\n",
    "    parameters = classifier.best_params_  # Parameter setting that gave the best results on the hold out data\n",
    "    classifier = classifier.best_estimator_  # Estimator which gave highest score on the left out data\n",
    "\n",
    "    train_time = time.time() - ti\n",
    "    hours, remainder = divmod(train_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    ## Store tuned hyperparameters\n",
    "    trials_params.append({**{'classifier':'DT'}, **{'trial':trial+1}, **parameters})\n",
    "\n",
    "    ## Save the trained model\n",
    "    with open('Models/Ensemble_DT_'+str(trial+1)+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "\n",
    "\n",
    "    ## Predict\n",
    "    predictions_train = classifier.predict_proba(inputs_crossval)[:,1]\n",
    "    np.save('Predictions/Ensemble_DT_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "    predictions_test = classifier.predict_proba(inputs_test)[:,1]\n",
    "    np.save('Predictions/Ensemble_DT_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "    ## Print the number of parameters in the model\n",
    "    num_params = classifier.tree_.node_count\n",
    "    print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "    ## Evaluate the model\n",
    "    results_train = evaluate_model(predictions_train, crossval_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "            \n",
    "    results_test = evaluate_model(predictions_test, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append(\n",
    "        {'classifier':'DT',\n",
    "         'trial':trial+1,\n",
    "         'parameters':num_params,\n",
    "         'trainTime':train_time,\n",
    "         **{'train_'+k:v for k,v in results_train.items()},\n",
    "         **{'test_'+k:v for k,v in results_test.items()}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Clear cache and TensorFlow session to release GPU memory\n",
    "    clear_session()\n",
    "    del img_classifier, cd_classifier, crossval_cd, crossval_front, crossval_L90, crossval_R90, crossval_labels, test_cd, test_front, test_L90, test_R90, test_labels  # Delete the references to objects\n",
    "    del pred_cd_crossval, pred_img_crossval, pred_cd_test, pred_img_test, predictions_train, predictions_test\n",
    "    gc.collect()  # Manually invoke garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "    print(), print(100*'#'), print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74fbd03-8626-4935-bd67-2d231ee7c919",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b0db1-45f8-4a7b-acdc-a9b69bb2355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2b01e-63d0-4ea6-aac8-7f878f89d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the splitter to split the dataset into 85% train and 15% tests sets N times \n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28cb01-54c8-426e-a660-8008b27224a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial, (train_index, test_index) in enumerate(splitter.split(clinical_data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset into train (85%) and test (15%) sets\n",
    "    crossval_cd = clinical_data[train_index]\n",
    "    crossval_front = front[train_index]\n",
    "    crossval_L90 = L90[train_index]\n",
    "    crossval_R90 = R90[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    \n",
    "    test_cd = clinical_data[test_index]\n",
    "    test_front = front[test_index]\n",
    "    test_L90 = L90[test_index]\n",
    "    test_R90 = R90[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Calculate the weight for training to address class imbalance\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    ## Load selected clinical data classifier and image classifier\n",
    "    # Clinical data classifier\n",
    "    with open('Models/'+selected_cd_classifier+'_'+str(trial+1)+'.pkl','rb') as f:\n",
    "        cd_classifier = pickle.load(f)\n",
    "\n",
    "    # Milti-view thermal image classifier\n",
    "    img_classifier = load_model('Models/'+selected_img_classifier+'_'+str(trial+1)+'.h5')\n",
    "\n",
    "    ## Get predictions (inputs)\n",
    "    pred_cd_crossval = cd_classifier.predict_proba(crossval_cd)[:,1]\n",
    "    pred_img_crossval = img_classifier.predict([crossval_front, crossval_L90, crossval_R90])\n",
    "    inputs_crossval = np.concatenate((pred_img_crossval.reshape(-1, 1),pred_cd_crossval.reshape(-1, 1)), axis=1)  # Concatenate predictions to generate the input vector\n",
    "\n",
    "    pred_cd_test = cd_classifier.predict_proba(test_cd)[:,1]\n",
    "    pred_img_test = img_classifier.predict([test_front, test_L90, test_R90])\n",
    "    inputs_test = np.concatenate((pred_img_test.reshape(-1, 1),pred_cd_test.reshape(-1, 1)), axis=1)  # Concatenate predictions to generate the input vector\n",
    "\n",
    "    \n",
    "    ## Tune hyperparameters with k-fold cross-validation and then train on the entire training set with the tuned hyperparameters\n",
    "    ti = time.time()\n",
    "    estimator = SVC(class_weight={0: 1, 1: rate_train}, probability=True)\n",
    "    param_grid = {'C': [1,10,100,1000], # Regularization parameter. Default: C=1.0\n",
    "                  'kernel': ['linear', 'rbf', 'sigmoid', 'poly'], # Default: kernel='rbf'\n",
    "                  'gamma': ['scale', 'auto', 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001] # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Default: gamma='scale'\n",
    "                 }\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring='roc_auc', cv=k)\n",
    "    classifier.fit(inputs_crossval, crossval_labels)\n",
    "\n",
    "    parameters = classifier.best_params_  # Parameter setting that gave the best results on the hold out data\n",
    "    classifier = classifier.best_estimator_  # Estimator which gave highest score on the left out data\n",
    "\n",
    "\n",
    "    train_time = time.time() - ti\n",
    "    hours, remainder = divmod(train_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    ## Store tuned hyperparameters\n",
    "    trials_params.append({**{'classifier':'SVM'}, **{'trial':trial+1}, **parameters})\n",
    "\n",
    "    ## Save the trained model\n",
    "    with open('Models/Ensemble_SVM_'+str(trial+1)+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "\n",
    "\n",
    "    ## Predict\n",
    "    predictions_train = classifier.predict_proba(inputs_crossval)[:,1]\n",
    "    np.save('Predictions/Ensemble_SVM_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "    predictions_test = classifier.predict_proba(inputs_test)[:,1]\n",
    "    np.save('Predictions/Ensemble_SVM_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "    ## Print the number of parameters in the model\n",
    "    n_support_vectors = len(classifier.support_vectors_)\n",
    "    n_coefficients = len(classifier.dual_coef_[0])\n",
    "    num_params = n_support_vectors + n_coefficients\n",
    "    print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "    ## Evaluate the model\n",
    "    results_train = evaluate_model(predictions_train, crossval_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "            \n",
    "    results_test = evaluate_model(predictions_test, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append(\n",
    "        {'classifier':'SVM',\n",
    "         'trial':trial+1,\n",
    "         'parameters':num_params,\n",
    "         'trainTime':train_time,\n",
    "         **{'train_'+k:v for k,v in results_train.items()},\n",
    "         **{'test_'+k:v for k,v in results_test.items()}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Clear cache and TensorFlow session to release GPU memory\n",
    "    clear_session()\n",
    "    del img_classifier, cd_classifier, crossval_cd, crossval_front, crossval_L90, crossval_R90, crossval_labels, test_cd, test_front, test_L90, test_R90, test_labels  # Delete the references to objects\n",
    "    del pred_cd_crossval, pred_img_crossval, pred_cd_test, pred_img_test, predictions_train, predictions_test\n",
    "    gc.collect()  # Manually invoke garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "    print(), print(100*'#'), print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401a5ce-5a7a-40a8-91bb-78f9cf4b71c9",
   "metadata": {},
   "source": [
    "# Neural Network (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f799f9-ad9f-4057-81b4-9ed29bb7cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde217e-d36a-4aea-9563-9a85100c1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the splitter to split the dataset into 85% train and 15% tests sets N times \n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6960d13-b44e-40f9-b6b6-69f8fce1df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial, (train_index, test_index) in enumerate(splitter.split(clinical_data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset into train (85%) and test (15%) sets\n",
    "    crossval_cd = clinical_data[train_index]\n",
    "    crossval_front = front[train_index]\n",
    "    crossval_L90 = L90[train_index]\n",
    "    crossval_R90 = R90[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    \n",
    "    test_cd = clinical_data[test_index]\n",
    "    test_front = front[test_index]\n",
    "    test_L90 = L90[test_index]\n",
    "    test_R90 = R90[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Calculate the weight for training to address class imbalance\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    ## Load selected clinical data classifier and image classifier\n",
    "    # Clinical data classifier\n",
    "    with open('Models/'+selected_cd_classifier+'_'+str(trial+1)+'.pkl','rb') as f:\n",
    "        cd_classifier = pickle.load(f)\n",
    "\n",
    "    # Milti-view thermal image classifier\n",
    "    img_classifier = load_model('Models/'+selected_img_classifier+'_'+str(trial+1)+'.h5')\n",
    "\n",
    "    ## Get predictions (inputs)\n",
    "    pred_cd_crossval = cd_classifier.predict_proba(crossval_cd)[:,1]\n",
    "    pred_img_crossval = img_classifier.predict([crossval_front, crossval_L90, crossval_R90])\n",
    "    inputs_crossval = np.concatenate((pred_img_crossval.reshape(-1, 1),pred_cd_crossval.reshape(-1, 1)), axis=1)  # Concatenate predictions to generate the input vector\n",
    "\n",
    "    pred_cd_test = cd_classifier.predict_proba(test_cd)[:,1]\n",
    "    pred_img_test = img_classifier.predict([test_front, test_L90, test_R90])\n",
    "    inputs_test = np.concatenate((pred_img_test.reshape(-1, 1),pred_cd_test.reshape(-1, 1)), axis=1)  # Concatenate predictions to generate the input vector\n",
    "\n",
    "\n",
    "    ## Tune hyperparameters with k-fold cross-validation and then train on the entire training set with the tuned hyperparameters\n",
    "    ti = time.time()\n",
    "    estimator = Perceptron(class_weight={0: 1, 1: rate_train})\n",
    "    param_grid = {'penalty': [None, 'l2', 'l1', 'elasticnet'], # The penalty (aka regularization term). Default: penalty=None\n",
    "                  'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1], # Constant that multiplies the regularization term if regularization is used. Default: alpha=0.0001\n",
    "                  'fit_intercept': [True, False], # Whether the intercept should be estimated or not. Default: fit_intercept=True\n",
    "                  'max_iter': [500, 1000, 1500, 2000], # The maximum number of passes over the training data (aka epochs). Default: max_iter=1000\n",
    "                  'tol': [None, 1e-2, 1e-3, 1e-4, 1e-5], # The stopping criterion. Default: tol=1e-3\n",
    "                  #'shuffle': [True, False], # Whether or not the training data should be shuffled after each epoch. Default: shuffle=True\n",
    "                  'eta0': [10, 1, 0.1, 0.01, 0.001], # Constant by which the updates are multiplied. Default: eta0=1\n",
    "                  'validation_fraction': [0.01, 0.1, 0.2, 0.3] # The proportion of training data to set aside as validation set for early stopping. Default: validation_fraction=0.1\n",
    "                }\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring='roc_auc', cv=k)\n",
    "    classifier.fit(inputs_crossval, crossval_labels)\n",
    "\n",
    "    parameters = classifier.best_params_  # Parameter setting that gave the best results on the hold out data\n",
    "    classifier = classifier.best_estimator_  # Estimator which gave highest score on the left out data\n",
    "\n",
    "    train_time = time.time() - ti\n",
    "    hours, remainder = divmod(train_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    ## Store tuned hyperparameters\n",
    "    trials_params.append({**{'classifier':'NN'}, **{'trial':trial+1}, **parameters})\n",
    "\n",
    "    ## Save the trained model\n",
    "    with open('Models/Ensemble_NN_'+str(trial+1)+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "\n",
    "\n",
    "    ## Predict\n",
    "    predictions_train = classifier.predict(inputs_crossval)\n",
    "    np.save('Predictions/Ensemble_NN_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "    predictions_test = classifier.predict(inputs_test)\n",
    "    np.save('Predictions/Ensemble_NN_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "    ## Print the number of parameters in the model\n",
    "    num_params = classifier.coef_.size + classifier.intercept_.size\n",
    "    print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "    ## Evaluate the model\n",
    "    results_train = evaluate_model(predictions_train, crossval_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "            \n",
    "    results_test = evaluate_model(predictions_test, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append(\n",
    "        {'classifier':'NN',\n",
    "         'trial':trial+1,\n",
    "         'parameters':num_params,\n",
    "         'trainTime':train_time,\n",
    "         **{'train_'+k:v for k,v in results_train.items()},\n",
    "         **{'test_'+k:v for k,v in results_test.items()}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Clear cache and TensorFlow session to release GPU memory\n",
    "    clear_session()\n",
    "    del img_classifier, cd_classifier, crossval_cd, crossval_front, crossval_L90, crossval_R90, crossval_labels, test_cd, test_front, test_L90, test_R90, test_labels  # Delete the references to objects\n",
    "    del pred_cd_crossval, pred_img_crossval, pred_cd_test, pred_img_test, predictions_train, predictions_test\n",
    "    gc.collect()  # Manually invoke garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "    print(), print(100*'#'), print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec1223-f0bc-45bf-a993-5b301472f299",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b822c588-d8f1-4e1a-96a8-b0758b487e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29f30f-ddc1-4411-b960-2bde3ad14303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the splitter to split the dataset into 85% train and 15% tests sets N times \n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823b1ad-e9d4-4f65-89ee-928f0f301f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for trial, (train_index, test_index) in enumerate(splitter.split(clinical_data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset into train (85%) and test (15%) sets\n",
    "    crossval_cd = clinical_data[train_index]\n",
    "    crossval_front = front[train_index]\n",
    "    crossval_L90 = L90[train_index]\n",
    "    crossval_R90 = R90[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    \n",
    "    test_cd = clinical_data[test_index]\n",
    "    test_front = front[test_index]\n",
    "    test_L90 = L90[test_index]\n",
    "    test_R90 = R90[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Calculate the weight for training to address class imbalance\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    ## Load selected clinical data classifier and image classifier\n",
    "    # Clinical data classifier\n",
    "    with open('Models/'+selected_cd_classifier+'_'+str(trial+1)+'.pkl','rb') as f:\n",
    "        cd_classifier = pickle.load(f)\n",
    "\n",
    "    # Milti-view thermal image classifier\n",
    "    img_classifier = load_model('Models/'+selected_img_classifier+'_'+str(trial+1)+'.h5')\n",
    "\n",
    "    ## Get predictions (inputs)\n",
    "    pred_cd_crossval = cd_classifier.predict_proba(crossval_cd)[:,1]\n",
    "    pred_img_crossval = img_classifier.predict([crossval_front, crossval_L90, crossval_R90])\n",
    "    inputs_crossval = np.concatenate((pred_img_crossval.reshape(-1, 1),pred_cd_crossval.reshape(-1, 1)), axis=1)  # Concatenate predictions to generate the input vector\n",
    "\n",
    "    pred_cd_test = cd_classifier.predict_proba(test_cd)[:,1]\n",
    "    pred_img_test = img_classifier.predict([test_front, test_L90, test_R90])\n",
    "    inputs_test = np.concatenate((pred_img_test.reshape(-1, 1),pred_cd_test.reshape(-1, 1)), axis=1)  # Concatenate predictions to generate the input vector\n",
    "\n",
    "\n",
    "    ## Tune hyperparameters with k-fold cross-validation and then train on the entire training set with the tuned hyperparameters\n",
    "    ti = time.time()\n",
    "    estimator = SGDClassifier(class_weight={0: 1, 1: rate_train})\n",
    "    param_grid = {'loss': ['log_loss', 'modified_huber'], # The loss function to be used. Default: loss='hinge'. Options: ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron', ‘squared_error’, ‘huber’, ‘epsilon_insensitive’, ‘squared_epsilon_insensitive’]\n",
    "                  'penalty': [None, 'l2', 'l1', 'elasticnet'], # The penalty (aka regularization term) to be used. Default: penalty='l2'\n",
    "                  'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1], # Constant that multiplies the regularization term. Default: alpha=0.0001\n",
    "                  'l1_ratio': [0.05, 0.15, 0.3, 0.5, 0.7, 0.9],  # The Elastic Net mixing parameter, only used if penalty is 'elasticnet'. Default: l1_ratio=0.15\n",
    "                  'max_iter': [500, 1000, 1500, 2000], # The maximum number of passes over the training data (aka epochs). Default: max_iter=1000\n",
    "                  'tol': [None, 1e-2, 1e-3, 1e-4, 1e-5], # The stopping criterion. Default: tol=1e-3\n",
    "                  'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'], # The learning rate schedule. Default: learning_rate='optimal'\n",
    "                  'eta0':  [0.001, 0.01, 0.1, 1, 10]  # The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. Default: eta0=0.0\n",
    "                }\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring='roc_auc', cv=k)\n",
    "    classifier.fit(inputs_crossval, crossval_labels)\n",
    "\n",
    "    parameters = classifier.best_params_  # Parameter setting that gave the best results on the hold out data\n",
    "    classifier = classifier.best_estimator_  # Estimator which gave highest score on the left out data\n",
    "\n",
    "    train_time = time.time() - ti\n",
    "    hours, remainder = divmod(train_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    ## Store tuned hyperparameters\n",
    "    trials_params.append({**{'classifier':'SGD'}, **{'trial':trial+1}, **parameters})\n",
    "\n",
    "    ## Save the trained model\n",
    "    with open('Models/Ensemble_SGD_'+str(trial+1)+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "\n",
    "\n",
    "    ## Predict\n",
    "    predictions_train = classifier.predict_proba(inputs_crossval)[:,1]\n",
    "    np.save('Predictions/Ensemble_SGD_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "    predictions_test = classifier.predict_proba(inputs_test)[:,1]\n",
    "    np.save('Predictions/Ensemble_SGD_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "    ## Print the number of parameters in the model\n",
    "    num_params = classifier.coef_.size + classifier.intercept_.size\n",
    "    print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "    ## Evaluate the model\n",
    "    results_train = evaluate_model(predictions_train, crossval_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "            \n",
    "    results_test = evaluate_model(predictions_test, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append(\n",
    "        {'classifier':'SGD',\n",
    "         'trial':trial+1,\n",
    "         'parameters':num_params,\n",
    "         'trainTime':train_time,\n",
    "         **{'train_'+k:v for k,v in results_train.items()},\n",
    "         **{'test_'+k:v for k,v in results_test.items()}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Clear cache and TensorFlow session to release GPU memory\n",
    "    clear_session()\n",
    "    del img_classifier, cd_classifier, crossval_cd, crossval_front, crossval_L90, crossval_R90, crossval_labels, test_cd, test_front, test_L90, test_R90, test_labels  # Delete the references to objects\n",
    "    del pred_cd_crossval, pred_img_crossval, pred_cd_test, pred_img_test, predictions_train, predictions_test\n",
    "    gc.collect()  # Manually invoke garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache\n",
    "\n",
    "    print(), print(100*'#'), print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd425159-d70f-4f3c-b653-bab38bf37c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd26bf-e574-4f05-9c9c-ac0d74b73bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store tuned hyperparameters and results\n",
    "pd.DataFrame(trials_params).to_csv('Ensemble_Parameters_'+str(N)+'trials.csv')\n",
    "pd.DataFrame(trials_results).round(decimals=5).to_csv('Ensemble_Results_'+str(N)+'trials.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd390d-d357-483a-bd73-d9f9021c7c9c",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7913645-fe6a-4e8f-ae72-cc025193cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_results = pd.DataFrame(trials_results).fillna(1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94552820-c059-4ec9-825a-9a00d3a5c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print statistics\n",
    "models = trials_results.classifier.unique()\n",
    "metrics = [c for c in trials_results.columns if 'test_' in c and c not in ['test_TP','test_FP','test_TN','test_FN']]\n",
    "    \n",
    "statistics = pd.DataFrame(index=models, columns=[item for sublist in [[metric+'_mean', metric+'_std'] for metric in metrics] for item in sublist])\n",
    "    \n",
    "for metric in metrics:\n",
    "    mn, st = metric+'_mean', metric+'_std'\n",
    "    for model in models:\n",
    "        results = trials_results[trials_results['classifier']==model][metric].values\n",
    "        statistics.at[model,mn] = results.mean()\n",
    "        statistics.at[model,st] = results.std()\n",
    "\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e1da9-d193-488e-a5cf-ed360f1a23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [m for m in metrics if 'test' in m]:\n",
    "    mn, st = metric+'_mean', metric+'_std'\n",
    "    if 'Loss' in metric or 'WE' in metric:\n",
    "        model_best = pd.to_numeric(statistics[metric+'_mean']).idxmin()\n",
    "        print(f'Model with lowest {metric} is {model_best} with value {statistics.loc[model_best,mn]} and standard deviation {statistics.loc[model_best,st]}')\n",
    "    else:\n",
    "        model_best = pd.to_numeric(statistics[metric+'_mean']).idxmax()\n",
    "        print(f'Model with highest {metric} is {model_best} with value {statistics.loc[model_best,mn]} and standard deviation {statistics.loc[model_best,st]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336097e-a85a-44ce-9c58-23f54adb6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print mean and std metrics for each model\n",
    "for classifier_type in trials_results.classifier.unique():\n",
    "    print(f'Classifier: {classifier_type}')\n",
    "    results = trials_results[trials_results['classifier'] == classifier_type]\n",
    "\n",
    "    # Number of parameters\n",
    "    parameters = results['parameters'].values\n",
    "    print(f'Mean number of parameters: {parameters.mean()} [{parameters.min()}, {parameters.max()}], std {parameters.std()}')\n",
    "\n",
    "    # training time\n",
    "    trainTime = results['trainTime'].values\n",
    "    hours, remainder = divmod(trainTime.mean(), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Mean training time: {hours} hours, {minutes} minutes, and {seconds} seconds, (std {trainTime.std()} sec)')\n",
    "    print()\n",
    "    \n",
    "    # TRAIN results\n",
    "    metrics = ['BCELoss','Accuracy','Sensitivity','Specificity','ROC_AUC','Precision','F1','WE']\n",
    "    for metric in metrics:\n",
    "        values = results['train_' + metric].values\n",
    "        print(f'Mean train {metric}: {values.mean()}, std {values.std()}')\n",
    "    print()\n",
    "\n",
    "    # TEST results\n",
    "    for metric in metrics:\n",
    "        values = results['test_' + metric].values\n",
    "        print(f'Mean test {metric}: {values.mean()}, std {values.std()}')\n",
    "    print()\n",
    "\n",
    "    print('-'*120), print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94db3a-059d-4f69-bfe0-7afad1c997f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show boxplots\n",
    "compared_metrics = ['test_BCELoss','test_Accuracy','test_F1','test_ROC_AUC','test_WE']  #[c for c in cd_trials_results.columns if 'test_' in c and c not in ['test_TP','test_FP','test_TN','test_FN','test_WE','test_Loss']]\n",
    "M = len(compared_metrics)\n",
    "R = M//2 + int(M % 2 > 0)\n",
    "plt.figure(figsize=(5*R, 20))\n",
    "for i,metric in enumerate(compared_metrics):\n",
    "    plt.subplot(R,2,i+1)\n",
    "    sns.boxplot(x='classifier', y=metric, data=trials_results, palette='Set2')\n",
    "    plt.xticks(rotation=25, ha='right')  # Rotate labels 25 degrees\n",
    "    plt.grid(axis='y')\n",
    "    plt.xlabel('')\n",
    "    if 'Loss' in metric or 'WE' in metric:\n",
    "        plt.ylim([0,trials_results[metric].values.max()+0.05*trials_results[metric].values.max()])\n",
    "    else:\n",
    "        plt.ylim([0,1])\n",
    "    plt.title(metric, fontsize=16)\n",
    "    #plt.tight_layout()\n",
    "    \n",
    "plt.savefig('Ensemble_Boxplots_allModels.png')\n",
    "plt.subplots_adjust(hspace=0.3)  # Increase hspace for more vertical spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e57f7c-63c6-4502-86d4-b30d577da0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistical model comparison\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import probplot\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "    \n",
    "models = trials_results.classifier.unique()\n",
    "compared_metrics = ['test_BCELoss','test_Accuracy','test_F1','test_ROC_AUC','test_WE']\n",
    "    \n",
    "## 1. Select independent metrics\n",
    "#These should be independent. Use the correlation to find dependent variables to remove, if necessary \n",
    "print('Check that the metrics are independent:')\n",
    "print(trials_results[compared_metrics].corr()), print()  # Check for multicollinearity using the correlation matrix\n",
    "\n",
    "var = trials_results[compared_metrics].var().to_numpy()\n",
    "compared_metrics = [metric for metric,v in zip(compared_metrics,var) if not v == 0]  # Remove metrics with zero variance across repetitions\n",
    "\n",
    "## 2. Check for Normality\n",
    "normality = np.ones(len(compared_metrics))\n",
    "L = len(models)//5 + (len(models) % 5 > 0)\n",
    "for i,metric in enumerate(compared_metrics): \n",
    "    print(metric)\n",
    "    ## Check for normality\n",
    "    plt.figure(figsize=(20,2*L))\n",
    "    for j,model in enumerate(models):\n",
    "        # Shapiro-Wilk test\n",
    "        stat, p_value = shapiro(trials_results[trials_results['classifier']==model][metric])\n",
    "        print(f'{model}: Shapiro-Wilk p-value={p_value}')\n",
    "        # If p-value < 0.05, reject normality (non-normal distribution)\n",
    "        normality[i] *= int(p_value > 0.05)\n",
    "\n",
    "        # Generate Q-Q plots to visually inspect normality \n",
    "        plt.subplot(L,5,j+1)\n",
    "        probplot(trials_results[trials_results['classifier']==model][metric], dist='norm', plot=plt)\n",
    "        plt.title(model)\n",
    "    plt.show()\n",
    "    print(f'\\nNormality test for {metric}: {bool(normality[i])}')\n",
    "    print(100*'-')\n",
    "print(), print()\n",
    "    \n",
    "    \n",
    "## 3. Statistical test: MANOVA\n",
    "dependent_variables = ' + '.join(compared_metrics)\n",
    "formula = f'{dependent_variables} ~ classifier'\n",
    "manova = MANOVA.from_formula(formula, data=trials_results)\n",
    "manova_test = manova.mv_test()\n",
    "print(manova_test)\n",
    "\n",
    "## 4. Post-hoc test: Tukey's HSD\n",
    "if manova_test.results['classifier']['stat']['Pr > F']['Pillai\\'s trace'] < 0.05:\n",
    "    for metric in compared_metrics:\n",
    "        print(metric)\n",
    "        tukey = pairwise_tukeyhsd(trials_results[metric],   # Metric data\n",
    "                                          trials_results['classifier'],   # Grouping variable (model)\n",
    "                                          alpha=0.05)   # Significance level\n",
    "        print(tukey)\n",
    "else:\n",
    "    print(f\"MANOVA is not significant, no need to apply Tukey's test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05437e-bfa0-42ef-bc31-bff36888d1a9",
   "metadata": {},
   "source": [
    "# Selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b5cc1-b723-492e-8eae-55af7b14214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'WV'\n",
    "results_selected = trials_results[trials_results['classifier']==selected_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4b2c0-a989-4841-8200-db45b79e7554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Show boxplots\n",
    "df_long = pd.melt(results_selected[compared_metrics], value_vars=compared_metrics, var_name='metric', value_name='value')\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(y='value',x='metric',data=df_long, palette='Set2', fliersize=5)\n",
    "new_labels = [l.replace('test_','') for l in compared_metrics]\n",
    "plt.xticks(ticks=range(len(new_labels)), labels=new_labels, fontsize=18, rotation=25, ha='right')\n",
    "plt.yticks(fontsize=18)\n",
    "plt.grid(axis='y')\n",
    "plt.xlabel('', fontsize=18)\n",
    "plt.ylabel('', fontsize=18)\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.savefig('Ensemble_Boxplot_'+selected_model+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1e673-2270-4753-becd-23c93c264adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12 MachineLearning GPU",
   "language": "python",
   "name": "ml312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
