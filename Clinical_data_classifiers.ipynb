{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b35008-39ba-4f7e-a561-71fffbe1e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from Utils import evaluate_model_skl, store_results, visualize_boxplots, visualize_boxplot_onemodel, compare_models #, weighted_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e9d37-ad9d-4d9d-bd54-d557f0944249",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Clinical_data_classifiers')\n",
    "os.mkdir('Clinical_data_classifiers/Models')\n",
    "os.mkdir('Clinical_data_classifiers/Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a2f47-6bd4-4702-ad96-690048dda066",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40b3b5-2d1c-4f1f-add7-11c74d1de369",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b5bea-f240-4bf5-9aaf-a88129c78444",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data_h = pd.read_csv('Clinical_data/clinical_data_h.csv')\n",
    "clinical_data_s = pd.read_csv('Clinical_data/clinical_data_s.csv')\n",
    "cd_colnames = clinical_data_h.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72920e-f8b8-42c3-bd72-fcfdbf36f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate labels\n",
    "labels_h = [0]*len(clinical_data_h)\n",
    "labels_s = [1]*len(clinical_data_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154356d0-32fd-4076-84cc-5e674289f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([clinical_data_h,clinical_data_s])\n",
    "labels = np.concatenate((labels_h, labels_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc899b-520e-4077-b0c9-fa99ba89b46a",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae96103-8be7-493e-a6d0-26b7b19df486",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize\n",
    "M = data.max().values\n",
    "M[M<1] = 1\n",
    "m = data.min().values\n",
    "\n",
    "data = (data-m)/(M-m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ec9df2-58f8-46b9-8e73-e725a954c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select columns\n",
    "id_columns_to_delete = [1, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 22, 23, 25]\n",
    "columns_to_delete = cd_colnames[id_columns_to_delete]\n",
    "data.drop(columns_to_delete,axis=1,inplace=True)\n",
    "cd_colnames = list(cd_colnames)\n",
    "for f in columns_to_delete:\n",
    "    cd_colnames.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2add1f-ebee-4912-9843-e3980f25c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to numpy array\n",
    "data = np.asarray(data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bff7535-7f37-44e3-959f-f38126a57b38",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cadcec-66c2-4910-afcf-f17c9acff3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "def tune_hyperparameters(classifier_type, data, labels, k=4):\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    if classifier_type == 'SVC' or classifier_type == 'SVM':\n",
    "        # Support Vector Machine (SVM) classifier\n",
    "        estimator = SVC(class_weight={0: 1, 1: rate_train}, probability=True)\n",
    "        param_grid = {'C': [1,10,100,1000], # Regularization parameter. Default: C=1.0\n",
    "                      'kernel': ['linear', 'rbf', 'sigmoid', 'poly'], # Default: kernel='rbf'\n",
    "                      'gamma': ['scale', 'auto', 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001] # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Default: gamma='scale'\n",
    "                     }\n",
    "\n",
    "    elif classifier_type == 'DT':\n",
    "        # Decision Tree (DT) classifier\n",
    "        estimator = DecisionTreeClassifier(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = {'ccp_alpha' : np.arange(0, 0.1, 0.01), # Complexity parameter used for Minimal Cost-Complexity Pruning. Default: ccp_alpha = 0.0\n",
    "                      'criterion': ['gini','entropy'],#'log_loss'], # The function to measure the quality of a split. Default: criterion='gini'\n",
    "                      'max_depth' : [None, 1, 5, 10, 15], # The maximum depth of the tree. Default: max_depth=None\n",
    "                      #'max_features': [None, 'sqrt', 'log2'], # The number of features to consider when looking for the best split. Default: max_features=None\n",
    "                      'max_leaf_nodes': [None, 3, 6, 9], # Grow a tree with max_leaf_nodes in best-first fashion. Default: None\n",
    "                      'min_samples_leaf': [1, 2, 3, 4], # The minimum number of samples required to be at a leaf node. Default: min_samples_leaf=1\n",
    "                      'min_samples_split' : [2, 5, 10, 15], # The minimum number of samples required to split an internal node. Default: min_samples_split=2\n",
    "                      'min_weight_fraction_leaf' : np.arange(0.0, 0.5, 0.05), # The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Default: min_weight_fraction_leaf=0\n",
    "                      #'splitter': ['best','random'] # The strategy used to choose the split at each node. Default: splitter='best'\n",
    "                     }\n",
    "\n",
    "    elif classifier_type == 'RF':\n",
    "        # Random Forest (RF) classifier\n",
    "        estimator = RandomForestClassifier(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = {'n_estimators': np.arange(50, 225, 25),  # Number of trees in random forest. Default: n_estimators=100\n",
    "                      'criterion': ['gini','entropy'],#'log_loss'], # The function to measure the quality of a split. Default: criterion='gini'\n",
    "                      'max_features': ['sqrt', 'log2'],  # Number of features to consider at every split. Default: max_features='sqrt'\n",
    "                      'max_depth': list(np.arange(10, 110, 10))+['None'],  # Maximum number of levels in tree. Default: max_depth=None\n",
    "                      'min_samples_split': [2, 3, 5, 10],  # Minimum number of samples required to split a node. Default: min_samples_split=2\n",
    "                      'min_samples_leaf': [1, 2, 3, 4],  # Minimum number of samples required at each leaf node. Default: min_samples_leaf=1\n",
    "                      #'bootstrap': [True, False]  # Method of selecting samples for training each tree. Default: bootstrap=True\n",
    "                     }\n",
    "\n",
    "    elif classifier_type == 'DT_AdaBoost':\n",
    "        # AdaBoost with Decision Tree base estimator\n",
    "        base_estimator = DecisionTreeClassifier(class_weight={0: 1, 1: rate_train})\n",
    "        estimator = AdaBoostClassifier(base_estimator)\n",
    "        param_grid = {'n_estimators': np.arange(10, 110, 10), # The maximum number of estimators at which boosting is terminated. Default: n_estimators=50\n",
    "                      'learning_rate': [0.01, 0.1, 0.5, 1.0], # Weight applied to each classifier at each boosting iteration. Default: learning_rate=1.0\n",
    "                      \n",
    "                      ## Decision Tree parameters\n",
    "                      #'base_estimator__ccp_alpha' : np.arange(0, 0.1, 0.01), # Complexity parameter used for Minimal Cost-Complexity Pruning. Default: ccp_alpha = 0.0\n",
    "                      #'base_estimator__criterion': ['gini','entropy'],#'log_loss'], # The function to measure the quality of a split. Default: criterion='gini'\n",
    "                      #'base_estimator__max_depth' : [None, 1, 5, 10, 15], # The maximum depth of the tree. Default: max_depth=None\n",
    "                      ##'base_estimator__max_features': [None, 'sqrt', 'log2'], # The number of features to consider when looking for the best split. Default: max_features=None\n",
    "                      #'base_estimator__max_leaf_nodes': [None, 3, 6, 9], # Grow a tree with max_leaf_nodes in best-first fashion. Default: None\n",
    "                      #'base_estimator__min_samples_leaf': [1, 2, 3, 4], # The minimum number of samples required to be at a leaf node. Default: min_samples_leaf=1\n",
    "                      #'base_estimator__min_samples_split' : [2, 5, 10, 15], # The minimum number of samples required to split an internal node. Default: min_samples_split=2\n",
    "                      #'base_estimator__min_weight_fraction_leaf' : np.arange(0.0, 0.5, 0.05), # The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Default: min_weight_fraction_leaf=0\n",
    "                      ##'base_estimator__splitter': ['best','random'] # The strategy used to choose the split at each node. Default: splitter='best'\n",
    "                     }\n",
    "\n",
    "    elif classifier_type == 'RF_AdaBoost':\n",
    "        # AdaBoost with Decision Tree base estimator\n",
    "        base_estimator = RandomForestClassifier(class_weight={0: 1, 1: rate_train})\n",
    "        estimator = AdaBoostClassifier(base_estimator)\n",
    "        param_grid = {'n_estimators': np.arange(10, 110, 10), # The maximum number of estimators at which boosting is terminated. Default: n_estimators=50\n",
    "                      'learning_rate': [0.01, 0.1, 0.2, 0.5], # Weight applied to each classifier at each boosting iteration. Default: learning_rate=1.0\n",
    "                      \n",
    "                      ## Random Forest parameters\n",
    "                      #'base_estimator__n_estimators': np.arange(50, 225, 25),  # Number of trees in random forest. Default: n_estimators=100\n",
    "                      #'base_estimator__criterion': ['gini','entropy'],#'log_loss'], # The function to measure the quality of a split. Default: criterion='gini'\n",
    "                      #'base_estimator__max_features': ['sqrt', 'log2'],  # Number of features to consider at every split. Default: max_features='sqrt'\n",
    "                      #'base_estimator__max_depth': list(np.arange(10, 110, 10))+['None'],  # Maximum number of levels in tree. Default: max_depth=None\n",
    "                      #'base_estimator__min_samples_split': [2, 3, 5, 10],  # Minimum number of samples required to split a node. Default: min_samples_split=2\n",
    "                      #'base_estimator__min_samples_leaf': [1, 2, 3, 4],  # Minimum number of samples required at each leaf node. Default: min_samples_leaf=1\n",
    "                      ##'base_estimator__bootstrap': [True, False]  # Method of selecting samples for training each tree. Default: bootstrap=True\n",
    "                     }\n",
    "\n",
    "    else:\n",
    "        print('Wrong classifier type')\n",
    "        return\n",
    "\n",
    "    cost_scorer = 'roc_auc'  # cost_scorer = make_scorer(weighted_error, greater_is_better=False)\n",
    "\n",
    "    # Tune hyperparameters with k-fold cross-validation on training set\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring=cost_scorer, cv=k)\n",
    "    classifier.fit(data, labels)\n",
    "        \n",
    "    return classifier.best_estimator_, classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b971b-f5a0-48a0-a1a3-c3bfbf5763b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of parameters in a classifier\n",
    "\n",
    "def num_parameters(classifier_type, classifier):\n",
    "        \n",
    "    if classifier_type == 'SVC' or classifier_type == 'SVM':\n",
    "        n_support_vectors = len(classifier.support_vectors_)\n",
    "        n_coefficients = len(classifier.dual_coef_[0])\n",
    "        n_parameters = n_support_vectors + n_coefficients\n",
    "        \n",
    "    elif classifier_type == 'DT':\n",
    "        n_parameters = classifier.tree_.node_count\n",
    "        \n",
    "    elif classifier_type == 'RF':\n",
    "        n_trees = len(classifier.estimators_)\n",
    "        n_parameters = sum(tree.tree_.node_count for tree in classifier.estimators_)\n",
    "        \n",
    "    elif classifier_type == 'DT_AdaBoost':\n",
    "        n_estimators = len(classifier.estimators_)\n",
    "        n_parameters = sum(estimator.tree_.node_count for estimator in classifier.estimators_)\n",
    "        \n",
    "    elif classifier_type == 'RF_AdaBoost':\n",
    "        n_estimators = len(classifier.estimators_)\n",
    "        n_parameters = sum(\n",
    "            sum(tree.tree_.node_count for tree in estimator.estimators_)\n",
    "            for estimator in classifier.estimators_\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        n_parameters = None\n",
    "        \n",
    "    return n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911e392-16a8-47bf-a820-334336597b4b",
   "metadata": {},
   "source": [
    "# Tune hyperparameters and train N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2024280-8da7-40e2-93df-800a57cc0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "k = 4  # k for k-fold cross-validation in hyperparameter tuning\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10a3da-3d8f-4f68-aa35-97bf8663e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_params, trials_results = [], []\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)\n",
    "\n",
    "for trial, (train_index, test_index) in enumerate(splitter.split(data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ### Split the dataset\n",
    "    crossval_data = data[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    test_data = data[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "    #crossval_data, crossval_labels, test_data, test_labels = split_dataset(data, labels)\n",
    "    \n",
    "    for classifier_type in ['SVM','DT','RF','DT_AdaBoost','RF_AdaBoost']:\n",
    "\n",
    "        print(f'Classifier: {classifier_type}')\n",
    "        \n",
    "        ## Tune hyperparameters with 4-fold cross-validation and then train on the entire training set with the tuned hyperparameters\n",
    "        ti = time.time()\n",
    "        classifier, parameters = tune_hyperparameters(classifier_type, crossval_data, crossval_labels, k)\n",
    "        trials_params.append({**{'classifier':classifier_type}, **{'trial':trial+1}, **parameters})\n",
    "        train_time = time.time() - ti\n",
    "\n",
    "        hours, remainder = divmod(train_time, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "        ## Save the model\n",
    "        with open('Clinical_data_classifiers/Models/'+classifier_type+'_'+str(trial+1)+'.pkl','wb') as f:\n",
    "            pickle.dump(classifier,f)\n",
    "        \n",
    "        ## Predict\n",
    "        predictions_train = classifier.predict_proba(crossval_data)[:,1]\n",
    "        np.save('Clinical_data_classifiers/Predictions/'+classifier_type+'_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "        predictions_test = classifier.predict_proba(test_data)[:,1]\n",
    "        np.save('Clinical_data_classifiers/Predictions/'+classifier_type+'_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "        ## Print the number of parameters in the model\n",
    "        num_params = num_parameters(classifier_type, classifier)\n",
    "        print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "        ## Evaluate the model\n",
    "        results_train = evaluate_model_skl(predictions_train, crossval_labels)\n",
    "        print('TRAIN results:')\n",
    "        for metric, value in results_train.items():\n",
    "            print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "        print()\n",
    "            \n",
    "        results_test = evaluate_model_skl(predictions_test, test_labels)\n",
    "        print('TEST results:')\n",
    "        for metric, value in results_test.items():\n",
    "            print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "        print()\n",
    "\n",
    "        ## Store results\n",
    "        trials_results.append({**{'classifier':classifier_type}, **{'trial':trial+1}, \n",
    "                               **store_results(num_params, train_time, results_train, results_test)})\n",
    "\n",
    "    print(), print(100*'#'), print()\n",
    "    \n",
    "pd.DataFrame(trials_params).to_csv('Clinical_data_classifiers/Parameters_'+str(N)+'trials.csv')\n",
    "pd.DataFrame(trials_results).round(decimals=5).to_csv('Clinical_data_classifiers/Results_'+str(N)+'trials.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baded060-a0b8-4d0a-a81b-95a4af00d44d",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96830e-7a26-43e6-bd21-268a4e0d3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read results\n",
    "trials_results = pd.read_csv('Clinical_data_classifiers/Results_'+str(N)+'trials.csv', index_col=0)\n",
    "trials_results.fillna(1e-10, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ce483-4333-49bf-b414-1f08246e62c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print statistics\n",
    "models = trials_results.classifier.unique()\n",
    "metrics = [c for c in trials_results.columns if 'test_' in c and c not in ['test_TP','test_FP','test_TN','test_FN']]\n",
    "    \n",
    "statistics = pd.DataFrame(index=models, columns=[item for sublist in [[metric+'_mean', metric+'_std'] for metric in metrics] for item in sublist])\n",
    "    \n",
    "for metric in metrics:\n",
    "    mn, st = metric+'_mean', metric+'_std'\n",
    "    for model in models:\n",
    "        results = trials_results[trials_results['classifier']==model][metric].values\n",
    "        statistics.at[model,mn] = results.mean()\n",
    "        statistics.at[model,st] = results.std()\n",
    "\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202973e7-173a-4d0f-a653-123543f64381",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [m for m in metrics if 'test' in m]:\n",
    "    mn, st = metric+'_mean', metric+'_std'\n",
    "    if 'Loss' in metric or 'WE' in metric:\n",
    "        model_best = pd.to_numeric(statistics[metric+'_mean']).idxmin()\n",
    "        print(f'Model with lowest {metric} is {model_best} with value {statistics.loc[model_best,mn]} and standard deviation {statistics.loc[model_best,st]}')\n",
    "    else:\n",
    "        model_best = pd.to_numeric(statistics[metric+'_mean']).idxmax()\n",
    "        print(f'Model with highest {metric} is {model_best} with value {statistics.loc[model_best,mn]} and standard deviation {statistics.loc[model_best,st]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacb4cd-2e41-4491-8d96-3f0ef5d8113d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Print mean and std metrics for each model\n",
    "for classifier_type in trials_results.classifier.unique():\n",
    "    print(f'Classifier: {classifier_type}')\n",
    "    results = trials_results[trials_results['classifier'] == classifier_type]\n",
    "\n",
    "    # Number of parameters\n",
    "    parameters = results['Parameters'].values\n",
    "    print(f'Mean number of parameters: {parameters.mean()} [{parameters.min()}, {parameters.max()}], std {parameters.std()}')\n",
    "\n",
    "    # training time\n",
    "    trainTime = results['trainTime'].values\n",
    "    hours, remainder = divmod(trainTime.mean(), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Mean training time: {hours} hours, {minutes} minutes, and {seconds} seconds, (std {trainTime.std()} sec)')\n",
    "    print()\n",
    "    \n",
    "    # TRAIN results\n",
    "    metrics = ['BCELoss','Accuracy','Sensitivity','Specificity','ROC_AUC','Precision','F1','WE']\n",
    "    for metric in metrics:\n",
    "        values = results['train_' + metric].values\n",
    "        print(f'Mean train {metric}: {values.mean()}, std {values.std()}')\n",
    "    print()\n",
    "\n",
    "    # TEST results\n",
    "    for metric in metrics:\n",
    "        values = results['test_' + metric].values\n",
    "        print(f'Mean test {metric}: {values.mean()}, std {values.std()}')\n",
    "    print()\n",
    "\n",
    "    print('-'*120), print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a3e55-e754-416f-ab3b-01ed7972794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show boxplots\n",
    "visualize_boxplots(trials_results,\n",
    "                   ['test_BCELoss','test_Accuracy','test_F1','test_ROC_AUC','test_WE'], #[c for c in cd_trials_results.columns if 'test_' in c and c not in ['test_TP','test_FP','test_TN','test_FN','test_WE','test_Loss']],\n",
    "                   True,'Clinical_data_classifiers/Boxplots_allModels.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63c3ca-365f-4e17-972b-4b2feb559d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistical model comparison\n",
    "compare_models(trials_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9542a56f-00fa-4637-9bde-a04952a90d30",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f326f5-03e0-40e3-b173-0d4c824b7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732380bc-435b-4d65-bfd5-55bfae1cf8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_boxplot_onemodel(trials_results[trials_results['classifier']==selected_model],\n",
    "                           ['test_Accuracy','test_Sensitivity','test_Specificity','test_F1','test_ROC_AUC'],\n",
    "                           True,'Clinical_data_classifiers/Boxplot_'+selected_model+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e739ef-9078-47fa-a5cc-1151e5c35c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
