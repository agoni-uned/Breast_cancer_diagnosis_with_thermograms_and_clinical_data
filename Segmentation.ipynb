{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Segmentation')\n",
    "\n",
    "os.mkdir('Models')\n",
    "os.mkdir('Learning_curves')\n",
    "os.mkdir('Predictions')\n",
    "os.mkdir('Examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-publication",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images and Masks are located in folders called 'Images' and 'Masks', respectively, and have the same names.\n",
    "from PIL import Image\n",
    "images = np.asarray([np.array(Image.open(os.path.join('Images',f))) for f in os.listdir('Images')])\n",
    "masks = np.asarray([np.array(Image.open(os.path.join('Masks',f))) for f in os.listdir('Masks')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,w,h = images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize images\n",
    "M = images.max()\n",
    "m = images.min()\n",
    "\n",
    "images = (images - m)/(M - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for NumPy and for TensorFlow for random operations\n",
    "seed = 15\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Randomly shuffle data\n",
    "p = np.random.permutation(len(images))\n",
    "images = images[p]\n",
    "masks = masks[p]\n",
    "\n",
    "# 80-20 train-test split\n",
    "p = round(0.8*len(images))\n",
    "images_train = images[0:p]\n",
    "images_test = images[p::]\n",
    "masks_train = masks[0:p]\n",
    "masks_test = masks[p::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert into 3-channel images\n",
    "images_train_3ch = np.repeat(np.expand_dims(images_train,axis=-1),3,axis=-1)\n",
    "images_test_3ch = np.repeat(np.expand_dims(images_test,axis=-1),3,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list,title_list):\n",
    "    N = len(display_list)\n",
    "    L = len(display_list[0])-1\n",
    "    plt.figure(figsize=(15, 5*N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        img = display_list[i][0]\n",
    "        for j in range(L):\n",
    "            mask = display_list[i][j+1]\n",
    "            plt.subplot(N, L, (i*L+1)+j)\n",
    "            plt.imshow(img,'gray'), plt.imshow(mask,alpha=0.5)            \n",
    "            plt.axis('off')\n",
    "            if i == 0: plt.title(title_list[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [np.random.randint(0,len(images_test)) for i in range(10)]\n",
    "display_list = [[images_test[i],masks_test[i]] for i in idx]\n",
    "title_list = ['Ground truth']\n",
    "display(display_list,['Ground Truth'])\n",
    "\n",
    "img_list = [images_test[i] for i in idx]\n",
    "gt_list = [masks_test[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-composite",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import densenet, inception_v3, mobilenet, mobilenet_v2, vgg16, vgg19, resnet\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Conv2DTranspose, MaxPooling2D, AveragePooling2D, DepthwiseConv2D, Cropping2D, ZeroPadding2D, Concatenate # UpSampling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, schedules\n",
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_dict = {'densenet121':'densenet.DenseNet121','densenet169':'densenet.DenseNet169','densenet201':'densenet.DenseNet201',\n",
    "            'mobilenet':'mobilenet.MobileNet','mobilenet_v2':'mobilenet_v2.MobileNetV2',\n",
    "            'vgg16':'vgg16.VGG16','vgg19':'vgg19.VGG19',\n",
    "            'resnet50':'resnet.ResNet50','resnet101':'resnet.ResNet101','resnet152':'resnet.ResNet152',\n",
    "            'inception_v3':'inception_v3.InceptionV3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_downsampling_layers(encoder):\n",
    "    downsampling_layers = []\n",
    "    last_block = 'conv-_block-'\n",
    "    for layer in encoder.layers:\n",
    "        if isinstance(layer, tf.keras.layers.MaxPooling2D) \\\n",
    "           or (isinstance(layer, tf.keras.layers.AveragePooling2D) and layer.strides[0] > 1) \\\n",
    "           or (isinstance(layer, tf.keras.layers.Conv2D) and layer.strides[0] > 1) \\\n",
    "           or (isinstance(layer, tf.keras.layers.DepthwiseConv2D) and layer.strides[0] > 1):\n",
    "            \n",
    "            if '_block' not in layer.name:\n",
    "                # Densenet, VGG and MobileNet models\n",
    "                downsampling_layers.append(layer.input)\n",
    "                \n",
    "            else:\n",
    "                if last_block[4] != layer.name[4]:\n",
    "                    # ResNet models\n",
    "                    downsampling_layers.append(layer.input)\n",
    "                last_block = layer.name\n",
    "    return downsampling_layers\n",
    "\n",
    "def detect_downsampling_layers_inception(encoder):\n",
    "    downsampling_layers = []\n",
    "    for layer in encoder.layers:\n",
    "        if (isinstance(layer, tf.keras.layers.Conv2D) and layer.strides[0] > 1 and layer.input.shape[1] == encoder.input.shape[1]) \\\n",
    "           or isinstance(layer, tf.keras.layers.MaxPooling2D):\n",
    "            downsampling_layers.append(layer.input)                \n",
    "    return downsampling_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_var(var,out_shape):\n",
    "    # Width\n",
    "    dw = var.shape[1] - out_shape[0]\n",
    "    if dw > 0:\n",
    "        dpad_w = (0,0)\n",
    "        \n",
    "        if dw % 2 == 0:\n",
    "            dcrop_w = (dw//2,dw//2)\n",
    "        else:\n",
    "            dcrop_w = (dw//2+1,dw//2)\n",
    "        \n",
    "    elif dw < 0:\n",
    "        dcrop_w = (0,0)\n",
    "        \n",
    "        if dw % 2 == 0:\n",
    "            dpad_w = (-dw//2, -dw//2) \n",
    "        else:\n",
    "            dpad_w = (-dw//2+1,-dw//2)\n",
    "    else:\n",
    "        dpad_w = (0,0)\n",
    "        dcrop_w = (0,0)\n",
    "    \n",
    "    # Height\n",
    "    dh = var.shape[2] - out_shape[1]\n",
    "    if dh > 0:\n",
    "        dpad_h = (0,0)\n",
    "        \n",
    "        if dh % 2 == 0:\n",
    "            dcrop_h = (dh//2,dh//2)\n",
    "        else:\n",
    "            dcrop_h = (dh//2+1,dh//2)\n",
    "        \n",
    "    elif dh < 0:\n",
    "        dcrop_h = (0,0)\n",
    "        \n",
    "        if dh % 2 == 0:\n",
    "            dpad_h = (-dh//2, -dh//2) \n",
    "        else:\n",
    "            dpad_h = (-dh//2+1,-dh//2)\n",
    "    else:\n",
    "        dpad_h = (0,0)\n",
    "        dcrop_h = (0,0)\n",
    "        \n",
    "    var = Cropping2D(cropping=(dcrop_w,dcrop_h))(var)\n",
    "    var = ZeroPadding2D(padding=(dpad_w,dpad_h))(var)\n",
    "        \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_,h_ = w,h\n",
    "skip_shapes = []\n",
    "while w_ >= 7:\n",
    "    skip_shapes.append((w_,h_))\n",
    "    w_ = w_//2\n",
    "    h_ = h_//2\n",
    "skip_shapes.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet_CNN(model_handle, img_width, img_height, skip_shapes, num_classes):    \n",
    "    # Load a pretrained model as the encoder (e.g., ResNet50)\n",
    "    base_model = eval(model_handle + '(include_top=False, input_shape=(img_width,img_height,3))')\n",
    "\n",
    "    # Iterate through the encoder's layers to identify downsampling layers\n",
    "    if 'inception' not in model_handle:\n",
    "        skip_connections = detect_downsampling_layers(base_model)\n",
    "    else:\n",
    "        skip_connections = detect_downsampling_layers_inception(base_model)\n",
    "    skip_connections.append(base_model.layers[-1].output)\n",
    "    encoder = Model(base_model.input, skip_connections, name='Encoder')\n",
    "    \n",
    "    # Create the decoder part of the U-Net\n",
    "    img_in = Input(shape=(img_width, img_height, 3))\n",
    "    skip_connections = encoder(img_in)\n",
    "    decoder = skip_connections[-1]\n",
    "    \n",
    "    for i,skip in enumerate(reversed(skip_connections[:-1])):\n",
    "        # upsample\n",
    "        num_filters = skip.shape[-1]\n",
    "        decoder = Conv2DTranspose(num_filters,(2, 2),activation='relu',padding='same',strides=(2, 2))(decoder)\n",
    "        \n",
    "        # Adjust skip and decoder size\n",
    "        skip = reshape_var(skip,skip_shapes[i+1])\n",
    "        decoder = reshape_var(decoder,skip_shapes[i+1])\n",
    "        \n",
    "        # concatenate\n",
    "        decoder = Concatenate()([decoder, skip])\n",
    "\n",
    "        # convolution + batch normalization\n",
    "        decoder = Conv2D(num_filters,activation='relu',kernel_size=3,strides=1,padding='same',use_bias=True)(decoder)\n",
    "        decoder = BatchNormalization()(decoder)\n",
    "        \n",
    "        # convolution + batch normalization\n",
    "        if i < len(skip_connections[:-1])-1:\n",
    "            decoder = Conv2D(num_filters,activation='relu',kernel_size=3,strides=1,padding='same',use_bias=True)(decoder)\n",
    "            decoder = BatchNormalization()(decoder)\n",
    "        else:\n",
    "            # Final segmentation (output) layer\n",
    "            decoder = Conv2D(num_classes,activation='sigmoid',kernel_size=3,strides=1,padding='same',use_bias=True)(decoder)\n",
    "                \n",
    "    # Create the U-Net model\n",
    "    unet = Model(img_in, decoder)\n",
    "    \n",
    "    return unet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-nation",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "def Accuracy(labels,preds,threshold=0.5,smooth=1e-6):\n",
    "    preds = tf.where(preds >= threshold, 1.0, 0.0)\n",
    "    correct_predictions = tf.reduce_sum(tf.cast(tf.equal(preds, labels), tf.float32))\n",
    "    total_pixels = tf.cast(tf.reduce_prod(tf.shape(labels)), tf.float32)\n",
    "    acc = correct_predictions / total_pixels        \n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "## Jaccard index (Intersection over Union, IoU)\n",
    "def jaccard(labels,preds,smooth=1e-6):\n",
    "    # Compute the intersection and union of the predicted and ground truth masks\n",
    "    intersection = tf.reduce_sum(labels * preds)\n",
    "    union = tf.reduce_sum(labels) + tf.reduce_sum(preds) - intersection\n",
    "    \n",
    "    # Calculate the Jaccard index (IoU)\n",
    "    jaccard_index = (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    return jaccard_index\n",
    "\n",
    "def Jaccard(labels,preds,threshold=0.5,smooth=1e-6):\n",
    "    preds = tf.where(preds >= threshold, 1.0, 0.0)\n",
    "    jaccard_index = jaccard(labels,preds,smooth)  \n",
    "    return jaccard_index\n",
    "\n",
    "\n",
    "## Dice coefficient\n",
    "def diceCoeff(labels,preds,smooth=1e-6):\n",
    "    # Compute the intersection and union of the predicted and ground truth masks\n",
    "    intersection = tf.reduce_sum(labels * preds)\n",
    "    union = tf.reduce_sum(labels) + tf.reduce_sum(preds)\n",
    "    \n",
    "    # Calculate the Jaccard index (IoU)\n",
    "    dice = (2.*intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    return dice\n",
    "\n",
    "def DiceCoeff(labels,preds,threshold=0.5,smooth=1e-6):\n",
    "    preds = tf.where(preds >= threshold, 1.0, 0.0)\n",
    "    dice = diceCoeff(labels,preds,smooth)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-mailman",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiceLoss(labels,preds,smooth=1e-6):\n",
    "    dice = diceCoeff(labels,preds,smooth)\n",
    "    return 1 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-eight",
   "metadata": {},
   "source": [
    "# Fine-tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer_type,lr):\n",
    "    if optimizer_type == 'Adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif optimizer_type == 'SGD':\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "    elif optimizer_type == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        print('Error: optimizer name')\n",
    "        return\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model,lr_freeze,lr_unfreeze,optim,criterion,train_images,train_masks,val_images,val_masks,\n",
    "             num_epochs=30,num_epochs_freeze=15,batch_size=8,perc_unfreeze=0.2):\n",
    "    ## Step 1: train randomly initialized weights\n",
    "    #num_epochs_freeze = num_epochs//2\n",
    "    print('Freezing base model...')\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = get_optimizer(optim,lr_freeze)\n",
    "    \n",
    "    # Freeze the base pretrained CNN\n",
    "    for layer in model.layers:\n",
    "        if layer.name == 'Encoder':\n",
    "            for layer2 in layer.layers:\n",
    "                layer2.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "\n",
    "    # Compile the model and train it on your dataset\n",
    "    model.compile(optimizer=optimizer, loss=criterion, metrics=[Accuracy,Jaccard,DiceCoeff])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_images,np.expand_dims(train_masks,axis=-1).astype(np.float32),\n",
    "                        validation_data=[val_images,np.expand_dims(val_masks,axis=-1).astype(np.float32)],\n",
    "                        batch_size=batch_size,epochs=num_epochs_freeze, verbose=0)\n",
    "\n",
    "    train_loss = history.history['loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    train_iou = history.history['jaccard']\n",
    "    train_dice = history.history['dice_coeff']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    val_iou = history.history['val_jaccard']\n",
    "    val_dice = history.history['val_dice_coeff']\n",
    "    \n",
    "    print('Step 1 completed.')\n",
    "            \n",
    "            \n",
    "    ## Step 2: fine tune all the parameters in the model\n",
    "    num_epochs_unfreeze = num_epochs - num_epochs_freeze\n",
    "    print('Unfreezing base model...')\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = get_optimizer(optim,lr_unfreeze)\n",
    "    \n",
    "    # Unfreeze the top perc_unfreeze% layers in the encoder\n",
    "    for layer in model.layers:\n",
    "        if layer.name == 'Encoder':\n",
    "            L = len(layer.layers)\n",
    "            L_unfreeze = round(L*perc_unfreeze)\n",
    "            for i,layer2 in enumerate(reversed(layer.layers)):\n",
    "                if i <= L_unfreeze:\n",
    "                    layer2.trainable = True\n",
    "                else:\n",
    "                    layer2.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "\n",
    "    # Compile the model and train it on your dataset\n",
    "    model.compile(optimizer=optimizer, loss=criterion, metrics=[Accuracy,Jaccard,DiceCoeff])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_images,np.expand_dims(train_masks,axis=-1).astype(np.float32),\n",
    "                        validation_data=[val_images,np.expand_dims(val_masks,axis=-1).astype(np.float32)],\n",
    "                        batch_size=batch_size,epochs=num_epochs_unfreeze,verbose=0)\n",
    "\n",
    "    train_loss = train_loss + history.history['loss']\n",
    "    train_acc = train_acc + history.history['accuracy']\n",
    "    train_iou = train_iou + history.history['jaccard']\n",
    "    train_dice = train_dice + history.history['dice_coeff']\n",
    "    val_loss = val_loss + history.history['val_loss']\n",
    "    val_acc = val_acc + history.history['val_accuracy']\n",
    "    val_iou = val_iou + history.history['val_jaccard']\n",
    "    val_dice = val_dice + history.history['val_dice_coeff']\n",
    "    \n",
    "    print('Step 2 completed.')\n",
    "    \n",
    "    train_history = [train_loss,train_acc,train_iou,train_dice]\n",
    "    val_history = [val_loss,val_acc,val_iou,val_dice]\n",
    "    \n",
    "    return model, train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_history,val_history,num_epochs):\n",
    "    font_title = 12\n",
    "    font_legend = 10\n",
    "    \n",
    "    # Visualize the training results\n",
    "    epochs_range = range(num_epochs)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    train_loss,train_acc,train_iou,train_dice = train_history\n",
    "    val_loss,val_acc,val_iou,val_dice = val_history\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs_range, train_loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Test Loss')\n",
    "    plt.legend(loc='upper right', fontsize=font_legend)\n",
    "    plt.title('Train and Test Loss', fontsize=font_title)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs_range, train_acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Test Accuracy')\n",
    "    plt.legend(loc='lower right', fontsize=font_legend)\n",
    "    plt.title('Train and Test Accuracy', fontsize=font_title)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs_range, train_iou, label='Training IoU')\n",
    "    plt.plot(epochs_range, val_iou, label='Test IoU')\n",
    "    plt.legend(loc='lower right', fontsize=font_legend)\n",
    "    plt.title('Train and Test IoU', fontsize=font_title)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs_range, train_dice, label='Training Dice')\n",
    "    plt.plot(epochs_range, val_dice, label='Test Dice')\n",
    "    plt.legend(loc='lower right', fontsize=font_legend)\n",
    "    plt.title('Train and Test Dice', fontsize=font_title)\n",
    "    #plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(model,img_list,gt_list,threshold=0.5):\n",
    "    # Plot a few examples\n",
    "    N = len(img_list)\n",
    "    plt.figure(figsize=(12, 5*N))\n",
    "    \n",
    "    for i,data in enumerate(zip(img_list,gt_list)):\n",
    "        img,gt = data\n",
    "        \n",
    "        # Display ground truth\n",
    "        plt.subplot(N, 2, 2*i+1)\n",
    "        plt.imshow(img,'gray'), plt.imshow(gt,alpha=0.5)            \n",
    "        plt.axis('off')\n",
    "        if i == 0: plt.title('Ground truth', fontsize=20)\n",
    "            \n",
    "        # Predict mask\n",
    "        img_ = np.repeat(np.expand_dims(img,axis=-1),3,axis=-1)\n",
    "        pred = np.round(model.predict(np.expand_dims(img_,0)))\n",
    "        \n",
    "        # Display predicted mask\n",
    "        plt.subplot(N, 2, 2*i+2)\n",
    "        plt.imshow(img,'gray'), plt.imshow(pred[0],alpha=0.5)            \n",
    "        plt.axis('off')\n",
    "        if i == 0: plt.title('Predicted', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "# Training\n",
    "optim = 'Adam'   # 'Adam','SGD','RMSprop'\n",
    "batch_size = 8\n",
    "num_epochs = 30\n",
    "\n",
    "# Fine-tuning\n",
    "num_epochs_freeze = num_epochs//2\n",
    "perc_unfreeze = 0.2\n",
    "\n",
    "# Number of classes (output)\n",
    "num_classes = 1\n",
    "    \n",
    "# Classification threshold (probability > threshold: 1; 0 otherwise)\n",
    "threshold = 0.5\n",
    "\n",
    "# Loss function\n",
    "criterion = DiceLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-sailing",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "n_trials = 30\n",
    "\n",
    "# k for k-fold cross-validation\n",
    "k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crossval_subsets(images,masks,k):\n",
    "    # Create train+val subsets for k-fold cross-validation\n",
    "    fold_size = len(images) // k\n",
    "    \n",
    "    train_val_subsets = []\n",
    "    for i in range(k):\n",
    "        val_images = images[i*fold_size:(i+1)*fold_size]\n",
    "        train_images = np.concatenate((images[0:i*fold_size],images[(i+1)*fold_size::]))\n",
    "        \n",
    "        val_masks = masks[i*fold_size:(i+1)*fold_size]\n",
    "        train_masks = np.concatenate((masks[0:i*fold_size],masks[(i+1)*fold_size::]))\n",
    "        \n",
    "        train_val_subsets.append([train_images,train_masks,val_images,val_masks])\n",
    "    \n",
    "    return train_val_subsets\n",
    "\n",
    "def objective(trial, model_handle, images, masks, num_classes, k, optim='Adam', criterion=DiceLoss, num_epochs=100,\n",
    "              num_epochs_freeze=50, batch_size=8, perc_unfreeze=0.2):  \n",
    "    # Initialize model\n",
    "    _,w,h,_ = images.shape\n",
    "    unet = UNet_CNN(model_handle, w, h, skip_shapes, num_classes)\n",
    "    init_weights = unet.get_weights() \n",
    "    \n",
    "    ## Hyperparameters\n",
    "    # Learning rate\n",
    "    lr_freeze = trial.suggest_float('lr_freeze', 1e-5, 1e-2, log=True)\n",
    "    lr_unfreeze = trial.suggest_float('lr_unfreeze', 1e-6, 1e-3, log=True)\n",
    "        \n",
    "    ## Cross-validation\n",
    "    train_val_subsets = create_crossval_subsets(images,masks,k)\n",
    "    fold_loss_train,fold_acc_train,fold_iou_train,fold_dice_train = [],[],[],[]\n",
    "    fold_loss,fold_acc,fold_iou,fold_dice = [],[],[],[]\n",
    "    train_time = []\n",
    "    \n",
    "    for j,data in enumerate(train_val_subsets):\n",
    "        print('Fold {}/{}'.format(j+1, k))\n",
    "        \n",
    "        train_images,train_masks,val_images,val_masks = data\n",
    "        \n",
    "        unet.set_weights(init_weights)\n",
    "        \n",
    "        # Train the model\n",
    "        unet, train_history, val_history = finetune(unet,lr_freeze,lr_unfreeze,optim,criterion,train_images,train_masks,val_images,val_masks,\n",
    "                                                     num_epochs=num_epochs,num_epochs_freeze=num_epochs_freeze,batch_size=batch_size,perc_unfreeze=perc_unfreeze)\n",
    "    \n",
    "        #fold_loss_train.append(train_history[0][-1])\n",
    "        #fold_acc_train.append(train_history[1][-1])\n",
    "        #fold_iou_train.append(train_history[2][-1])\n",
    "        #fold_dice_train.append(train_history[3][-1])\n",
    "        fold_loss.append(val_history[0][-1])\n",
    "        #fold_acc.append(val_history[1][-1])\n",
    "        #fold_iou.append(val_history[2][-1])\n",
    "        fold_dice.append(val_history[3][-1])\n",
    "        \n",
    "    print('Fold Dice coeffs:'), print(fold_dice)\n",
    "    \n",
    "    return np.asarray(fold_loss).mean()  # Objective value linked with the Trial object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the objective inside a lambda and call objective inside it\n",
    "func = lambda trial: objective(trial, model_handle, images_train_3ch, masks_train, num_classes, k, optim=optim, criterion=criterion, num_epochs=num_epochs,\n",
    "                               num_epochs_freeze=num_epochs_freeze, batch_size=batch_size, perc_unfreeze=perc_unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['pretrainedCNN','nTrials','k-fold','optimizer','lossFunc','numEpochsFreeze','numEpochsUnfreeze','batchSize','percUnfreezeLayers','trainTime','lrFreeze','lrUnfreeze',\n",
    "          'bestMeanLoss']\n",
    "df_hyperparameter_tuning = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(model_handle,images,masks,num_classes,n_trials=50,k=4,optim='Adam',criterion=DiceLoss,num_epochs=100,num_epochs_freeze=50,\n",
    "                         batch_size=8,perc_unfreeze=0.2):\n",
    "                  \n",
    "    # Tune hyperparameters (lr_freeze and lr_unfreeze)\n",
    "    study = optuna.create_study()  # Create a new study.\n",
    "    study.optimize(func, n_trials=n_trials)  # Invoke optimization of the objective function.\n",
    "\n",
    "    # Get best hyperparameter combination\n",
    "    lr_freeze = study.best_params['lr_freeze']\n",
    "    lr_unfreeze = study.best_params['lr_unfreeze']\n",
    "    optuna.visualization.plot_parallel_coordinate(study, params=['lr_freeze','lr_unfreeze'])\n",
    "    \n",
    "    hyperparameters = [lr_freeze,lr_unfreeze]\n",
    "    \n",
    "    return hyperparameters,study.best_value,study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-credit",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuned_lr = {}\n",
    "for baseModel_name in CNN_dict.keys():\n",
    "    print(baseModel_name), print()\n",
    "    model_handle = CNN_dict[baseModel_name]\n",
    "\n",
    "    # Tune hyperparameters (lr_freeze and lr_unfreeze)\n",
    "    tic = time.time()\n",
    "    hyperparameters,best_value,trials_df = tune_hyperparameters(model_handle,images_train_3ch,masks_train,num_classes,n_trials,4,optim,criterion,num_epochs,\n",
    "                                                                num_epochs_freeze,batch_size,perc_unfreeze)\n",
    "    elapsedTime = time.time() - tic\n",
    "    print('Hyperparameter tuning took ' + str(elapsedTime//60) + ' minutes and ' + str(elapsedTime%60) + ' seconds')\n",
    "    \n",
    "    lr_freeze,lr_unfreeze = hyperparameters\n",
    "    tuned_lr[baseModel_name] = {'lr_freeze' : lr_freeze, 'lr_unfreeze' : lr_unfreeze}\n",
    "    \n",
    "    df_hyperparameter_tuning.loc[len(df_hyperparameter_tuning.index)] = [baseModel_name,n_trials,k,optim,'DiceLoss',num_epochs_freeze,num_epochs-num_epochs_freeze,\n",
    "                                                                        batch_size,100*perc_unfreeze,elapsedTime,lr_freeze,lr_unfreeze,best_value]\n",
    "    df_hyperparameter_tuning.to_csv('HyperparamTuning_Results.csv',index=False)\n",
    "    print(df_hyperparameter_tuning.iloc[len(df_hyperparameter_tuning.index)-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tuned_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-banner",
   "metadata": {},
   "source": [
    "# Training with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155a4bc-d66c-4c32-a199-77db05af32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyperparameter_tuning = pd.read_csv(HyperparamTuning_Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(baseModel_name,images_train,masks_train,images_test,masks_test,num_classes,CNN_dict,lr_freeze=0.005,lr_unfreeze=0.0005,optim='Adam',criterion=DiceLoss,\n",
    "                num_epochs=100,num_epochs_freeze=50,batch_size=8,perc_unfreeze=0.2):   \n",
    "\n",
    "    model_handle = CNN_dict[baseModel_name]\n",
    "    \n",
    "    # Initialize model\n",
    "    _,w,h,_ = images_train.shape\n",
    "    unet = UNet_CNN(model_handle, w, h, skip_shapes, num_classes)\n",
    "        \n",
    "    # Train the model\n",
    "    tic = time.time()\n",
    "    unet, train_history, test_history = finetune(unet,lr_freeze,lr_unfreeze,optim,criterion,images_train,masks_train,images_test,masks_test,\n",
    "                                                 num_epochs=num_epochs,num_epochs_freeze=num_epochs_freeze,batch_size=batch_size,perc_unfreeze=perc_unfreeze)\n",
    "    train_time = time.time() - tic\n",
    "    \n",
    "    results = [train_history[0][-1],train_history[1][-1],train_history[2][-1],train_history[3][-1],\n",
    "               test_history[0][-1],test_history[1][-1],test_history[2][-1],test_history[3][-1]]\n",
    "    \n",
    "    ## Save trained model\n",
    "    unet.save('Models/' + baseModel_name + '.h5')\n",
    "    \n",
    "    # Plot training process\n",
    "    plot_training(train_history,test_history,num_epochs)\n",
    "    plt.savefig('Learning_curves/' + baseModel_name + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save predicted masks\n",
    "    preds = unet.predict(np.concatenate((images_train,images_test)))\n",
    "    if preds.ndim == 3:\n",
    "        preds = (preds > threshold).astype(int)\n",
    "    else:\n",
    "        preds = np.argmax(preds,axis=-1)\n",
    "    np.save('Predictions/'+baseModel_name+'.npy',preds)\n",
    "    \n",
    "    return unet,results,train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['pretrainedCNN','optimizer','lossFunc','numEpochsFreeze','numEpochsUnfreeze','batchSize','percUnfreezeLayers','lrFreeze','lrUnfreeze','trainTime','inferenceTime',\n",
    "          'trainLoss','trainAcc','trainIOU','trainDice','testLoss','testAcc','testIOU','testDice']\n",
    "df_results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "for baseModel_name in CNN_dict.keys():\n",
    "    print(baseModel_name), print()\n",
    "    \n",
    "    # Train the model\n",
    "    lr_freeze = df_hyperparameter_tuning[df_hyperparameter_tuning['pretrainedCNN']==baseModel_name]['lrFreeze'].values[0]\n",
    "    lr_unfreeze = df_hyperparameter_tuning[df_hyperparameter_tuning['pretrainedCNN']==baseModel_name]['lrUnfreeze'].values[0]\n",
    "    model,results,train_time = train_model(baseModel_name,images_train_3ch,masks_train,images_test_3ch,masks_test,num_classes,CNN_dict,lr_freeze,lr_unfreeze,\n",
    "                                           optim,criterion,num_epochs,num_epochs_freeze,batch_size,perc_unfreeze)\n",
    "    \n",
    "    # Inference time\n",
    "    inference_time = []\n",
    "    for img in images_test_3ch:\n",
    "        tic = time.time()\n",
    "        pred = model.predict(img[np.newaxis,:])\n",
    "        inference_time.append(time.time() - tic)\n",
    "    inference_time = np.asarray(inference_time).mean()\n",
    "    \n",
    "    # Store results\n",
    "    train_loss,train_acc,train_iou,train_dice,test_loss,test_acc,test_iou,test_dice = results\n",
    "    train_time = train_time//60 + train_time%60\n",
    "    df_results.loc[len(df_results.index)] = [baseModel_name,optim,'DiceLoss',num_epochs_freeze,num_epochs-num_epochs_freeze,batch_size,100*perc_unfreeze,lr_freeze,\n",
    "                                            lr_unfreeze,train_time,inference_time,train_loss,train_acc,train_iou,train_dice,test_loss,test_acc,test_iou,test_dice]\n",
    "    df_results.to_csv('Results.csv',index=False)\n",
    "    print(df_results.iloc[len(df_results.index)-1])\n",
    "    \n",
    "    # Plot some examples\n",
    "    plot_examples(model,img_list,gt_list, threshold)\n",
    "    plt.savefig('Examples/' + baseModel_name + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a4052-cd5c-4d55-b0a8-66fbbc2efdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732c7c5-366b-4283-85f5-150390c5e3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
