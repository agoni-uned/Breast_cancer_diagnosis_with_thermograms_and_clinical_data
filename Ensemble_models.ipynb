{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19233e-4ceb-4f91-bf04-15d0a83085e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from Utils import evaluate_model_skl, store_results, visualize_boxplots, visualize_boxplot_onemodel, compare_models, weighted_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d0614-c82d-41d8-a52b-13f07a8e4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795203c-4ba3-4fad-9e21-2ff897da0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10  # experiment repetitions\n",
    "k = 4  # k for k-fold cross-validation in hyperparameter tuning\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e09cb4-8b42-4172-9987-e29262849ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selected models\n",
    "cd_classifier = 'RF'\n",
    "img_classifier = 'multiInputCNN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6433c-be32-4135-af5c-43b8b0487806",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb6290-068c-4144-8bca-aaa67beca8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Images\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Frontal images\n",
    "front_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/Front',f))) for f in os.listdir('Images/Healthy/Front')])\n",
    "front_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/Front',f))) for f in os.listdir('Images/Sick/Front')])\n",
    "front = np.concatenate((front_images_h, front_images_s))\n",
    "\n",
    "## Left lateral (L90) images\n",
    "L90_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/L90',f))) for f in os.listdir('Images/Healthy/L90')])\n",
    "L90_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/L90',f))) for f in os.listdir('Images/Sick/L90')])\n",
    "L90 = np.concatenate((L90_images_h, L90_images_s))\n",
    "\n",
    "# Right lateral (R90) images\n",
    "R90_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/R90',f))) for f in os.listdir('Images/Healthy/R90')])\n",
    "R90_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/R90',f))) for f in os.listdir('Images/Sick/R90')])\n",
    "R90 = np.concatenate((R90_images_h, R90_images_s))\n",
    "\n",
    "## Shape of thermograms\n",
    "_,h,w = front.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ea4a0-8e58-4bee-ba23-abc352639e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clinical data\n",
    "clinical_data_h = pd.read_csv('Clinical_data/clinical_data_h.csv')\n",
    "clinical_data_s = pd.read_csv('Clinical_data/clinical_data_s.csv')\n",
    "cd_colnames = clinical_data_h.columns\n",
    "\n",
    "clinical_data = pd.concat([clinical_data_h,clinical_data_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7957fa5-1c69-4014-a0fb-d3454df9d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate labels\n",
    "labels_h = [0]*len(front_images_h)\n",
    "labels_s = [1]*len(front_images_s)\n",
    "labels = np.concatenate((labels_h, labels_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916bf10a-2713-4a7b-bb1d-2e2766c245ef",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8127f494-af86-46d9-8e3b-5f7dec072c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min-max normalization\n",
    "\n",
    "# Images\n",
    "M = np.concatenate((front, L90, R90)).max()\n",
    "m = np.concatenate((front, L90, R90))).min()\n",
    "\n",
    "front = ((front - m) / (M - m)).astype('float32')\n",
    "L90 = ((L90 - m) / (M - m)).astype('float32')\n",
    "R90 = ((R90 - m) / (M - m)).astype('float32')\n",
    "\n",
    "# Clinical data\n",
    "M = clinical_data.max().values\n",
    "M[M<1] = 1\n",
    "m = clinical_data.min().values\n",
    "\n",
    "clinical_data = (clinical_data-m)/(M-m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70710e22-b2ad-4064-a4e8-43a3bb60652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select columns of clinical data\n",
    "id_columns_to_delete = [1, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 22, 23, 25]\n",
    "columns_to_delete = cd_colnames[id_columns_to_delete]\n",
    "clinical_data.drop(columns_to_delete,axis=1,inplace=True)\n",
    "cd_colnames = list(cd_colnames)\n",
    "for f in columns_to_delete:\n",
    "    cd_colnames.remove(f)\n",
    "\n",
    "## Convert to numpy array\n",
    "clinical_data = np.asarray(clinical_data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f9e251-3372-44df-a268-20a782a13f27",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe91b0-236a-43a7-be1f-f4fd819459da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "def tune_hyperparameters(classifier_type, data, labels, k=4):\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    if classifier_type == 'SVC' or classifier_type == 'SVM':\n",
    "        # Support Vector Machine (SVM) classifier\n",
    "        estimator = SVC(class_weight={0: 1, 1: rate_train}, probability=True)\n",
    "        param_grid = {'C': [1,10,100,1000], # Regularization parameter. Default: C=1.0\n",
    "                      'kernel': ['linear', 'rbf', 'sigmoid', 'poly'], # Default: kernel='rbf'\n",
    "                      'gamma': ['scale', 'auto', 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001] # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Default: gamma='scale'\n",
    "                     }\n",
    "\n",
    "    elif classifier_type == 'DT':\n",
    "        # Decision Tree (DT) classifier\n",
    "        estimator = DecisionTreeClassifier(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = {'ccp_alpha' : np.arange(0, 0.1, 0.01), # Complexity parameter used for Minimal Cost-Complexity Pruning. Default: ccp_alpha = 0.0\n",
    "                      'criterion': ['gini','entropy'],#'log_loss'], # The function to measure the quality of a split. Default: criterion='gini'\n",
    "                      'max_depth' : [None, 1, 5, 10, 15], # The maximum depth of the tree. Default: max_depth=None\n",
    "                      #'max_features': [None, 'sqrt', 'log2'], # The number of features to consider when looking for the best split. Default: max_features=None\n",
    "                      'max_leaf_nodes': [None, 3, 6, 9], # Grow a tree with max_leaf_nodes in best-first fashion. Default: None\n",
    "                      'min_samples_leaf': [1, 2, 3, 4], # The minimum number of samples required to be at a leaf node. Default: min_samples_leaf=1\n",
    "                      'min_samples_split' : [2, 5, 10, 15], # The minimum number of samples required to split an internal node. Default: min_samples_split=2\n",
    "                      'min_weight_fraction_leaf' : np.arange(0.0, 0.5, 0.05), # The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Default: min_weight_fraction_leaf=0\n",
    "                      #'splitter': ['best','random'] # The strategy used to choose the split at each node. Default: splitter='best'\n",
    "                     }\n",
    "\n",
    "    elif classifier_type == 'SGD':\n",
    "        # Linear classifiers (SVM, logistic regression, etc.) with Stochastic Gradient Descent (SGD) training\n",
    "        estimator = SGDClassifier(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = {'loss': ['log_loss', 'modified_huber'], # The loss function to be used. Default: loss='hinge'. Options: ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron', ‘squared_error’, ‘huber’, ‘epsilon_insensitive’, ‘squared_epsilon_insensitive’]\n",
    "                      'penalty': [None, 'l2', 'l1', 'elasticnet'], # The penalty (aka regularization term) to be used. Default: penalty='l2'\n",
    "                      'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1], # Constant that multiplies the regularization term. Default: alpha=0.0001\n",
    "                      'l1_ratio': [0.05, 0.15, 0.3, 0.5, 0.7, 0.9],  # The Elastic Net mixing parameter, only used if penalty is 'elasticnet'. Default: l1_ratio=0.15\n",
    "                      'max_iter': [500, 1000, 1500, 2000], # The maximum number of passes over the training data (aka epochs). Default: max_iter=1000\n",
    "                      'tol': [None, 1e-2, 1e-3, 1e-4, 1e-5], # The stopping criterion. Default: tol=1e-3\n",
    "                      'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'], # The learning rate schedule. Default: learning_rate='optimal'\n",
    "                      'eta0':  [0.001, 0.01, 0.1, 1, 10]  # The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. Default: eta0=0.0\n",
    "                    }\n",
    "\n",
    "    elif classifier_type == 'NN':\n",
    "        # Linear perceptron classifier (single-layer)\n",
    "        estimator = Perceptron(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = {'penalty': [None, 'l2', 'l1', 'elasticnet'], # The penalty (aka regularization term). Default: penalty=None\n",
    "                      'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1], # Constant that multiplies the regularization term if regularization is used. Default: alpha=0.0001\n",
    "                      'fit_intercept': [True, False], # Whether the intercept should be estimated or not. Default: fit_intercept=True\n",
    "                      'max_iter': [500, 1000, 1500, 2000], # The maximum number of passes over the training data (aka epochs). Default: max_iter=1000\n",
    "                      'tol': [None, 1e-2, 1e-3, 1e-4, 1e-5], # The stopping criterion. Default: tol=1e-3\n",
    "                      #'shuffle': [True, False], # Whether or not the training data should be shuffled after each epoch. Default: shuffle=True\n",
    "                      'eta0': [10, 1, 0.1, 0.01, 0.001], # Constant by which the updates are multiplied. Default: eta0=1\n",
    "                      'validation_fraction': [0.01, 0.1, 0.2, 0.3] # The proportion of training data to set aside as validation set for early stopping. Default: validation_fraction=0.1\n",
    "                    }\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print('Wrong classifier type')\n",
    "        return\n",
    "\n",
    "    cost_scorer = 'roc_auc'  # cost_scorer = make_scorer(weighted_error, greater_is_better=False)\n",
    "\n",
    "    # Tune hyperparameters with k-fold cross-validation on training set\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring=cost_scorer, cv=k)\n",
    "    classifier.fit(data, labels)\n",
    "        \n",
    "    return classifier.best_estimator_, classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02308519-1cc6-4371-ad43-d39d5f93307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to train the weight for clinical data in the Weighted Voting (WV) classifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, make_scorer\n",
    "\n",
    "def tune_weight(pred_CD, pred_img, labels, metric=None):\n",
    "    best_weight = 0\n",
    "    best_metric = 0\n",
    "    if metric == \"we\":\n",
    "        best_metric = np.inf\n",
    "            \n",
    "    for weight_cd in range(0, 10000, 1):\n",
    "        weight = weight_cd/10000\n",
    "                \n",
    "        new_pred = pred_CD * weight + pred_img * (1 - weight)\n",
    "        \n",
    "        if metric == \"accuracy\":\n",
    "            accuracy = accuracy_score(labels, np.round(new_pred))\n",
    "            if accuracy > best_metric:\n",
    "                best_metric = accuracy\n",
    "                best_weight = weight\n",
    "        elif metric == \"roc_auc\":\n",
    "            roc_auc = roc_auc_score(labels, new_pred)\n",
    "            if roc_auc > best_metric:\n",
    "                best_metric = roc_auc\n",
    "                best_weight = weight\n",
    "        elif metric == \"we\":\n",
    "            we = weighted_error(labels, np.round(new_pred))\n",
    "            if we < best_metric:\n",
    "                best_metric = we\n",
    "                best_weight = weight\n",
    "        else:\n",
    "            tune_weight(pred_CD, pred_img, labels, \"accuracy\")\n",
    "            tune_weight(pred_CD, pred_img, labels, \"roc_auc\")\n",
    "            tune_weight(pred_CD, pred_img, labels, \"we\")\n",
    "            break\n",
    "            \n",
    "    print(f'Best {metric}: {best_metric} | Best weight: {best_weight}')\n",
    "    return best_weight\n",
    "\n",
    "def weighted_voting(pred_cd, pred_img, weight = 0.698):\n",
    "    return pred_cd * weight + pred_img * (1 - weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f375642-d55c-4bf3-8afb-02568caef767",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of parameters in a classifier\n",
    "\n",
    "def num_parameters(classifier_type, classifier):\n",
    "        \n",
    "    if classifier_type == 'WV':\n",
    "        n_parameters = 1\n",
    "        \n",
    "    elif classifier_type == 'SVC' or classifier_type == 'SVM':\n",
    "        n_support_vectors = len(classifier.support_vectors_)\n",
    "        n_coefficients = len(classifier.dual_coef_[0])\n",
    "        n_parameters = n_support_vectors + n_coefficients\n",
    "        \n",
    "    elif classifier_type == 'DT':\n",
    "        n_parameters = classifier.tree_.node_count\n",
    "        \n",
    "    elif classifier_type == 'SGD':\n",
    "            n_parameters = classifier.coef_.size + classifier.intercept_.size\n",
    "            \n",
    "    elif classifier_type == 'NN':\n",
    "        n_parameters = classifier.coef_.size + classifier.intercept_.size\n",
    "        \n",
    "    else:\n",
    "        n_parameters = None\n",
    "        \n",
    "    return n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ef8e5-fd34-4ff5-aa9d-3fb2a0478995",
   "metadata": {},
   "source": [
    "# Tune hyperparameters and train N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586aacfc-54c2-4fd6-9a7a-8a9b3f061693",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_params, trials_results = [], []\n",
    "\n",
    "#np.random.seed(seed)\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)\n",
    "\n",
    "for trial, (train_index, test_index) in enumerate(splitter.split(clinical_data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ### Split the dataset\n",
    "    crossval_front = front[train_index]\n",
    "    crossval_L90 = L90[train_index]\n",
    "    crossval_R90 = R90[train_index]\n",
    "    crossval_clinical_data = clinical_data[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "\n",
    "    test_front = front[train_index]\n",
    "    test_L90 = L90[train_index]\n",
    "    test_R90 = R90[train_index]\n",
    "    test_clinical_data = clinical_data[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    \n",
    "    ## Load trained models and predict\n",
    "    # Clinical data classifier\n",
    "    with open('Clinical_data_classifiers/Models/'+cd_classifier+'_'+str(trial+1)+'.pkl', 'rb') as f:\n",
    "        cd_model = pickle.load(f)\n",
    "        \n",
    "    pred_cd_crossval = cd_model.predict_proba(crossval_clinical_data)[:,1]\n",
    "    pred_cd_test = cd_model.predict_proba(test_clinical_data)[:,1]\n",
    "        \n",
    "    # Multi-view image classifier\n",
    "    img_model = tf.keras.models.load_model('Image_classifiers/Models/'+img_classifier+'_'+str(trial+1)+'.h5', compile=False)\n",
    "    if img_classifier == 'multiInputCNN':\n",
    "        # Input: list of views\n",
    "        pred_img_crossval = img_model([crossval_front, crossval_L90, crossval_R90])[:,0]\n",
    "        pred_img_test = img_model([test_front, test_L90, test_R90])[:,0]\n",
    "    else:\n",
    "        # Input: concatenate views along the channel axis\n",
    "        pred_img_crossval = img_model(np.stack((crossval_front, crossval_L90, crossval_R90), axis=-1))[:,0]\n",
    "        pred_img_test = img_modelnp.stack((test_front, test_L90, test_R90), axis=-1))[:,0]        \n",
    "\n",
    "    \n",
    "    for classifier_type in ['WV','SVM','DT','SGD','NN']:\n",
    "\n",
    "        print(f'Classifier: {classifier_type}')\n",
    "\n",
    "        if classifier_type == 'WV':\n",
    "            # Weighted Voting (WV) classifier does not require hyperparameter tuning\n",
    "            ti = time.time()\n",
    "            weight = tune_weight(pred_cd_crossval, pred_img_crossval, crossval_labels, metric='roc_auc')\n",
    "            trials_params.append({'classifier':classifier_type, 'trial':trial+1, 'weight':weight})\n",
    "            train_time = time.time() - ti\n",
    "    \n",
    "            hours, remainder = divmod(train_time, 3600)\n",
    "            minutes, seconds = divmod(remainder, 60)\n",
    "            print(f'Training took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "            ## Predict\n",
    "            predictions_train = weighted_voting(pred_cd_crossval, pred_img_crossval, weight)\n",
    "            np.save('Ensemble_models/Predictions/'+classifier_type+'_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "            predictions_test = weighted_voting(pred_cd_test, pred_img_test, weight)\n",
    "            np.save('Ensemble_models/Predictions/'+classifier_type+'_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "            ## Print the number of parameters in the model\n",
    "            num_params = num_parameters(classifier_type, _)\n",
    "            print(f'Classifier has {num_params} parameters.'), print()\n",
    "            \n",
    "        else:\n",
    "\n",
    "            ## Concatenate predictions to generate the input vector\n",
    "            inputs_crossval = np.concatenate((pred_img_crossval.reshape(-1, 1),pred_cd_crossval.reshape(-1, 1)), axis=1)\n",
    "            inputs_test = np.concatenate((pred_img_test.reshape(-1, 1),pred_cd_test.reshape(-1, 1)), axis=1)\n",
    "            \n",
    "            ## Tune hyperparameters with k-fold cross-validation and then train on the entire training set with the tuned hyperparameters\n",
    "            ti = time.time()\n",
    "            classifier, parameters = tune_hyperparameters(classifier_type, inputs_crossval, crossval_labels, k)\n",
    "            trials_params.append({**{'classifier':classifier_type}, **{'trial':trial+1}, **parameters})\n",
    "            train_time = time.time() - ti\n",
    "    \n",
    "            hours, remainder = divmod(train_time, 3600)\n",
    "            minutes, seconds = divmod(remainder, 60)\n",
    "            print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "    \n",
    "            ## Save the model\n",
    "            with open('Ensemble_models/Models/'+classifier_type+'_'+str(trial+1)+'.pkl','wb') as f:\n",
    "                pickle.dump(classifier,f)\n",
    "        \n",
    "            ## Predict\n",
    "            if not classifier_type == 'NN':\n",
    "                predictions_train = classifier.predict_proba(inputs_crossval)[:,1]\n",
    "                predictions_test = classifier.predict_proba(inputs_test)[:,1]\n",
    "            else:\n",
    "                predictions_train = classifier.predict(inputs_crossval)\n",
    "                predictions_test = classifier.predict(inputs_test)\n",
    "            np.save('Ensemble_models/Predictions/'+classifier_type+'_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "            np.save('Ensemble_models/Predictions/'+classifier_type+'_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "            ## Print the number of parameters in the model\n",
    "            num_params = num_parameters(classifier_type, classifier)\n",
    "            print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "        \n",
    "        ## Evaluate the model\n",
    "        results_train = evaluate_model_skl(predictions_train, crossval_labels)\n",
    "        print('TRAIN results:')\n",
    "        for metric, value in results_train.items():\n",
    "            print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "        print()\n",
    "            \n",
    "        results_test = evaluate_model_skl(predictions_test, test_labels)\n",
    "        print('TEST results:')\n",
    "        for metric, value in results_test.items():\n",
    "            print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "        print()\n",
    "\n",
    "        ## Store results\n",
    "        trials_results.append({**{'classifier':classifier_type}, **{'trial':trial+1}, \n",
    "                               **store_results(num_params, train_time, results_train, results_test)})\n",
    "\n",
    "    print(), print(100*'#'), print()\n",
    "    \n",
    "pd.DataFrame(trials_params).to_csv('Ensemble_models/Parameters_'+str(N)+'trials.csv')\n",
    "pd.DataFrame(trials_results).round(decimals=5).to_csv('Ensemble_models/Results_'+str(N)+'trials.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf18c45-a480-4722-b3de-afba75453cfb",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0acad-33c4-493e-ba10-cb7e755f38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read results\n",
    "trials_results = pd.read_csv('Ensemble_models/Results_'+str(N)+'trials.csv', index_col=0)\n",
    "trials_results.fillna(1e-10, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed646b-7039-4069-a775-4c51bc49bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print statistics\n",
    "models = trials_results.classifier.unique()\n",
    "metrics = [c for c in trials_results.columns if 'test_' in c and c not in ['test_TP','test_FP','test_TN','test_FN']]\n",
    "    \n",
    "statistics = pd.DataFrame(index=models, columns=[item for sublist in [[metric+'_mean', metric+'_std'] for metric in metrics] for item in sublist])\n",
    "    \n",
    "for metric in metrics:\n",
    "    mn, st = metric+'_mean', metric+'_std'\n",
    "    for model in models:\n",
    "        results = trials_results[trials_results['classifier']==model][metric].values\n",
    "        statistics.at[model,mn] = results.mean()\n",
    "        statistics.at[model,st] = results.std()\n",
    "\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec041a8-d508-4809-a160-46115660dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [m for m in metrics if 'test' in m]:\n",
    "    mn, st = metric+'_mean', metric+'_std'\n",
    "    if 'Loss' in metric or 'WE' in metric:\n",
    "        model_best = pd.to_numeric(statistics[metric+'_mean']).idxmin()\n",
    "        print(f'Model with lowest {metric} is {model_best} with value {statistics.loc[model_best,mn]} and standard deviation {statistics.loc[model_best,st]}')\n",
    "    else:\n",
    "        model_best = pd.to_numeric(statistics[metric+'_mean']).idxmax()\n",
    "        print(f'Model with highest {metric} is {model_best} with value {statistics.loc[model_best,mn]} and standard deviation {statistics.loc[model_best,st]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1345d5-12b5-4179-af9d-73a5cab26040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Print mean and std metrics for each model\n",
    "for classifier_type in trials_results.classifier.unique():\n",
    "    print(f'Classifier: {classifier_type}')\n",
    "    results = trials_results[trials_results['classifier'] == classifier_type]\n",
    "\n",
    "    # Number of parameters\n",
    "    parameters = results['Parameters'].values\n",
    "    print(f'Mean number of parameters: {parameters.mean()} [{parameters.min()}, {parameters.max()}], std {parameters.std()}')\n",
    "\n",
    "    # training time\n",
    "    trainTime = results['trainTime'].values\n",
    "    hours, remainder = divmod(trainTime.mean(), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Mean training time: {hours} hours, {minutes} minutes, and {seconds} seconds, (std {trainTime.std()} sec)')\n",
    "    print()\n",
    "    \n",
    "    # TRAIN results\n",
    "    metrics = ['BCELoss','Accuracy','Sensitivity','Specificity','ROC_AUC','Precision','F1','WE']\n",
    "    for metric in metrics:\n",
    "        values = results['train_' + metric].values\n",
    "        print(f'Mean train {metric}: {values.mean()}, std {values.std()}')\n",
    "    print()\n",
    "\n",
    "    # TEST results\n",
    "    for metric in metrics:\n",
    "        values = results['test_' + metric].values\n",
    "        print(f'Mean test {metric}: {values.mean()}, std {values.std()}')\n",
    "    print()\n",
    "\n",
    "    print('-'*120), print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc60d4-79a8-4d71-a1b4-e75772671e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show boxplots\n",
    "visualize_boxplots(trials_results,\n",
    "                   ['test_BCELoss','test_Accuracy','test_F1','test_ROC_AUC','test_WE'], #[c for c in cd_trials_results.columns if 'test_' in c and c not in ['test_TP','test_FP','test_TN','test_FN','test_WE','test_Loss']],\n",
    "                   True,'Ensemble_models/Boxplots_allModels.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925b9a9-4224-487b-97cd-0087dfd8fee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Statistical model comparison\n",
    "compare_models(trials_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c07c7-5f3c-450f-8070-73555d651a2b",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b6a34-2701-45fc-8c13-609528f23b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'WV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb239e3-9563-4d94-9577-09db6be36ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_boxplot_onemodel(trials_results[trials_results['classifier']==selected_model],\n",
    "                           ['test_Accuracy','test_Sensitivity','test_Specificity','test_F1','test_ROC_AUC'],\n",
    "                           True,'Ensemble_models/Boxplot_'+selected_model+'.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
