{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec552f5-84dc-4c3b-92cf-20ab2db4716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, cross_val_score, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eecec0-c4d2-4694-97b8-d2e3837b3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the display option for wide dataframes\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99fc71-1b78-42f0-b192-405dad584591",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set variables\n",
    "N = 10 # experiment repetitions\n",
    "k = 4  # k for k-fold cross-validation in hyperparameter tuning\n",
    "seed = 42  # seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a36f1e-f4a1-4c88-87f4-2a284a952b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize variables to store tuned hyperparameters and results\n",
    "trials_params, trials_results = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d4075-70fe-4ef8-97ba-9784ce328e84",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c500f55c-a9dc-4c55-a9c3-e4911d3d18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read list of selected patients\n",
    "f = open('Data/selected_patients.txt', 'r')\n",
    "patient_IDs = []\n",
    "for x in f:\n",
    "  patient_IDs.append(int(x.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a1978-39db-4533-9792-65e7787c3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load clinical data\n",
    "data = pd.read_csv('Data/clinical_data.csv', header=0, index_col=0, delimiter=';')\n",
    "data = data.loc[patient_IDs]  # Sort the clinical data DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9abd30-7bbf-4eb7-84a8-8dcb879745c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate binary labels\n",
    "diagnosis = data['vx-diagnosis']\n",
    "labels = np.zeros(len(patient_IDs))\n",
    "labels[diagnosis == 'Sick'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0baa5ea-b99d-49c8-9c7b-b25ea41b4278",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2d7c5-c7ef-40ac-994c-7d2470347809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Select features\n",
    "selected_features = ['age_at_visit', 'vx-PH-Eating habits', 'vx-PH-Cancer family?', 'vx-MH-Radiotherapy', 'vx-MH-Use of hormone replacement?',\n",
    "                     'age_at_menarche', 'age_at_menopause', 'diabetes', 'nodules']\n",
    "data = data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996e160-bac1-4c9a-99b0-4f104725ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min-max normalization\n",
    "M = data.max().values\n",
    "M[M<1] = 1\n",
    "m = data.min().values\n",
    "\n",
    "data = (data-m)/(M-m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8f9a9-2f88-4981-bcb8-cd0760ade051",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to numpy array\n",
    "data = np.asarray(data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2e8e9-1e03-4943-88b8-fc25c275a103",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b64481-415a-460f-bc2b-318f3fd62112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_error(y_test, y_test_pred):\n",
    "    WE = 20\n",
    "\n",
    "    # Number of elements for which y_test > y_test_pred, i.e., y_test = 1 and y_test_pred = 0 (FN)\n",
    "    fn = np.sum(np.greater(y_test, y_test_pred))\n",
    "\n",
    "    # Number of elements for which y_test < y_test_pred, i.e., y_test = 0 and y_test_pred = 1 (FP)\n",
    "    fp = np.sum(np.less(y_test, y_test_pred))\n",
    "\n",
    "    return fn*WE + fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c35699-ae6a-4a2c-91c5-b2f8a086d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(preds_raw, ground_truth):\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, log_loss, roc_auc_score #, accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "    \n",
    "    # BCE loss\n",
    "    bce = log_loss(ground_truth, preds_raw)\n",
    "\n",
    "    # Confusion matrix\n",
    "    TN, FP, FN, TP = confusion_matrix(ground_truth, np.round(preds_raw)).ravel()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)  # accuracy = accuracy_score(ground_truth, np.round(preds_raw))\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    #gmean = np.sqrt((TP/(TP+FN))*(TN/(TN+FP)))\n",
    "    precision = TP / (TP + FP)  # precision = precision_score(ground_truth, np.round(preds_raw))\n",
    "    F1 = 2 * TP / (2*TP + FP + FN)  # F1 = f1_score(ground_truth, np.round(preds_raw))\n",
    "    \n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(ground_truth, preds_raw)\n",
    "    \n",
    "    # Weighted error    \n",
    "    we = weighted_error(ground_truth, np.round(preds_raw))\n",
    "    \n",
    "    results = {\n",
    "        'BCELoss':bce,\n",
    "        'Accuracy':accuracy,\n",
    "        'TP':TP,\n",
    "        'FP':FP,\n",
    "        'TN':TN,\n",
    "        'FN':FN,\n",
    "        'Sensitivity':sensitivity,\n",
    "        'Specificity':specificity,\n",
    "        #'G-mean':gmean,\n",
    "        'Precision':precision,\n",
    "        'Recall':sensitivity,\n",
    "        'F1':F1,\n",
    "        'ROC_AUC':auc,\n",
    "        'WE':we\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd042c0-2e2b-4088-a502-aba89686efdf",
   "metadata": {},
   "source": [
    "# Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866893a-b111-4455-b301-b34fe4e39634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9dcee6-caf7-49f8-ae3a-5f2966a50265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the splitter to split the dataset into 85% train and 15% tests sets N times \n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026e932-6e34-4c7d-879b-d3cccc2d24f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial, (train_index, test_index) in enumerate(splitter.split(data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset into train (85%) and test (15%) sets\n",
    "    crossval_data = data[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    test_data = data[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Calculate the weight for training to address class imbalance\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    \n",
    "    ## Use training set for k-fold cross-validation to tune the hyperparameters\n",
    "    ti = time.time()\n",
    "    estimator = DecisionTreeClassifier(class_weight={0: 1, 1: rate_train})\n",
    "    param_grid = {'ccp_alpha' : np.arange(0, 0.1, 0.01), # Complexity parameter used for Minimal Cost-Complexity Pruning. Default: ccp_alpha = 0.0\n",
    "                  'criterion': ['gini','entropy'],#'log_loss'], # The function to measure the quality of a split. Default: criterion='gini'\n",
    "                  'max_depth' : [None, 1, 5, 10, 15], # The maximum depth of the tree. Default: max_depth=None\n",
    "                  #'max_features': [None, 'sqrt', 'log2'], # The number of features to consider when looking for the best split. Default: max_features=None\n",
    "                  'max_leaf_nodes': [None, 3, 6, 9], # Grow a tree with max_leaf_nodes in best-first fashion. Default: None\n",
    "                  'min_samples_leaf': [1, 2, 3, 4], # The minimum number of samples required to be at a leaf node. Default: min_samples_leaf=1\n",
    "                  'min_samples_split' : [2, 5, 10, 15], # The minimum number of samples required to split an internal node. Default: min_samples_split=2\n",
    "                  'min_weight_fraction_leaf' : np.arange(0.0, 0.5, 0.05), # The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Default: min_weight_fraction_leaf=0\n",
    "                  #'splitter': ['best','random'] # The strategy used to choose the split at each node. Default: splitter='best'\n",
    "                 }\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring='roc_auc', cv=k)\n",
    "    classifier.fit(crossval_data, crossval_labels)\n",
    "\n",
    "    parameters = classifier.best_params_  # Parameter setting that gave the best results on the hold out data\n",
    "    classifier = classifier.best_estimator_  # Estimator which gave highest score on the left out data\n",
    "    \n",
    "    train_time = time.time() - ti\n",
    "    hours, remainder = divmod(train_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    ## Store tuned hyperparameters\n",
    "    trials_params.append({**{'classifier':'DT'}, **{'trial':trial+1}, **parameters})\n",
    "\n",
    "    ## Save the trained model\n",
    "    with open('Models/DT_'+str(trial+1)+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "\n",
    "    ## Predict\n",
    "    predictions_train = classifier.predict_proba(crossval_data)[:,1]\n",
    "    np.save('Predictions/DT_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "    predictions_test = classifier.predict_proba(test_data)[:,1]\n",
    "    np.save('Predictions/DT_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "    ## Print the number of parameters in the model\n",
    "    num_params = classifier.tree_.node_count\n",
    "    print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "    ## Evaluate the model\n",
    "    results_train = evaluate_model(predictions_train, crossval_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "            \n",
    "    results_test = evaluate_model(predictions_test, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append(\n",
    "        {'classifier':'DT',\n",
    "         'trial':trial+1,\n",
    "         'parameters':num_params,\n",
    "         'trainTime':train_time,\n",
    "         **{'train_'+k:v for k,v in results_train.items()},\n",
    "         **{'test_'+k:v for k,v in results_test.items()}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(), print(100*'#'), print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca393406-1f45-4224-9c53-d1089b9de48d",
   "metadata": {},
   "source": [
    "# Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25547a6f-fb12-429e-a0f8-816dd2128cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ed0a7-b791-49fb-ab0d-4460a6895cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the splitter to split the dataset into 85% train and 15% tests sets N times \n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4f053-20ce-4c15-bc3a-48214a67e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial, (train_index, test_index) in enumerate(splitter.split(data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset into train (85%) and test (15%) sets\n",
    "    crossval_data = data[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    test_data = data[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Calculate the weight for training to address class imbalance\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    \n",
    "    ## Use training set for k-fold cross-validation to tune the hyperparameters\n",
    "    ti = time.time()\n",
    "    estimator = RandomForestClassifier(class_weight={0: 1, 1: rate_train})\n",
    "    param_grid = {'n_estimators': np.arange(50, 225, 25),  # Number of trees in random forest. Default: n_estimators=100\n",
    "                  'criterion': ['gini','entropy'],#'log_loss'], # The function to measure the quality of a split. Default: criterion='gini'\n",
    "                  'max_features': ['sqrt', 'log2'],  # Number of features to consider at every split. Default: max_features='sqrt'\n",
    "                  'max_depth': list(np.arange(10, 110, 10))+[None],  # Maximum number of levels in tree. Default: max_depth=None\n",
    "                  'min_samples_split': [2, 3, 5, 10],  # Minimum number of samples required to split a node. Default: min_samples_split=2\n",
    "                  'min_samples_leaf': [1, 2, 3, 4],  # Minimum number of samples required at each leaf node. Default: min_samples_leaf=1\n",
    "                  #'bootstrap': [True, False]  # Method of selecting samples for training each tree. Default: bootstrap=True\n",
    "                 }\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring='roc_auc', cv=k)\n",
    "    classifier.fit(crossval_data, crossval_labels)\n",
    "\n",
    "    parameters = classifier.best_params_  # Parameter setting that gave the best results on the hold out data\n",
    "    classifier = classifier.best_estimator_  # Estimator which gave highest score on the left out data\n",
    "    \n",
    "    train_time = time.time() - ti\n",
    "    hours, remainder = divmod(train_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    ## Store tuned hyperparameters\n",
    "    trials_params.append({**{'classifier':'RF'}, **{'trial':trial+1}, **parameters})\n",
    "\n",
    "    ## Save the trained model\n",
    "    with open('Models/RF_'+str(trial+1)+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "\n",
    "    ## Predict\n",
    "    predictions_train = classifier.predict_proba(crossval_data)[:,1]\n",
    "    np.save('Predictions/RF_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "    predictions_test = classifier.predict_proba(test_data)[:,1]\n",
    "    np.save('Predictions/RF_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "    ## Print the number of parameters in the model\n",
    "    num_params = sum([tree.tree_.node_count for tree in classifier.estimators_])\n",
    "    print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "    ## Evaluate the model\n",
    "    results_train = evaluate_model(predictions_train, crossval_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "            \n",
    "    results_test = evaluate_model(predictions_test, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append(\n",
    "        {'classifier':'RF',\n",
    "         'trial':trial+1,\n",
    "         'parameters':num_params,\n",
    "         'trainTime':train_time,\n",
    "         **{'train_'+k:v for k,v in results_train.items()},\n",
    "         **{'test_'+k:v for k,v in results_test.items()}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(), print(100*'#'), print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d01541-614a-4326-ac7d-1709c831f487",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca0d15-33ff-4fcb-866c-e606b92cf583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ccc21-e157-4b09-a6d1-f2cac6d7f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the splitter to split the dataset into 85% train and 15% tests sets N times \n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6bfc5-5790-4c24-b62c-c6ed34ba0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial, (train_index, test_index) in enumerate(splitter.split(data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset into train (85%) and test (15%) sets\n",
    "    crossval_data = data[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    test_data = data[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Calculate the weight for training to address class imbalance\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    \n",
    "    ## Use training set for k-fold cross-validation to tune the hyperparameters\n",
    "    ti = time.time()\n",
    "    estimator = SVC(class_weight={0: 1, 1: rate_train}, probability=True)\n",
    "    param_grid = {'C': [1,10,100,1000], # Regularization parameter. Default: C=1.0\n",
    "                  'kernel': ['linear', 'rbf', 'sigmoid', 'poly'], # Default: kernel='rbf'\n",
    "                  'gamma': ['scale', 'auto', 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001] # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Default: gamma='scale'\n",
    "                 }\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring='roc_auc', cv=k)\n",
    "    classifier.fit(crossval_data, crossval_labels)\n",
    "\n",
    "    parameters = classifier.best_params_  # Parameter setting that gave the best results on the hold out data\n",
    "    classifier = classifier.best_estimator_  # Estimator which gave highest score on the left out data\n",
    "    \n",
    "    train_time = time.time() - ti\n",
    "    hours, remainder = divmod(train_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    ## Store tuned hyperparameters\n",
    "    trials_params.append({**{'classifier':'SVM'}, **{'trial':trial+1}, **parameters})\n",
    "\n",
    "    ## Save the trained model\n",
    "    with open('Models/SVM_'+str(trial+1)+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "\n",
    "    ## Predict\n",
    "    predictions_train = classifier.predict_proba(crossval_data)[:,1]\n",
    "    np.save('Predictions/SVM_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "    predictions_test = classifier.predict_proba(test_data)[:,1]\n",
    "    np.save('Predictions/SVM_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "    ## Print the number of parameters in the model\n",
    "    n_support_vectors = len(classifier.support_vectors_)\n",
    "    n_coefficients = len(classifier.dual_coef_[0])\n",
    "    num_params = n_support_vectors + n_coefficients\n",
    "    print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "    ## Evaluate the model\n",
    "    results_train = evaluate_model(predictions_train, crossval_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "            \n",
    "    results_test = evaluate_model(predictions_test, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append(\n",
    "        {'classifier':'SVM',\n",
    "         'trial':trial+1,\n",
    "         'parameters':num_params,\n",
    "         'trainTime':train_time,\n",
    "         **{'train_'+k:v for k,v in results_train.items()},\n",
    "         **{'test_'+k:v for k,v in results_test.items()}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(), print(100*'#'), print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617014e5-e221-48a9-b2a5-85052f2eddcb",
   "metadata": {},
   "source": [
    "# AdaBoost with DT base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad0c9a-56dc-4c30-b9cd-5caac83392de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9add56-0eeb-4ef1-838c-4cee8b2da4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the splitter to split the dataset into 85% train and 15% tests sets N times \n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ca425-3605-451e-8cb9-0c4049518efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial, (train_index, test_index) in enumerate(splitter.split(data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset into train (85%) and test (15%) sets\n",
    "    crossval_data = data[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    test_data = data[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Calculate the weight for training to address class imbalance\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    \n",
    "    ## Use training set for k-fold cross-validation to tune the hyperparameters\n",
    "    ti = time.time()\n",
    "    base_estimator = DecisionTreeClassifier(class_weight={0: 1, 1: rate_train})\n",
    "    estimator = AdaBoostClassifier(base_estimator)\n",
    "    param_grid = {'n_estimators': np.arange(10, 110, 10), # The maximum number of estimators at which boosting is terminated. Default: n_estimators=50\n",
    "                  'learning_rate': [0.01, 0.1, 0.5, 1.0], # Weight applied to each classifier at each boosting iteration. Default: learning_rate=1.0\n",
    "                      \n",
    "                  ## Decision Tree parameters\n",
    "                  #'base_estimator__ccp_alpha' : np.arange(0, 0.1, 0.01), # Complexity parameter used for Minimal Cost-Complexity Pruning. Default: ccp_alpha = 0.0\n",
    "                  #'base_estimator__criterion': ['gini','entropy'],#'log_loss'], # The function to measure the quality of a split. Default: criterion='gini'\n",
    "                  #'base_estimator__max_depth' : [None, 1, 5, 10, 15], # The maximum depth of the tree. Default: max_depth=None\n",
    "                  ##'base_estimator__max_features': [None, 'sqrt', 'log2'], # The number of features to consider when looking for the best split. Default: max_features=None\n",
    "                  #'base_estimator__max_leaf_nodes': [None, 3, 6, 9], # Grow a tree with max_leaf_nodes in best-first fashion. Default: None\n",
    "                  #'base_estimator__min_samples_leaf': [1, 2, 3, 4], # The minimum number of samples required to be at a leaf node. Default: min_samples_leaf=1\n",
    "                  #'base_estimator__min_samples_split' : [2, 5, 10, 15], # The minimum number of samples required to split an internal node. Default: min_samples_split=2\n",
    "                  #'base_estimator__min_weight_fraction_leaf' : np.arange(0.0, 0.5, 0.05), # The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Default: min_weight_fraction_leaf=0\n",
    "                  ##'base_estimator__splitter': ['best','random'] # The strategy used to choose the split at each node. Default: splitter='best'\n",
    "                 }\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring='roc_auc', cv=k)\n",
    "    classifier.fit(crossval_data, crossval_labels)\n",
    "\n",
    "    parameters = classifier.best_params_  # Parameter setting that gave the best results on the hold out data\n",
    "    classifier = classifier.best_estimator_  # Estimator which gave highest score on the left out data\n",
    "    \n",
    "    train_time = time.time() - ti\n",
    "    hours, remainder = divmod(train_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    ## Store tuned hyperparameters\n",
    "    trials_params.append({**{'classifier':'DT_AdaBoost'}, **{'trial':trial+1}, **parameters})\n",
    "\n",
    "    ## Save the trained model\n",
    "    with open('Models/DT_AdaBoost_'+str(trial+1)+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "\n",
    "    ## Predict\n",
    "    predictions_train = classifier.predict_proba(crossval_data)[:,1]\n",
    "    np.save('Predictions/DT_AdaBoost_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "    predictions_test = classifier.predict_proba(test_data)[:,1]\n",
    "    np.save('Predictions/DT_AdaBoost_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "    ## Print the number of parameters in the model\n",
    "    num_params = sum(estimator.tree_.node_count for estimator in classifier.estimators_)\n",
    "    print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "    ## Evaluate the model\n",
    "    results_train = evaluate_model(predictions_train, crossval_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "            \n",
    "    results_test = evaluate_model(predictions_test, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append(\n",
    "        {'classifier':'DT_AdaBoost',\n",
    "         'trial':trial+1,\n",
    "         'parameters':num_params,\n",
    "         'trainTime':train_time,\n",
    "         **{'train_'+k:v for k,v in results_train.items()},\n",
    "         **{'test_'+k:v for k,v in results_test.items()}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(), print(100*'#'), print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca9bfe-a832-482b-842f-864d09aa42d3",
   "metadata": {},
   "source": [
    "# AdaBoost with RF base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e3489-7b54-4e45-8964-ecb79bc875ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26ff27-f8b7-4810-8a91-39cf9bfa35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the splitter to split the dataset into 85% train and 15% tests sets N times \n",
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c86de1-9215-4e54-a8d7-5d0406300211",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial, (train_index, test_index) in enumerate(splitter.split(data, labels)):\n",
    "    \n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset into train (85%) and test (15%) sets\n",
    "    crossval_data = data[train_index]\n",
    "    crossval_labels = labels[train_index]\n",
    "    test_data = data[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Calculate the weight for training to address class imbalance\n",
    "    n_sick = labels.sum()\n",
    "    n_healthy = len(labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    \n",
    "    ## Use training set for k-fold cross-validation to tune the hyperparameters\n",
    "    ti = time.time()\n",
    "    base_estimator = RandomForestClassifier(class_weight={0: 1, 1: rate_train})\n",
    "    estimator = AdaBoostClassifier(base_estimator)\n",
    "    param_grid = {'n_estimators': np.arange(10, 110, 10), # The maximum number of estimators at which boosting is terminated. Default: n_estimators=50\n",
    "                  'learning_rate': [0.01, 0.1, 0.2, 0.5], # Weight applied to each classifier at each boosting iteration. Default: learning_rate=1.0\n",
    "                      \n",
    "                  ## Random Forest parameters\n",
    "                  #'base_estimator__n_estimators': np.arange(50, 225, 25),  # Number of trees in random forest. Default: n_estimators=100\n",
    "                  #'base_estimator__criterion': ['gini','entropy'],#'log_loss'], # The function to measure the quality of a split. Default: criterion='gini'\n",
    "                  #'base_estimator__max_features': ['sqrt', 'log2'],  # Number of features to consider at every split. Default: max_features='sqrt'\n",
    "                  #'base_estimator__max_depth': list(np.arange(10, 110, 10))+['None'],  # Maximum number of levels in tree. Default: max_depth=None\n",
    "                  #'base_estimator__min_samples_split': [2, 3, 5, 10],  # Minimum number of samples required to split a node. Default: min_samples_split=2\n",
    "                  #'base_estimator__min_samples_leaf': [1, 2, 3, 4],  # Minimum number of samples required at each leaf node. Default: min_samples_leaf=1\n",
    "                  ##'base_estimator__bootstrap': [True, False]  # Method of selecting samples for training each tree. Default: bootstrap=True\n",
    "                 }\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring='roc_auc', cv=k)\n",
    "    classifier.fit(crossval_data, crossval_labels)\n",
    "\n",
    "    parameters = classifier.best_params_  # Parameter setting that gave the best results on the hold out data\n",
    "    classifier = classifier.best_estimator_  # Estimator which gave highest score on the left out data\n",
    "    \n",
    "    train_time = time.time() - ti\n",
    "    hours, remainder = divmod(train_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    ## Store tuned hyperparameters\n",
    "    trials_params.append({**{'classifier':'RF_AdaBoost'}, **{'trial':trial+1}, **parameters})\n",
    "\n",
    "    ## Save the trained model\n",
    "    with open('Models/RF_AdaBoost_'+str(trial+1)+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "\n",
    "    ## Predict\n",
    "    predictions_train = classifier.predict_proba(crossval_data)[:,1]\n",
    "    np.save('Predictions/RF_AdaBoost_train_'+str(trial+1)+'.npy',predictions_train)\n",
    "    predictions_test = classifier.predict_proba(test_data)[:,1]\n",
    "    np.save('Predictions/RF_AdaBoost_test_'+str(trial+1)+'.npy',predictions_test)\n",
    "\n",
    "    ## Print the number of parameters in the model\n",
    "    n_estimators = len(classifier.estimators_)\n",
    "    num_params = sum(\n",
    "        sum(tree.tree_.node_count for tree in estimator.estimators_)\n",
    "        for estimator in classifier.estimators_\n",
    "    )\n",
    "    print(f'Classifier has {num_params} parameters.'), print()\n",
    "        \n",
    "    ## Evaluate the model\n",
    "    results_train = evaluate_model(predictions_train, crossval_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "            \n",
    "    results_test = evaluate_model(predictions_test, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append(\n",
    "        {'classifier':'RF_AdaBoost',\n",
    "         'trial':trial+1,\n",
    "         'parameters':num_params,\n",
    "         'trainTime':train_time,\n",
    "         **{'train_'+k:v for k,v in results_train.items()},\n",
    "         **{'test_'+k:v for k,v in results_test.items()}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(), print(100*'#'), print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21b62a-c07c-4974-a5f9-79300d6ae691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f837f79-fe32-47a8-80aa-7387141c1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store tuned hyperparameters and results\n",
    "pd.DataFrame(trials_params).to_csv('CD_Parameters_'+str(N)+'trials.csv')\n",
    "pd.DataFrame(trials_results).round(decimals=5).to_csv('CD_Results_'+str(N)+'trials.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f603efbf-07d8-4e38-a566-a7d88b6f64ed",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd86b0-61fb-4a38-abef-c42334632806",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_results = pd.DataFrame(trials_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4bd57-f3e9-4c99-89d2-4d1dcd242ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print statistics\n",
    "models = trials_results.classifier.unique()\n",
    "metrics = [c for c in trials_results.columns if 'test_' in c and c not in ['test_TP','test_FP','test_TN','test_FN']]\n",
    "    \n",
    "statistics = pd.DataFrame(index=models, columns=[item for sublist in [[metric+'_mean', metric+'_std'] for metric in metrics] for item in sublist])\n",
    "    \n",
    "for metric in metrics:\n",
    "    mn, st = metric+'_mean', metric+'_std'\n",
    "    for model in models:\n",
    "        results = trials_results[trials_results['classifier']==model][metric].values\n",
    "        statistics.at[model,mn] = results.mean()\n",
    "        statistics.at[model,st] = results.std()\n",
    "\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2b00a-860b-4b09-810d-b0ce4fc006cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [m for m in metrics if 'test' in m]:\n",
    "    mn, st = metric+'_mean', metric+'_std'\n",
    "    if 'Loss' in metric or 'WE' in metric:\n",
    "        model_best = pd.to_numeric(statistics[metric+'_mean']).idxmin()\n",
    "        print(f'Model with lowest {metric} is {model_best} with value {statistics.loc[model_best,mn]} and standard deviation {statistics.loc[model_best,st]}')\n",
    "    else:\n",
    "        model_best = pd.to_numeric(statistics[metric+'_mean']).idxmax()\n",
    "        print(f'Model with highest {metric} is {model_best} with value {statistics.loc[model_best,mn]} and standard deviation {statistics.loc[model_best,st]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c42d84-6dce-49ad-8ba8-1d3e1f395117",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print mean and std metrics for each model\n",
    "for classifier_type in trials_results.classifier.unique():\n",
    "    print(f'Classifier: {classifier_type}')\n",
    "    results = trials_results[trials_results['classifier'] == classifier_type]\n",
    "\n",
    "    # Number of parameters\n",
    "    parameters = results['parameters'].values\n",
    "    print(f'Mean number of parameters: {parameters.mean()} [{parameters.min()}, {parameters.max()}], std {parameters.std()}')\n",
    "\n",
    "    # training time\n",
    "    trainTime = results['trainTime'].values\n",
    "    hours, remainder = divmod(trainTime.mean(), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Mean training time: {hours} hours, {minutes} minutes, and {seconds} seconds, (std {trainTime.std()} sec)')\n",
    "    print()\n",
    "    \n",
    "    # TRAIN results\n",
    "    metrics = ['BCELoss','Accuracy','Sensitivity','Specificity','ROC_AUC','Precision','F1','WE']\n",
    "    for metric in metrics:\n",
    "        values = results['train_' + metric].values\n",
    "        print(f'Mean train {metric}: {values.mean()}, std {values.std()}')\n",
    "    print()\n",
    "\n",
    "    # TEST results\n",
    "    for metric in metrics:\n",
    "        values = results['test_' + metric].values\n",
    "        print(f'Mean test {metric}: {values.mean()}, std {values.std()}')\n",
    "    print()\n",
    "\n",
    "    print('-'*120), print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355825b-780a-47e3-a1df-7f7f0902fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show boxplots\n",
    "compared_metrics = ['test_BCELoss','test_Accuracy','test_F1','test_ROC_AUC','test_WE']  #[c for c in cd_trials_results.columns if 'test_' in c and c not in ['test_TP','test_FP','test_TN','test_FN','test_WE','test_Loss']]\n",
    "M = len(compared_metrics)\n",
    "R = M//2 + int(M % 2 > 0)\n",
    "plt.figure(figsize=(5*R, 20))\n",
    "for i,metric in enumerate(compared_metrics):\n",
    "    plt.subplot(R,2,i+1)\n",
    "    sns.boxplot(x='classifier', y=metric, data=trials_results, palette='Set2')\n",
    "    plt.xticks(rotation=25, ha='right')  # Rotate labels 25 degrees\n",
    "    plt.grid(axis='y')\n",
    "    plt.xlabel('')\n",
    "    if 'Loss' in metric or 'WE' in metric:\n",
    "        plt.ylim([0,trials_results[metric].values.max()+0.05*trials_results[metric].values.max()])\n",
    "    else:\n",
    "        plt.ylim([0,1])\n",
    "    plt.title(metric, fontsize=16)\n",
    "    #plt.tight_layout()\n",
    "    \n",
    "plt.savefig('CD_Boxplots_allModels.png')\n",
    "plt.subplots_adjust(hspace=0.3)  # Increase hspace for more vertical spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f2e9c-eefa-4849-a502-99495662ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistical model comparison\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import probplot\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "    \n",
    "models = trials_results.classifier.unique()\n",
    "compared_metrics = ['test_BCELoss','test_Accuracy','test_F1','test_ROC_AUC','test_WE']\n",
    "    \n",
    "## 1. Select independent metrics\n",
    "#These should be independent. Use the correlation to find dependent variables to remove, if necessary \n",
    "print('Check that the metrics are independent:')\n",
    "print(trials_results[compared_metrics].corr()), print()  # Check for multicollinearity using the correlation matrix\n",
    "\n",
    "var = trials_results[compared_metrics].var().to_numpy()\n",
    "compared_metrics = [metric for metric,v in zip(compared_metrics,var) if not v == 0]  # Remove metrics with zero variance across repetitions\n",
    "\n",
    "## 2. Check for Normality\n",
    "normality = np.ones(len(compared_metrics))\n",
    "L = len(models)//5 + (len(models) % 5 > 0)\n",
    "for i,metric in enumerate(compared_metrics): \n",
    "    print(metric)\n",
    "    ## Check for normality\n",
    "    plt.figure(figsize=(20,2*L))\n",
    "    for j,model in enumerate(models):\n",
    "        # Shapiro-Wilk test\n",
    "        stat, p_value = shapiro(trials_results[trials_results['classifier']==model][metric])\n",
    "        print(f'{model}: Shapiro-Wilk p-value={p_value}')\n",
    "        # If p-value < 0.05, reject normality (non-normal distribution)\n",
    "        normality[i] *= int(p_value > 0.05)\n",
    "\n",
    "        # Generate Q-Q plots to visually inspect normality \n",
    "        plt.subplot(L,5,j+1)\n",
    "        probplot(trials_results[trials_results['classifier']==model][metric], dist='norm', plot=plt)\n",
    "        plt.title(model)\n",
    "    plt.show()\n",
    "    print(f'\\nNormality test for {metric}: {bool(normality[i])}')\n",
    "    print(100*'-')\n",
    "print(), print()\n",
    "    \n",
    "    \n",
    "## 3. Statistical test: MANOVA\n",
    "dependent_variables = ' + '.join(compared_metrics)\n",
    "formula = f'{dependent_variables} ~ classifier'\n",
    "manova = MANOVA.from_formula(formula, data=trials_results)\n",
    "manova_test = manova.mv_test()\n",
    "print(manova_test)\n",
    "\n",
    "## 4. Post-hoc test: Tukey's HSD\n",
    "if manova_test.results['classifier']['stat']['Pr > F']['Pillai\\'s trace'] < 0.05:\n",
    "    for metric in compared_metrics:\n",
    "        print(metric)\n",
    "        tukey = pairwise_tukeyhsd(trials_results[metric],   # Metric data\n",
    "                                          trials_results['classifier'],   # Grouping variable (model)\n",
    "                                          alpha=0.05)   # Significance level\n",
    "        print(tukey)\n",
    "else:\n",
    "    print(f\"MANOVA is not significant, no need to apply Tukey's test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9961d-a1b5-494b-aad0-389d3ee6521d",
   "metadata": {},
   "source": [
    "# Selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c705d-dc96-4131-a501-48352a0f59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'RF'\n",
    "results_selected = trials_results[trials_results['classifier']==selected_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159b4b7-cbaf-4353-b84f-967b5295696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show boxplots\n",
    "df_long = pd.melt(results_selected[compared_metrics], value_vars=compared_metrics, var_name='metric', value_name='value')\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(y='value',x='metric',data=df_long, palette='Set2', fliersize=5)\n",
    "new_labels = [l.replace('test_','') for l in compared_metrics]\n",
    "plt.xticks(ticks=range(len(new_labels)), labels=new_labels, fontsize=18, rotation=25, ha='right')\n",
    "plt.yticks(fontsize=18)\n",
    "plt.grid(axis='y')\n",
    "plt.xlabel('', fontsize=18)\n",
    "plt.ylabel('', fontsize=18)\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.savefig('CD_Boxplot_'+selected_model+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e23f8a-410e-495d-8747-5065d4cd6837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
