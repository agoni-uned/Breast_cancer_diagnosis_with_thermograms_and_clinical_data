{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e0b5a-38e1-4e1d-afd1-2b6ed47c9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow.keras.backend as K\n",
    "import gc\n",
    "from hyperopt import fmin, tpe, hp, space_eval, Trials, STATUS_OK #, rand\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from Utils import WeightedError, Specificity, evaluate_model_skl, store_results, visualize_boxplots, visualize_boxplot_onemodel, compare_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9d8ff-3294-408c-86ad-d0d7952cea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deacab8f-b238-4986-98ee-94de13bde074",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10  # experiment repetitions\n",
    "seed = 42\n",
    "epochs = 30\n",
    "batch_size = 8\n",
    "\n",
    "k = 5  # k for k-fold cross-validation in hyperparameter tuning\n",
    "seed_tuning = 13\n",
    "max_evals = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c4018-0003-416c-81d2-987c3139019e",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d6dfa-d8ef-4f6a-b1a3-2ed42460f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "## Frontal images\n",
    "front_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/Front',f))) for f in os.listdir('Images/Healthy/Front')])\n",
    "front_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/Front',f))) for f in os.listdir('Images/Sick/Front')])\n",
    "front = np.concatenate((front_images_h, front_images_s))\n",
    "\n",
    "## Left lateral (L90) images\n",
    "L90_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/L90',f))) for f in os.listdir('Images/Healthy/L90')])\n",
    "L90_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/L90',f))) for f in os.listdir('Images/Sick/L90')])\n",
    "L90 = np.concatenate((L90_images_h, L90_images_s))\n",
    "\n",
    "## Right lateral (R90) images\n",
    "R90_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/R90',f))) for f in os.listdir('Images/Healthy/R90')])\n",
    "R90_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/R90',f))) for f in os.listdir('Images/Sick/R90')])\n",
    "R90 = np.concatenate((R90_images_h, R90_images_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb55b3a-ac2c-4235-8135-03835ce2b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shape of thermograms\n",
    "_,h,w = front.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a24a5-09cb-4654-9cce-5902e2771aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate labels\n",
    "labels_h = [0]*len(front_images_h)\n",
    "labels_s = [1]*len(front_images_s)\n",
    "labels = np.concatenate((labels_h, labels_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1605ca4-deaa-47b5-ade8-435598f87243",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f930cce-812f-4508-b9d5-3031217e93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min-max normalization\n",
    "M = np.concatenate((front, L90, R90)).max()\n",
    "m = np.concatenate((front, L90, R90))).min()\n",
    "\n",
    "front = ((front - m) / (M - m)).astype('float32')\n",
    "L90 = ((L90 - m) / (M - m)).astype('float32')\n",
    "R90 = ((R90 - m) / (M - m)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9da84-8ee2-4fdf-b90b-72b31654ca49",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef13644-06f9-447c-9260-fb0fb931cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "class NaNCheckCallback(Callback):\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if np.isnan(logs.get('loss')):\n",
    "            print(f'NaN value encountered at batch {batch}')\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04de1c-e882-470c-8dea-e8423d4c22da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get the data for hyperparameter tuning\n",
    "def data():    \n",
    "    ## Frontal images\n",
    "    front_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/Front',f))) for f in os.listdir('Images/Healthy/Front')])\n",
    "    front_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/Front',f))) for f in os.listdir('Images/Sick/Front')])\n",
    "    front = np.concatenate((front_images_h, front_images_s))\n",
    "\n",
    "    ## Left lateral (L90) images\n",
    "    L90_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/L90',f))) for f in os.listdir('Images/Healthy/L90')])\n",
    "    L90_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/L90',f))) for f in os.listdir('Images/Sick/L90')])\n",
    "    L90 = np.concatenate((L90_images_h, L90_images_s))\n",
    "\n",
    "    ## Right lateral (R90) images\n",
    "    R90_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/R90',f))) for f in os.listdir('Images/Healthy/R90')])\n",
    "    R90_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/R90',f))) for f in os.listdir('Images/Sick/R90')])\n",
    "    R90 = np.concatenate((R90_images_h, R90_images_s))\n",
    "    \n",
    "    ## Labels\n",
    "    labels_h = [0]*len(front_images_h)\n",
    "    labels_s = [1]*len(front_images_s)\n",
    "    labels = np.concatenate((labels_h, labels_s))\n",
    "\n",
    "    ## Split the dataset into crossval and test sets\n",
    "    seed_tuning = 13  # Seed for hyperparameter tuning\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=int(round(0.15*len(labels))), random_state = seed_tuning)\n",
    "    crossval_index, test_index = [[crossval_index, test_index] for crossval_index, test_index in splitter.split(front, labels)][0]\n",
    "    \n",
    "    front_crossval = front[crossval_index]\n",
    "    L90_crossval = L90[crossval_index]\n",
    "    R90_crossval = R90[crossval_index]\n",
    "    labels_crossval = labels[crossval_index]\n",
    "\n",
    "    #front_test = front[test_index]\n",
    "    #L90_test = L90[test_index]\n",
    "    #R90_tets = R90[test_index]\n",
    "    #labels_test = labels[test_index]\n",
    "    \n",
    "    ## add the channels dimension\n",
    "    front_crossval = np.expand_dims(front_crossval,-1)\n",
    "    L90_crossval = np.expand_dims(L90_crossval,-1)\n",
    "    R90_crossval = np.expand_dims(R90_crossval,-1)\n",
    "\n",
    "    #front_test = np.expand_dims(front_test,-1)\n",
    "    #L90_test = np.expand_dims(L90_test,-1)\n",
    "    #R90_tets = np.expand_dims(R90_tets,-1)\n",
    "\n",
    "    return front_crossval, L90_crossval, R90_crossval, labels_crossval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4bbcc-a9d3-4326-9eee-85127695fa7e",
   "metadata": {},
   "source": [
    "# Custom CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc904b3e-92db-4372-9083-d7059f93c0fb",
   "metadata": {},
   "source": [
    "## Tune hyperparameters (model architecture and learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74585da2-f278-4101-803d-13532f8d87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to check tensor size to see if another downsampling layer can be applied\n",
    "def can_apply_layer(x, min_height, min_width):\n",
    "    \"\"\"Check if the current layer can be applied based on tensor dimensions.\"\"\"\n",
    "    shape = K.int_shape(x)\n",
    "    if shape[1] is None or shape[2] is None:\n",
    "        return False  # Dynamic shape, cannot evaluate\n",
    "    return shape[1] >= min_height and shape[2] >= min_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8d371-07f8-4a8d-8cf5-84da7e5d50b4",
   "metadata": {},
   "source": [
    "### Single-input CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4957f-7fd1-4dbe-b02b-05661c619e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single-input model\n",
    "def create_singleInput_model(params, a=480, b=640):\n",
    "            \n",
    "    # Input\n",
    "    stacked_input = Input(shape=(a, b, 3))\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation='relu')(stacked_input)\n",
    "    x = Conv2D(params['conv1_filters'], kernel_size=params['conv1_kernel'], strides=params['conv1_strides'], activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=params['pool1_size'])(x)\n",
    "    \n",
    "    x = Conv2D(64, kernel_size=params['conv2_kernel'], strides=params['conv2_strides'], activation='relu')(x)\n",
    "    if params['add_conv_3']:\n",
    "        x = Conv2D(64, kernel_size=params['conv3_kernel'], activation='relu')(x)\n",
    "    if params['add_conv_4']:\n",
    "        x = Conv2D(64, kernel_size=params['conv4_kernel'], activation='relu')(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        \n",
    "    if can_apply_layer(x, 5, 5):\n",
    "        if params['add_conv_5']:\n",
    "            x = Conv2D(params['conv5_filters'], kernel_size=params['conv5_kernel'], activation='relu')(x)\n",
    "    if can_apply_layer(x, 5, 5):\n",
    "        if params['add_conv_6']:\n",
    "            x = Conv2D(params['conv6_filters'], kernel_size=params['conv6_kernel'], activation='relu')(x)\n",
    "    if can_apply_layer(x, 5, 5):\n",
    "        if params['add_conv_7']:\n",
    "            x = Conv2D(params['conv7_filters'], kernel_size=params['conv7_kernel'], activation='relu')(x)\n",
    "            if can_apply_layer(x, 2, 2):\n",
    "                x = MaxPooling2D((2, 2))(x)\n",
    "                \n",
    "    if can_apply_layer(x, 5, 5):\n",
    "        if params['add_conv_8']:\n",
    "            x = Conv2D(128, kernel_size=params['conv8_kernel'], activation='relu')(x)\n",
    "    if can_apply_layer(x, 5, 5):\n",
    "        if params['add_conv_9']:\n",
    "            x = Conv2D(128, kernel_size=params['conv9_kernel'], activation='relu')(x)\n",
    "    if can_apply_layer(x, 5, 5):\n",
    "        if params['add_conv_10']:\n",
    "            x = Conv2D(128, kernel_size=params['conv10_kernel'], activation='relu')(x)\n",
    "            if can_apply_layer(x, 2, 2):\n",
    "                x = MaxPooling2D((2, 2))(x)\n",
    "                \n",
    "    if can_apply_layer(x, 5, 5):\n",
    "        if params['add_conv_11']:\n",
    "            x = Conv2D(128, kernel_size=params['conv11_kernel'], activation='relu')(x)\n",
    "    if can_apply_layer(x, 5, 5):\n",
    "        if params['add_conv_12']:\n",
    "            x = Conv2D(128, kernel_size=params['conv12_kernel'], activation='relu')(x)\n",
    "            if can_apply_layer(x, 2, 2):\n",
    "                x = MaxPooling2D((2, 2))(x)\n",
    "        \n",
    "    if can_apply_layer(x, 10, 10):\n",
    "        if params['add_conv_13']:\n",
    "            x = Conv2D(128, kernel_size=params['conv13_kernel'], activation='relu')(x)\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "        \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(params['dense1_units'], activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x) \n",
    "    \n",
    "    if params['add_dense_2']:\n",
    "        x = Dense(params['dense2_units'], use_bias=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(0.5)(x)   #new\n",
    "\n",
    "    x = Dense(params['dense3_units'], activation='relu')(x)\n",
    "    if params['add_dense_4']:\n",
    "        x = Dense(params['dense4_units'], use_bias=False)(x)\n",
    "    X = Dense(1, activation='sigmoid')(x)   \n",
    "    \n",
    "    ## create the model\n",
    "    return Model(stacked_input, X)\n",
    "\n",
    "\n",
    "# Objective function to minimize\n",
    "def objective_singleInput(params):\n",
    "\n",
    "    # Load the data\n",
    "    front, L90, R90, labels = data()\n",
    "    _,h,w,_ = front.shape\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    epochs = 30\n",
    "    batch_size = 8\n",
    "    lr = params['lr']\n",
    "    folds = 5\n",
    "    seed = 13\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Store results from each fold\n",
    "    fold_metrics = []\n",
    "\n",
    "    # Initialize the model\n",
    "    model = create_singleInput_model(params, h, w)\n",
    "\n",
    "    # Save the initial weights of the model\n",
    "    initial_weights = model.get_weights()\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(skf.split(front, labels)):\n",
    "        \n",
    "        print(f\"Fold {i+1}/{folds}\")\n",
    "        \n",
    "        # Split the data into training and validation sets\n",
    "        train_front, val_front = front[train_idx], front[val_idx]\n",
    "        train_L90, val_L90 = L90[train_idx], L90[val_idx]\n",
    "        train_R90, val_R90 = R90[train_idx], R90[val_idx]\n",
    "        train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "        rate_train = sum(train_labels==0)/sum(train_labels)\n",
    "\n",
    "        # Stack views along the channel axis\n",
    "        train_images = np.concatenate((train_front, train_L90, train_R90), axis=-1)\n",
    "        val_images = np.concatenate((val_front, val_L90, val_R90), axis=-1)\n",
    "\n",
    "        # Reset the model to initial weights\n",
    "        model.set_weights(initial_weights)\n",
    "\n",
    "        # Train the model\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "        history  = model.fit(\n",
    "            train_images, train_labels,\n",
    "            class_weight={0: 1, 1: rate_train},\n",
    "            validation_data=(val_images, val_labels),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[NaNCheckCallback()],\n",
    "            verbose=0  # Suppress training output\n",
    "        )\n",
    "\n",
    "        # Record the best validation AUC for this fold\n",
    "        best_auc = max(history.history['val_AUC'])\n",
    "        fold_metrics.append(best_auc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Clear TensorFlow session to release GPU memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    # Return the negative mean AUC across all folds as the loss\n",
    "    print(f'Max test ROC AUC in each fold: {fold_metrics}')\n",
    "    mean_auc = np.mean(fold_metrics)\n",
    "    return -mean_auc  # Maximize AUC by minimizing its negative\n",
    "\n",
    "\n",
    "## Hyperparameter search space\n",
    "space = {\n",
    "    'lr': hp.loguniform('lr', np.log(1e-5), np.log(1e-2)),\n",
    "\n",
    "    'conv1_filters': hp.choice('conv1_filters', [32, 64]),\n",
    "    'conv1_kernel': hp.choice('conv1_kernel', [(3, 3), (5, 5)]),\n",
    "    'conv1_strides': hp.choice('conv1_strides', [1, 2]),\n",
    "    'pool1_size': hp.choice('pool1_size', [2, 3]),\n",
    "    'conv2_kernel': hp.choice('conv2_kernel', [(3, 3), (5, 5)]),\n",
    "    'conv2_strides': hp.choice('conv2_strides', [1, 2]),\n",
    "    'add_conv_3' : hp.choice('add_conv_3', [True, False]),\n",
    "    'conv3_kernel': hp.choice('conv3_kernel', [(3, 3), (5, 5)]),\n",
    "    'add_conv_4' : hp.choice('add_conv_4', [True, False]),\n",
    "    'conv4_kernel': hp.choice('conv4_kernel', [(3, 3), (5, 5)]),\n",
    "    'add_conv_5' : hp.choice('add_conv_5', [True, False]),\n",
    "    'conv5_filters': hp.choice('conv5_filters', [64, 128]),\n",
    "    'conv5_kernel': hp.choice('conv5_kernel', [(3, 3), (5, 5)]),\n",
    "    'add_conv_6' : hp.choice('add_conv_6', [True, False]),\n",
    "    'conv6_filters': hp.choice('conv6_filters', [64, 128]),\n",
    "    'conv6_kernel': hp.choice('conv6_kernel', [(3, 3), (5, 5)]),\n",
    "    'add_conv_7' : hp.choice('add_conv_7', [True, False]),\n",
    "    'conv7_filters': hp.choice('conv7_filters', [64, 128]),\n",
    "    'conv7_kernel': hp.choice('conv7_kernel', [(3, 3), (5, 5)]),\n",
    "    'add_conv_8' : hp.choice('add_conv_8', [True, False]),\n",
    "    'conv8_kernel': hp.choice('conv8_kernel', [(3, 3), (5, 5)]),\n",
    "    'add_conv_9' : hp.choice('add_conv_9', [True, False]),\n",
    "    'conv9_kernel': hp.choice('conv9_kernel', [(3, 3), (5, 5)]),\n",
    "    'add_conv_10' : hp.choice('add_conv_10', [True, False]),\n",
    "    'conv10_kernel': hp.choice('conv10_kernel', [(3, 3), (5, 5)]),\n",
    "    'add_conv_11' : hp.choice('add_conv_11', [True, False]),\n",
    "    'conv11_kernel': hp.choice('conv11_kernel', [(3, 3), (5, 5)]),\n",
    "    'add_conv_12' : hp.choice('add_conv_12', [True, False]),\n",
    "    'conv12_kernel': hp.choice('conv12_kernel', [(3, 3), (5, 5)]),\n",
    "    'add_conv_13' : hp.choice('add_conv_13', [True, False]),\n",
    "    'conv13_kernel': hp.choice('conv13_kernel', [(3, 3), (5, 5)]),\n",
    "    \n",
    "    'dense1_units': hp.choice('dense1_units', [256, 512]),\n",
    "    'add_dense_2' : hp.choice('add_dense_2', [True, False]),\n",
    "    'dense2_units': hp.choice('dense2_units', [64, 128]),\n",
    "    'dense3_units': hp.choice('dense3_units', [32, 64]),\n",
    "    'add_dense_4' : hp.choice('add_dense_4', [True, False]),\n",
    "    'dense4_units': hp.choice('dense4_units', [32, 64])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045b917-13c4-48be-b449-d1556a7f4dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create trials object to store optimization results\n",
    "trials = Trials()\n",
    "\n",
    "# Run optimization\n",
    "ti = time.time()\n",
    "best_singleInput = fmin(\n",
    "    fn=objective_singleInput,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=max_evals,\n",
    "    trials=trials\n",
    ")\n",
    "tuningTime = time.time() - ti\n",
    "\n",
    "hours, remainder = divmod(tuningTime, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "print(\"Best hyperparameters:\", space_eval(space, best_singleInput))\n",
    "\n",
    "# Save the results\n",
    "json_compatible_params = {\n",
    "    key: (list(value) if isinstance(value, tuple) else value)\n",
    "    for key, value in space_eval(space, best_singleInput).items()\n",
    "}\n",
    "with open('Image_classifiers/Parameters_singleInputCNN.json', \"w\") as f:\n",
    "    json.dump(json_compatible_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec641e-ccd5-4d1d-873f-98e121f78d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_singleInput_model(space_eval(space, best_singleInput), h, w)\n",
    "plot_model(model, to_file='Image_classifiers/Models/singleInputCNN.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e63f9b-3b97-4dca-bbb7-1fe727692dab",
   "metadata": {},
   "source": [
    "### Multi-input CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b074ce-659f-4113-968f-c6d03b204ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multi-input model\n",
    "def create_multiInput_model(params, a=480, b=640):\n",
    "\n",
    "    ## Front\n",
    "    front_input = Input(shape=(a, b, 1))\n",
    "    front = Conv2D(32, (3, 3), activation='relu')(front_input)\n",
    "    front = Conv2D(64, kernel_size=params['front_conv1_kernel'], strides=params['front_conv1_strides'], activation='relu')(front)  # strides=2\n",
    "    front = MaxPooling2D((2, 2))(front)\n",
    "    front = Conv2D(params['front_conv2_filters'], kernel_size=params['front_conv2_kernel'], activation='relu')(front)  #256, 128\n",
    "    front = Conv2D(params['front_conv3_filters'], kernel_size=params['front_conv3_kernel'], activation='relu')(front)  #64\n",
    "    front = MaxPooling2D((2, 2))(front)\n",
    "    front = Flatten()(front) \n",
    "    front = Dense(params['front_dense1_units'], activation='relu')(front)  #256, 512\n",
    "    if params['front_add_dense2']:\n",
    "        front = Dropout(0.5)(front)\n",
    "        front = Dense(params['front_dense2_units'], activation='relu')(front)  #128, 256 \n",
    "    \n",
    "    ## L90\n",
    "    L90_input = Input(shape=(a, b, 1))\n",
    "    L90 = Conv2D(32, (3, 3), activation='relu')(L90_input)\n",
    "    L90 = Conv2D(params['L90_conv1_filters'], kernel_size=params['L90_conv1_kernel'], activation='relu')(L90)  #64\n",
    "    L90 = MaxPooling2D((2, 2))(L90)\n",
    "    L90 = Conv2D(params['L90_conv2_filters'], kernel_size=params['L90_conv2_kernel'], strides=2, activation='relu')(L90)  #128\n",
    "    #L90 = Conv2D(64, (5, 5), activation='relu')(L90) \n",
    "    if params['L90_add_conv3']:\n",
    "        L90 = Conv2D(params['L90_conv3_filters'], kernel_size=params['L90_conv3_kernel'], activation='relu')(L90) \n",
    "    L90 = Conv2D(params['L90_conv4_filters'], kernel_size=params['L90_conv4_kernel'], activation='relu')(L90) # 64, (5, 5)\n",
    "    L90 = MaxPooling2D((2, 2))(L90)\n",
    "    L90 = Flatten()(L90)\n",
    "    L90 = Dense(256, activation='relu')(L90)  \n",
    "    #L90 = Dropout(0.5)(L90)\n",
    "    #L90 = Dense(128, activation='relu')(L90)\n",
    "    \n",
    "    ## R90\n",
    "    R90_input = Input(shape=(a, b, 1))\n",
    "    R90 = Conv2D(32, (3, 3), activation='relu')(R90_input)\n",
    "    R90 = Conv2D(params['R90_conv1_filters'], kernel_size=params['R90_conv1_kernel'], activation='relu')(R90)   \n",
    "    R90 = MaxPooling2D((2, 2))(R90)\n",
    "    R90 = Conv2D(params['R90_conv2_filters'], kernel_size=params['R90_conv2_kernel'], strides=2, activation='relu')(R90)  #128\n",
    "    #R90 = Conv2D(64, (5, 5), activation='relu')(R90)  \n",
    "    if params['R90_add_conv3']:\n",
    "        R90 = Conv2D(params['R90_conv3_filters'], kernel_size=params['R90_conv3_kernel'], activation='relu')(R90) \n",
    "    R90 = Conv2D(params['R90_conv4_filters'], kernel_size=params['R90_conv4_kernel'], activation='relu')(R90)  # 64, (5, 5)\n",
    "    R90 = MaxPooling2D((2, 2))(R90)\n",
    "    R90 = Flatten()(R90)\n",
    "    R90 = Dense(256, activation='relu')(R90)  \n",
    "    #R90 = Dropout(0.5)(R90)\n",
    "    #R90 = Dense(128, activation='relu')(R90)\n",
    "\n",
    "    ## concatenate all the outputs of each stalk of the net.\n",
    "    concatenated = concatenate([front, L90, R90], axis=-1)\n",
    "    x = Dense(params['dense1_units'], activation='relu')(concatenated)  ## antes 512, 256\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x) \n",
    "\n",
    "    if params['add_dense2']:\n",
    "        x = Dense(params['dense2_units'], use_bias=False)(x)  ## 64\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        #x = Dense(64, activation='relu')(x)  ## 64, 128\n",
    "        #x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)   #new\n",
    "\n",
    "    x = Dense(params['dense3_units'], activation='relu')(x)   # <-- batchnorm en esta capa funciona mal\n",
    "    X = Dense(1, activation='sigmoid')(x)   \n",
    "            \n",
    "    ## create the model\n",
    "    return Model([front_input, L90_input, R90_input], X)\n",
    "\n",
    "\n",
    "# Objective function to minimize\n",
    "def objective_multiInput(params):\n",
    "\n",
    "    # Load the data\n",
    "    front, L90, R90, labels = data()\n",
    "    _,h,w,_ = front.shape\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    epochs = 30\n",
    "    batch_size = 8\n",
    "    lr = params['lr']\n",
    "    folds = 5\n",
    "    seed = 13\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Store results from each fold\n",
    "    fold_metrics = []\n",
    "\n",
    "    # Initialize the model\n",
    "    model = create_multiInput_model(params, h, w)\n",
    "\n",
    "    # Save the initial weights of the model\n",
    "    initial_weights = model.get_weights()\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(skf.split(front, labels)):\n",
    "        \n",
    "        print(f\"Fold {i+1}/{folds}\")\n",
    "        \n",
    "        # Split the data into training and validation sets\n",
    "        train_front, val_front = front[train_idx], front[val_idx]\n",
    "        train_L90, val_L90 = L90[train_idx], L90[val_idx]\n",
    "        train_R90, val_R90 = R90[train_idx], R90[val_idx]\n",
    "        train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "        rate_train = sum(train_labels==0)/sum(train_labels)\n",
    "\n",
    "        # Stack views along the channel axis\n",
    "        train_images = [train_front, train_L90, train_R90]\n",
    "        val_images = [val_front, val_L90, val_R90]\n",
    "\n",
    "        # Reset the model to initial weights\n",
    "        model.set_weights(initial_weights)\n",
    "\n",
    "        # Train the model\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "        history = model.fit(\n",
    "            train_images, train_labels,\n",
    "            class_weight={0: 1, 1: rate_train},\n",
    "            validation_data=(val_images, val_labels),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[NaNCheckCallback()],\n",
    "            verbose=0  # Suppress training output\n",
    "        )\n",
    "\n",
    "        # Record the best validation AUC for this fold\n",
    "        best_auc = max(history.history['val_AUC'])\n",
    "        fold_metrics.append(best_auc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Clear TensorFlow session to release GPU memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    # Return the negative mean AUC across all folds as the loss\n",
    "    print(f'Max test ROC AUC in each fold: {fold_metrics}')\n",
    "    mean_auc = np.mean(fold_metrics)\n",
    "    return -mean_auc  # Maximize AUC by minimizing its negative\n",
    "\n",
    "\n",
    "## Hyperparameter search space\n",
    "space = {\n",
    "    'lr': hp.loguniform('lr', np.log(1e-5), np.log(1e-2)),\n",
    "    \n",
    "    'front_conv1_kernel': hp.choice('front_conv1_kernel', [(3, 3), (5, 5)]),\n",
    "    'front_conv1_strides': hp.choice('front_conv1_strides', [1, 2]),\n",
    "    'front_conv2_filters': hp.choice('front_conv2_filters', [64, 128, 256]),\n",
    "    'front_conv2_kernel': hp.choice('front_conv2_kernel', [(3, 3), (5, 5)]),\n",
    "    'front_conv3_filters': hp.choice('front_conv3_filters', [64, 128]),\n",
    "    'front_conv3_kernel': hp.choice('front_conv3_kernel', [(3, 3), (5, 5)]),\n",
    "    'front_dense1_units': hp.choice('front_dense1_units', [256, 512]),\n",
    "    'front_add_dense2' : hp.choice('front_add_dense2', [True, False]),\n",
    "    'front_dense2_units': hp.choice('front_dense2_units', [128, 256]),\n",
    "\n",
    "    'L90_conv1_filters': hp.choice('L90_conv1_filters', [32, 64]),\n",
    "    'L90_conv1_kernel': hp.choice('L90_conv1_kernel', [(3, 3), (5, 5)]),\n",
    "    'L90_conv2_filters': hp.choice('L90_conv2_filters', [64, 128]),\n",
    "    'L90_conv2_kernel': hp.choice('L90_conv2_kernel', [(3, 3), (5, 5)]),\n",
    "    'L90_add_conv3' : hp.choice('L90_add_conv3', [True, False]),\n",
    "    'L90_conv3_filters': hp.choice('L90_conv3_filters', [64, 128]),\n",
    "    'L90_conv3_kernel': hp.choice('L90_conv3_kernel', [(3, 3), (5, 5)]),\n",
    "    'L90_conv4_filters': hp.choice('L90_conv4_filters', [64, 128]),\n",
    "    'L90_conv4_kernel': hp.choice('L90_conv4_kernel', [(3, 3), (5, 5)]),\n",
    "\n",
    "    'R90_conv1_filters': hp.choice('R90_conv1_filters', [32, 64]),\n",
    "    'R90_conv1_kernel': hp.choice('R90_conv1_kernel', [(3, 3), (5, 5)]),\n",
    "    'R90_conv2_filters': hp.choice('R90_conv2_filters', [64, 128]),\n",
    "    'R90_conv2_kernel': hp.choice('R90_conv2_kernel', [(3, 3), (5, 5)]),\n",
    "    'R90_add_conv3' : hp.choice('R90_add_conv3', [True, False]),\n",
    "    'R90_conv3_filters': hp.choice('R90_conv3_filters', [64, 128]),\n",
    "    'R90_conv3_kernel': hp.choice('R90_conv3_kernel', [(3, 3), (5, 5)]),\n",
    "    'R90_conv4_filters': hp.choice('R90_conv4_filters', [64, 128]),\n",
    "    'R90_conv4_kernel': hp.choice('R90_conv4_kernel', [(3, 3), (5, 5)]),\n",
    "\n",
    "    'dense1_units': hp.choice('dense1_units', [256, 512]),\n",
    "    'add_dense2' : hp.choice('add_dense2', [True, False]),\n",
    "    'dense2_units': hp.choice('dense2_units', [64, 128]),\n",
    "    'dense3_units': hp.choice('dense3_units', [32, 64])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c16c7-c282-45a9-a792-3dcde3a7823f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create trials object to store optimization results\n",
    "trials = Trials()\n",
    "\n",
    "# Run optimization\n",
    "ti = time.time()\n",
    "best_multiInput = fmin(\n",
    "    fn=objective_multiInput,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=max_evals,\n",
    "    trials=trials\n",
    ")\n",
    "tuningTime = time.time() - ti\n",
    "\n",
    "hours, remainder = divmod(tuningTime, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "print(\"Best hyperparameters:\", space_eval(space, best_multiInput))\n",
    "\n",
    "# Save the results\n",
    "json_compatible_params = {\n",
    "    key: (list(value) if isinstance(value, tuple) else value)\n",
    "    for key, value in space_eval(space, best_multiInput).items()\n",
    "}\n",
    "with open('Image_classifiers/Parameters_multiInputCNN.json', \"w\") as f:\n",
    "    json.dump(json_compatible_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a81e2-950a-42ba-a35c-0f86dedc253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_multiInput_model(space_eval(space, best_multiInput), h, w)\n",
    "plot_model(model, to_file='Image_classifiers/Models/multiInputCNN.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c461bed-acb5-4191-973c-82099ce72957",
   "metadata": {},
   "source": [
    "## Train N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c916a0c-f048-441f-92b1-32fe8daa10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9f86a-bfc3-47ef-8586-e7fb33cc1478",
   "metadata": {},
   "source": [
    "### Single-input CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916d778-bce4-4013-9534-64dc7f9eb81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tuned hyperparameters\n",
    "with open(\"Image_classifiers/Parameters_singleInputCNN.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "lr = params['lr']\n",
    "params.pop('lr')\n",
    "\n",
    "# Convert lists back to tuples if needed\n",
    "params = {\n",
    "    key: (tuple(value) if isinstance(value, list) else value)\n",
    "    for key, value in params.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655a211-200c-45e6-9315-80c88c28754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)\n",
    "\n",
    "for trial, (train_index, test_index) in enumerate(splitter.split(front, labels)):\n",
    "\n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    # Split the dataset\n",
    "    train_front = front[train_index]\n",
    "    train_L90 = L90[train_index]\n",
    "    train_R90 = R90[train_index]\n",
    "    train_labels = labels[train_index]\n",
    "    \n",
    "    test_front = front[test_index]\n",
    "    test_L90 = L90[test_index]\n",
    "    test_R90 = R90[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Stack the three views\n",
    "    train_images = np.stack((train_front, train_L90, train_R90), axis=-1)\n",
    "    test_images = np.stack((test_front, test_L90, test_R90), axis=-1)\n",
    "\n",
    "    # Compute the class weight difference\n",
    "    rate_train = sum(train_labels == 0) / sum(train_labels)\n",
    "\n",
    "    # Initialize and compile the model\n",
    "    model = create_singleInput_model(params, h, w)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'AUC', 'TruePositives', 'FalseNegatives', 'TrueNegatives', 'FalsePositives', 'Precision', 'Recall', Specificity(), WeightedError(rate=20)], \n",
    "                  optimizer=SGD(learning_rate=lr, clipvalue=1.0))\n",
    "    \n",
    "    # Train the model\n",
    "    ti = time.time()\n",
    "    history = model.fit(train_images, train_labels, class_weight = {0: 1, 1: rate_train}, \n",
    "                        validation_data = (test_images, test_labels),\n",
    "                        batch_size=batch_size, epochs=epochs, verbose=0, callbacks=[NaNCheckCallback()]) \n",
    "    trainTime = time.time() - ti\n",
    "\n",
    "    hours, remainder = divmod(trainTime, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Training took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    # Save learning curves\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(range(epochs), history.history['loss'], label='Train Loss')\n",
    "    plt.plot(range(epochs), history.history['val_loss'], label='Test Loss')\n",
    "    plt.legend(loc='upper right',fontsize=10)\n",
    "    plt.title('Train and Test Loss',fontsize=12)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(epochs), history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(range(epochs), history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right',fontsize=10)\n",
    "    plt.title('Train and Test Accuracy',fontsize=12)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(range(epochs), history.history['AUC'], label='Train AUC')\n",
    "    plt.plot(range(epochs), history.history['val_AUC'], label='Test AUC')\n",
    "    plt.legend(loc='lower right',fontsize=10)\n",
    "    plt.title('Train and Test ROC AUC',fontsize=12)\n",
    "    \n",
    "    plt.savefig('Image_classifiers/Learning_curves/SingleInputCNN_'+str(trial+1)+'.png'), plt.show()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the model\n",
    "    model.save('Image_classifiers/Models/SingleInputCNN_'+str(trial+1)+'.h5')\n",
    "\n",
    "    # Print the number of parameters in the model\n",
    "    total_params = model.count_params()  # total_params = np.sum([np.prod(v.shape.as_list()) for v in model.variables])\n",
    "    trainable_params = np.sum([np.prod(v.shape.as_list()) for v in model.trainable_variables])\n",
    "    print(f'Classifier has {total_params} total parameters, {trainable_params} of which are trainable.'), print()\n",
    "    \n",
    "    # Predict\n",
    "    train_preds = model.predict(train_images)\n",
    "    np.save('Image_classifiers/Predictions/SingleInputCNN_train_'+str(trial+1)+'.npy',train_preds)\n",
    "    test_preds = model.predict(test_images)\n",
    "    np.save('Image_classifiers/Predictions/SingleInputCNN_test_'+str(trial+1)+'.npy',test_preds)\n",
    "\n",
    "    # Evaluate the model\n",
    "    results_train = evaluate_model_skl(train_preds, train_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    results_test = evaluate_model_skl(test_preds, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append({**{'classifier':'singleInputCNN'}, **{'trial':trial+1}, \n",
    "                            **store_results(trainable_params, trainTime, results_train, results_test)})\n",
    "\n",
    "    # Clear TensorFlow session to release GPU memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    print(), print(100*'#'), print()\n",
    "\n",
    "pd.DataFrame(trials_results).to_csv('Image_classifiers/Results_'+str(N)+'trials.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e93531-0de8-4152-b2b0-17f48247ba24",
   "metadata": {},
   "source": [
    "### Multi-input CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc65fc-95c1-40f2-a839-fa63b8f247dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tuned hyperparameters\n",
    "with open(\"Image_classifiers/Parameters_multiInputCNN.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "lr = params['lr']\n",
    "params.pop('lr')\n",
    "\n",
    "# Convert lists back to tuples if needed\n",
    "params = {\n",
    "    key: (tuple(value) if isinstance(value, list) else value)\n",
    "    for key, value in params.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6331570-3b08-494d-ab6d-6007589a0b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_results = pd.read_csv('Image_classifiers/Results_'+str(N)+'trials.csv', index_col = 0).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd069ea-9e8d-4d9d-a7f7-cba3e3b31bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)\n",
    "\n",
    "for trial, (train_index, test_index) in enumerate(splitter.split(front, labels)):\n",
    "\n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    # Split the dataset\n",
    "    train_front = front[train_index]\n",
    "    train_L90 = L90[train_index]\n",
    "    train_R90 = R90[train_index]\n",
    "    train_labels = labels[train_index]\n",
    "    \n",
    "    test_front = front[test_index]\n",
    "    test_L90 = L90[test_index]\n",
    "    test_R90 = R90[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    # Add the channel dimension\n",
    "    train_front = np.expand_dims(train_front, -1)\n",
    "    train_L90 = np.expand_dims(train_L90, -1)\n",
    "    train_R90 = np.expand_dims(train_R90, -1)\n",
    "\n",
    "    test_front = np.expand_dims(test_front, -1)\n",
    "    test_L90 = np.expand_dims(test_L90, -1)\n",
    "    test_R90 = np.expand_dims(test_R90, -1)\n",
    "\n",
    "    # Compute the class weight difference\n",
    "    rate_train = sum(train_labels == 0) / sum(train_labels)\n",
    "\n",
    "    # Initialize and compile the model\n",
    "    model = create_multiInput_model(params, h, w)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'AUC', 'TruePositives', 'FalseNegatives', 'TrueNegatives', 'FalsePositives', 'Precision', 'Recall', Specificity(), WeightedError(rate=20)], \n",
    "                  optimizer=SGD(learning_rate=lr, clipvalue=1.0))\n",
    "    \n",
    "    # Train the model\n",
    "    ti = time.time()\n",
    "    history = model.fit([train_front, train_L90, train_R90], train_labels, class_weight = {0: 1, 1: rate_train}, \n",
    "                        validation_data = ([test_front, test_L90, test_R90], test_labels)\n",
    "                        batch_size=batch_size, epochs=epochs, verbose=0, callbacks=[NaNCheckCallback()]) \n",
    "    trainTime = time.time() - ti\n",
    "\n",
    "    hours, remainder = divmod(trainTime, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Training took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    # Save learning curves\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(range(epochs), history.history['loss'], label='Train Loss')\n",
    "    plt.plot(range(epochs), history.history['val_loss'], label='Test Loss')\n",
    "    plt.legend(loc='upper right',fontsize=10)\n",
    "    plt.title('Train and Test Loss',fontsize=12)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(epochs), history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(range(epochs), history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right',fontsize=10)\n",
    "    plt.title('Train and Test Accuracy',fontsize=12)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(range(epochs), history.history['AUC'], label='Train AUC')\n",
    "    plt.plot(range(epochs), history.history['val_AUC'], label='Test AUC')\n",
    "    plt.legend(loc='lower right',fontsize=10)\n",
    "    plt.title('Train and Test ROC AUC',fontsize=12)\n",
    "    \n",
    "    plt.savefig('Image_classifiers/Learning_curves/MultiInputCNN_'+str(trial+1)+'.png'), plt.show()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the model\n",
    "    model.save('Image_classifiers/Models/MultiInputCNN_'+str(trial+1)+'.h5')\n",
    "\n",
    "    # Print the number of parameters in the model\n",
    "    total_params = model.count_params()  # total_params = np.sum([np.prod(v.shape.as_list()) for v in model.variables])\n",
    "    trainable_params = np.sum([np.prod(v.shape.as_list()) for v in model.trainable_variables])\n",
    "    print(f'Classifier has {total_params} total parameters, {trainable_params} of which are trainable.'), print()\n",
    "    \n",
    "    # Predict\n",
    "    train_preds = model.predict([train_front, train_L90, train_R90])\n",
    "    np.save('Image_classifiers/Predictions/MultiInputCNN_train_'+str(trial+1)+'.npy',train_preds)\n",
    "    test_preds = model.predict([test_front, test_L90, test_R90])\n",
    "    np.save('Image_classifiers/Predictions/MultiInputCNN_test_'+str(trial+1)+'.npy',test_preds)\n",
    "\n",
    "    # Evaluate the model\n",
    "    results_train = evaluate_model_skl(train_preds, train_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    results_test = evaluate_model_skl(test_preds, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append({**{'classifier':'multiInputCNN'}, **{'trial':trial+1}, \n",
    "                            **store_results(trainable_params, trainTime, results_train, results_test)})\n",
    "\n",
    "    # Clear TensorFlow session to release GPU memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    print(), print(100*'#'), print()\n",
    "\n",
    "pd.DataFrame(trials_results).to_csv('Image_classifiers/Results_'+str(N)+'trials.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d43de-7167-4472-8d19-58a73a0ac7c4",
   "metadata": {},
   "source": [
    "# Pre-trained CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d8445-9a20-41ca-9cfa-76bb95d57e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import pre-trained CNNs\n",
    "from tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201, VGG16, VGG19, ResNet50, ResNet101, ResNet152, InceptionV3, MobileNet, MobileNetV3Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed8ab1-35d1-4a3d-8c53-e736d46c5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of models\n",
    "pretrained_dict = {\n",
    "    'VGG16': VGG16,\n",
    "    'VGG19': VGG19,\n",
    "    'ResNet50': ResNet50,\n",
    "    'ResNet101': ResNet101,\n",
    "    'ResNet152': ResNet152,\n",
    "    'DenseNet121': DenseNet121,\n",
    "    'DenseNet169': DenseNet169,\n",
    "    'DenseNet201': DenseNet201,\n",
    "    'InceptionV3': InceptionV3,\n",
    "    'MobileNet': MobileNet,\n",
    "    'MobileNetV3Small': MobileNetV3Small\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d2a6e-f691-41a1-9ad6-02a70193347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of top layers to unfreeze during second step of fine-tuning\n",
    "perc_unfreeze = 0.2\n",
    "\n",
    "# Number of epochs for the first step of the fine-tuning process\n",
    "epochs_freeze = epochs//2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524b057-e457-4ae5-9ff2-6988e698afd7",
   "metadata": {},
   "source": [
    "## Tune learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c5cdd8-73d1-4e64-9e10-37a348936b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective function to minimize\n",
    "def objective(params, baseModel_fn):\n",
    "\n",
    "    # Load the data\n",
    "    front, L90, R90, labels = data()\n",
    "    _,h,w,_ = front.shape\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    folds = 5\n",
    "    seed = 13\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Store results from each fold\n",
    "    fold_metrics = []\n",
    "\n",
    "    # Load the base model\n",
    "    baseModel = baseModel_fn(weights='imagenet', include_top=False, input_shape=(h, w, 3))\n",
    "\n",
    "    # Initially freeze all layers of the base model\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Build the full model\n",
    "    model = Sequential()\n",
    "    model.add(baseModel)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Save the initial weights of the model\n",
    "    initial_weights = model.get_weights()\n",
    "\n",
    "    epochs = 30\n",
    "    epochs_freeze = epochs//2\n",
    "    batch_size = 8\n",
    "    perc_unfreeze = 0.2\n",
    "    lr_freeze = params['lr_freeze']\n",
    "    lr_unfreeze = params['lr_unfreeze']\n",
    "    \n",
    "    for i, (train_idx, val_idx) in enumerate(skf.split(front, labels)):\n",
    "        \n",
    "        print(f\"Fold {i+1}/{folds}\")\n",
    "        \n",
    "        # Split the data into training and validation sets\n",
    "        train_front, val_front = front[train_idx], front[val_idx]\n",
    "        train_L90, val_L90 = L90[train_idx], L90[val_idx]\n",
    "        train_R90, val_R90 = R90[train_idx], R90[val_idx]\n",
    "        train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "        rate_train = sum(train_labels==0)/sum(train_labels)\n",
    "\n",
    "        # Stack views along the channel axis\n",
    "        train_images = np.concatenate((train_front, train_L90, train_R90), axis=-1)\n",
    "        val_images = np.concatenate((val_front, val_L90, val_R90), axis=-1)\n",
    "\n",
    "        # Reset the model to initial weights\n",
    "        model.set_weights(initial_weights)\n",
    "\n",
    "        # Step 1: Train only the fully connected layers\n",
    "        optimizer = SGD(learning_rate=lr_freeze)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "        model.fit(\n",
    "            train_images, train_labels,\n",
    "            class_weight={0: 1, 1: rate_train},\n",
    "            epochs=epochs_freeze,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[NaNCheckCallback()],\n",
    "            verbose=0  # Suppress training output\n",
    "        )\n",
    "\n",
    "        test_AUC_1 = model.evaluate(val_images, val_labels, verbose=0)[1]\n",
    "        print(f'Test ROC AUC after step 1 of fine-tuning: {test_AUC_1}')\n",
    "\n",
    "\n",
    "        # Step 2: Unfreeze part of the base model\n",
    "        total_layers = len(baseModel.layers)\n",
    "        unfreeze_layers = int(total_layers * perc_unfreeze)\n",
    "\n",
    "        for layer in baseModel.layers[total_layers - unfreeze_layers:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # Recompile the model with a reduced learning rate\n",
    "        optimizer = SGD(learning_rate=lr_unfreeze)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "        # Train the model for fine-tuning\n",
    "        history = model.fit(\n",
    "            train_images, train_labels,\n",
    "            class_weight={0: 1, 1: rate_train},\n",
    "            validation_data=(val_images, val_labels),\n",
    "            epochs=epochs-epochs_freeze,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[NaNCheckCallback()],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        test_AUC_2 = model.evaluate(val_images, val_labels, verbose=0)[1]\n",
    "        print(f'Test ROC AUC after step 2 of fine-tuning: {test_AUC_2}')\n",
    "\n",
    "        # Record the best validation AUC for this fold\n",
    "        best_auc = max(history.history['val_AUC'])\n",
    "        fold_metrics.append(best_auc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Clear TensorFlow session to release GPU memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    # Return the negative mean AUC across all folds as the loss\n",
    "    mean_auc = np.mean(fold_metrics)\n",
    "    return -mean_auc  # Maximize AUC by minimizing its negative\n",
    "\n",
    "\n",
    "## Hyperparameter search space\n",
    "space = {\n",
    "    'lr_freeze': hp.loguniform('lr_freeze', np.log(1e-5), np.log(1e-2)),\n",
    "    'lr_unfreeze': hp.loguniform('lr_unfreeze', np.log(1e-6), np.log(1e-3))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8729c7d-8a6d-4697-a526-91dcd56ab9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates_pt = {}\n",
    "for model_name, baseModel_fn in pretrained_dict.items():\n",
    "    print(f'Base classifier: {model_name}')\n",
    "    \n",
    "    # Objective function for the specific pre-trained CNN\n",
    "    objective_pt = lambda params: objective(params, baseModel_fn)\n",
    "\n",
    "    # Create trials object to store optimization results\n",
    "    trials = Trials()\n",
    "\n",
    "    # Run optimization\n",
    "    ti = time.time()\n",
    "    best = fmin(\n",
    "        fn=objective_pt,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials = trials\n",
    "    )\n",
    "    tuningTime = time.time() - ti\n",
    "\n",
    "    hours, remainder = divmod(tuningTime, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    print(\"Best hyperparameters:\", best)\n",
    "\n",
    "    learning_rates_pt[model_name] = {'lr_freeze' : best['lr_freeze'], 'lr_unfreeze' : best['lr_unfreeze']}\n",
    "\n",
    "    # Plot the parameter tuning process\n",
    "    lrs_freeze = np.asarray([trial['misc']['vals']['lr_freeze'][0] for trial in trials.trials])\n",
    "    lrs_unfreeze = np.asarray([trial['misc']['vals']['lr_unfreeze'][0] for trial in trials.trials])\n",
    "    losses = np.asarray([trial['result']['loss'] for trial in trials.trials])\n",
    "    \n",
    "    # Plot the results\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(lrs_freeze, lrs_unfreeze, -losses, c=losses, cmap='viridis', label='Loss vs. Learning Rates')\n",
    "    ax.set_xlabel('Learning Rate for Step 1')\n",
    "    ax.set_ylabel('Learning Rate for Step 2')\n",
    "    ax.set_zlabel('Mean ROC AUC')\n",
    "    ax.set_title('Hyperparameter Tuning: ROC AUC vs. Learning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Image_classifiers/LearningRate_tuning/'+model_name+'.png'), plt.show()\n",
    "    plt.show()\n",
    "    \n",
    "    print('-'*120), print()\n",
    "\n",
    "pd.DataFrame(learning_rates_pt).to_csv('Image_classifiers/Parameters_preTrainedCNNs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cae6b-884d-44d9-8dcf-0178b836ba75",
   "metadata": {},
   "source": [
    "## Train N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448c620-4cc3-46d9-9a78-15baafa5132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_results = pd.read_csv('Image_classifiers/Results_'+str(N)+'trials.csv', index_col = 0).to_dict(orient='records')\n",
    "learning_rates_pt = pd.read_csv('Image_classifiers/Parameters_preTrainedCNNs.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057b1bb-f5b4-4d4e-bbb1-2f4853069488",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, baseModel_fn in pretrained_dict.items():\n",
    "    if not model_name in learning_rates_pt.columns:\n",
    "        continue\n",
    "            \n",
    "    print(f'Base classifier: {model_name}')\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)\n",
    "\n",
    "    # Get the tuned learning rates\n",
    "    lr_freeze = learning_rates_pt[model_name]['lr_freeze']\n",
    "    lr_unfreeze = learning_rates_pt[model_name]['lr_unfreeze']\n",
    "\n",
    "    #best_test_auc = 0\n",
    "    for trial, (train_index, test_index) in enumerate(splitter.split(front, labels)):\n",
    "    \n",
    "        print(f'Trial {trial + 1}'), print()\n",
    "    \n",
    "        ## Split the dataset\n",
    "        train_front = front[train_index]\n",
    "        train_L90 = L90[train_index]\n",
    "        train_R90 = R90[train_index]\n",
    "        train_labels = labels[train_index]\n",
    "        \n",
    "        test_front = front[test_index]\n",
    "        test_L90 = L90[test_index]\n",
    "        test_R90 = R90[test_index]\n",
    "        test_labels = labels[test_index]\n",
    "    \n",
    "        # Stack the three views\n",
    "        train_images = np.stack((train_front, train_L90, train_R90), axis=-1)\n",
    "        test_images = np.stack((test_front, test_L90, test_R90), axis=-1)\n",
    "    \n",
    "        rate_train = sum(train_labels==0)/sum(train_labels)\n",
    "\n",
    "        ## Build the model\n",
    "        # Load the base model\n",
    "        baseModel = baseModel_fn(weights='imagenet', include_top=False, input_shape=(h, w, 3))\n",
    "    \n",
    "        # Initially freeze all layers of the base model\n",
    "        for layer in baseModel.layers:\n",
    "            layer.trainable = False\n",
    "    \n",
    "        # Build the full model\n",
    "        model = Sequential()\n",
    "        model.add(baseModel)\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "\n",
    "        ## Train the model\n",
    "        # Step 1: Train only the fully connected layers\n",
    "        optimizer = SGD(learning_rate=lr_freeze, clipvalue=1.0)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      metrics=['accuracy', 'AUC', 'TruePositives', 'FalseNegatives', 'TrueNegatives', 'FalsePositives', 'Precision', 'Recall', Specificity(), WeightedError(rate=20)], \n",
    "                      optimizer=optimizer)\n",
    "\n",
    "        ti = time.time()\n",
    "        history1 = model.fit(\n",
    "            train_images, train_labels,\n",
    "            class_weight={0: 1, 1: rate_train},\n",
    "            validation_data=(test_images, test_labels),\n",
    "            epochs=epochs_freeze,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[NaNCheckCallback()],\n",
    "            verbose=0  # Suppress training output\n",
    "        )\n",
    "\n",
    "        test_AUC_1 = model.evaluate(test_images, test_labels, verbose=0)[2]\n",
    "        print(f'Test ROC AUC after step 1 of fine-tuning: {test_AUC_1}')\n",
    "\n",
    "\n",
    "        # Step 2: Unfreeze part of the base model\n",
    "        total_layers = len(baseModel.layers)\n",
    "        unfreeze_layers = int(total_layers * perc_unfreeze)\n",
    "\n",
    "        for layer in baseModel.layers[total_layers - unfreeze_layers:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # Recompile the model with a reduced learning rate\n",
    "        optimizer = SGD(learning_rate=lr_unfreeze, clipvalue=1.0)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      metrics=['accuracy', 'AUC', 'TruePositives', 'FalseNegatives', 'TrueNegatives', 'FalsePositives', 'Precision', 'Recall', Specificity(), WeightedError(rate=20)], \n",
    "                      optimizer=optimizer)\n",
    "\n",
    "        # Train the model for fine-tuning\n",
    "        history2 = model.fit(\n",
    "            train_images, train_labels,\n",
    "            class_weight={0: 1, 1: rate_train},\n",
    "            validation_data=(test_images, test_labels),\n",
    "            epochs=epochs-epochs_freeze,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[NaNCheckCallback()],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        test_AUC_2 = model.evaluate(test_images, test_labels, verbose=0)[2]\n",
    "        print(f'Test ROC AUC after step 2 of fine-tuning: {test_AUC_2}')\n",
    "\n",
    "        trainTime = time.time() - ti\n",
    "\n",
    "        hours, remainder = divmod(trainTime, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        print(f'Training took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "        # Save learning curves\n",
    "        plt.figure(figsize=(30, 10))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(range(epochs), history1.history['loss']+history2.history['loss'], label='Train Loss')\n",
    "        plt.plot(range(epochs), history1.history['val_loss']+history2.history['val_loss'], label='Test Loss')\n",
    "        plt.legend(loc='upper right',fontsize=10)\n",
    "        plt.title('Train and Test Loss',fontsize=12)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(range(epochs), history1.history['accuracy']+history2.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(range(epochs), history1.history['val_accuracy']+history2.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.legend(loc='lower right',fontsize=10)\n",
    "        plt.title('Train and Test Accuracy',fontsize=12)\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(range(epochs), history1.history['AUC']+history2.history['AUC'], label='Train AUC')\n",
    "        plt.plot(range(epochs), history1.history['val_AUC']+history2.history['val_AUC'], label='Test AUC')\n",
    "        plt.legend(loc='lower right',fontsize=10)\n",
    "        plt.title('Train and Test ROC AUC',fontsize=12)\n",
    "        \n",
    "        plt.savefig('Image_classifiers/Learning_curves/'+model_name+'_'+str(trial+1)+'.png'), plt.show()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        # Save the model\n",
    "        model.save('Image_classifiers/Models/'+model_name+'_'+str(trial+1)+'.h5')\n",
    "\n",
    "        \n",
    "        # Print the number of parameters in the model\n",
    "        total_params = model.count_params()  # total_params = np.sum([np.prod(v.shape.as_list()) for v in model.variables])\n",
    "        trainable_params = np.sum([np.prod(v.shape.as_list()) for v in model.trainable_variables])\n",
    "        print(f'Classifier has {total_params} total parameters, {trainable_params} of which are trainable.'), print()\n",
    "    \n",
    "        # Predict\n",
    "        train_preds = model.predict(train_images)\n",
    "        np.save('Image_classifiers/Predictions/'+model_name+'_train_'+str(trial+1)+'.npy',train_preds)\n",
    "        test_preds = model.predict(test_images)\n",
    "        np.save('Image_classifiers/Predictions/'+model_name+'_test_'+str(trial+1)+'.npy',test_preds)\n",
    "\n",
    "        # Evaluate the model\n",
    "        results_train = evaluate_model_skl(train_preds, train_labels)\n",
    "        print('TRAIN results:')\n",
    "        for metric, value in results_train.items():\n",
    "            print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "        print()\n",
    "\n",
    "        results_test = evaluate_model_skl(test_preds, test_labels)\n",
    "        print('TEST results:')\n",
    "        for metric, value in results_test.items():\n",
    "            print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "        print()\n",
    "\n",
    "        ## Store results\n",
    "        trials_results.append({**{'classifier':model_name}, **{'trial':trial+1}, \n",
    "                                **store_results(trainable_params, trainTime, results_train, results_test)})\n",
    "    \n",
    "        # Clear TensorFlow session to release GPU memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        del model\n",
    "        gc.collect()\n",
    "    \n",
    "        print(), print(100*'#'), print()\n",
    "\n",
    "pd.DataFrame(trials_results).to_csv('Image_classifiers/Results_'+str(N)+'trials.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc03d2-ed68-4b2b-be31-09d5efb4f7f8",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4a0be-e594-4528-b1f5-b5adc171ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read results\n",
    "trials_results = pd.read_csv('Image_classifiers/Results_'+str(N)+'trials.csv', index_col=0)\n",
    "trials_results.fillna(1e-10, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde34d7-491c-42f3-bd25-c6b605253b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print statistics\n",
    "models = trials_results.classifier.unique()\n",
    "metrics = [c for c in trials_results.columns if 'test_' in c and c not in ['test_TP','test_FP','test_TN','test_FN']]\n",
    "    \n",
    "statistics = pd.DataFrame(index=models, columns=[item for sublist in [[metric+'_mean', metric+'_std'] for metric in metrics] for item in sublist])\n",
    "    \n",
    "for metric in metrics:\n",
    "    mn, st = metric+'_mean', metric+'_std'\n",
    "    for model in models:\n",
    "        results = trials_results[trials_results['classifier']==model][metric].values\n",
    "        statistics.at[model,mn] = results.mean()\n",
    "        statistics.at[model,st] = results.std()\n",
    "\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1f186-5079-4578-b2a1-623597663c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [m for m in metrics if 'test' in m]:\n",
    "    mn, st = metric+'_mean', metric+'_std'\n",
    "    if 'Loss' in metric or 'WE' in metric:\n",
    "        model_best = pd.to_numeric(statistics[metric+'_mean']).idxmin()\n",
    "        print(f'Model with lowest {metric} is {model_best} with value {statistics.loc[model_best,mn]} and standard deviation {statistics.loc[model_best,st]}')\n",
    "    else:\n",
    "        model_best = pd.to_numeric(statistics[metric+'_mean']).idxmax()\n",
    "        print(f'Model with highest {metric} is {model_best} with value {statistics.loc[model_best,mn]} and standard deviation {statistics.loc[model_best,st]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7281cd-af4e-4cac-8c77-242e8892342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print mean and std metrics for each model\n",
    "for classifier_type in trials_results.classifier.unique():\n",
    "    print(f'Classifier: {classifier_type}')\n",
    "    results = trials_results[trials_results['classifier'] == classifier_type]\n",
    "\n",
    "    # Number of parameters\n",
    "    parameters = results['Parameters'].values\n",
    "    print(f'Number of parameters: {parameters[0]}')\n",
    "\n",
    "    # training time\n",
    "    trainTime = results['trainTime'].values\n",
    "    hours, remainder = divmod(trainTime.mean(), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Mean training time: {hours} hours, {minutes} minutes, and {seconds} seconds, (std {trainTime.std()} sec)')\n",
    "    print()\n",
    "    \n",
    "    # TRAIN results\n",
    "    metrics = ['BCELoss','Accuracy','Sensitivity','Specificity','ROC_AUC','Precision','F1','WE']\n",
    "    for metric in metrics:\n",
    "        values = results['train_' + metric].values\n",
    "        print(f'Mean train {metric}: {values.mean()}, std {values.std()}')\n",
    "    print()\n",
    "\n",
    "    # TEST results\n",
    "    for metric in metrics:\n",
    "        values = results['test_' + metric].values\n",
    "        print(f'Mean test {metric}: {values.mean()}, std {values.std()}')\n",
    "    print()\n",
    "\n",
    "    print('-'*120), print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212d5ce-1bde-4210-ae52-6ef1a859504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show boxplots\n",
    "visualize_boxplots(trials_results,\n",
    "                   ['test_BCELoss','test_Accuracy','test_F1','test_ROC_AUC','test_WE'], #[c for c in cd_trials_results.columns if 'test_' in c and c not in ['test_TP','test_FP','test_TN','test_FN','test_WE','test_Loss']],\n",
    "                   True,'Image_classifiers/Boxplots_allModels.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0aeaa-ea1f-4ea2-a063-0f81b83859a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistical model comparison\n",
    "compare_models(trials_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11ebea-fb27-486c-ad98-3fc7a63aba44",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f8e5c-6bb5-4e34-989b-a2f81f89f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'multiInputCNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b018c-54cb-4d49-8592-b302ea29dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_boxplot_onemodel(trials_results[trials_results['classifier']==selected_model],\n",
    "                           ['test_Accuracy','test_Sensitivity','test_Specificity','test_F1','test_ROC_AUC'],\n",
    "                           True,'Image_classifiers/Boxplot_'+selected_model+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a3178-f828-48a7-a88b-f073a6476105",
   "metadata": {},
   "source": [
    "# With segmented frontal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1387ea9-41cf-4681-b6d5-a9e4b209cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c67b4-b857-4c3e-b3db-778140b3c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load segmentation model\n",
    "\n",
    "path_segmentation = 'Segmentation'\n",
    "\n",
    "# Select model with highest Dice coefficient\n",
    "segmentation_results = pd.read_csv(os.path.join(path_segmentation, 'Results.csv'), index_col=0)\n",
    "best_segmentation_model = segmentation_results.iloc[np.argmax(segmentation_results['testDice'].values)].name\n",
    "print(best_segmentation_model)\n",
    "\n",
    "# load the model\n",
    "segmentation_model = tf.keras.models.load_model(os.path.join(path_segmentation, 'Models', best_segmentation_model+'.h5'), compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0e3b8-02ea-41a8-9904-0d956bee59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine threshold to remove background\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(np.concatenate((front, L90, R90)).flatten(), \n",
    "         bins=256, range=(0, 1), density=True, color='blue', alpha=0.7)\n",
    "plt.title('Histogram of Pixel Values')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "threshold = float(input('Enter threshold value: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498bc081-0fd5-4dd5-ae84-065792757512",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to preprocess images: segment the breast region in frontal imagesm + remove background in all views\n",
    "def apply_segmentation(images, segmentation_model):\n",
    "    \n",
    "    ## Segment the breast region in frontal images\n",
    "    # Resize images to match the segmentation model's input shape\n",
    "    dsize = segmentation_model.input.shape\n",
    "    images_reduced = np.asarray([cv2.resize(img, (dsize[2],dsize[1])) for img in images])  # Reduce image size to 240x320\n",
    "    images_reduced = np.repeat(np.expand_dims(images_reduced,axis=-1), 3, axis=-1)  # Convert to 3-channel image\n",
    "    \n",
    "    # Predict masks\n",
    "    masks = segmentation_model(images_reduced)\n",
    "    masks = np.round(masks)  # Convert to binary\n",
    "\n",
    "    # Resize masks back to original image size\n",
    "    masks = np.asarray([cv2.resize(tf.squeeze(mask).numpy(), (front.shape[2], front.shape[1])) for mask in masks])  # Resize mask to original image shape\n",
    "    \n",
    "    images_segmented = np.multiply(images, masks)\n",
    "\n",
    "    return images_segmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f54f0-4bef-454b-97a0-4dca7bc079b7",
   "metadata": {},
   "source": [
    "## Tune learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a59bb7-3eb5-4370-90ce-d60eac394e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get the data for hyperparameter tuning\n",
    "def data_segmentation():    \n",
    "    ## Frontal images\n",
    "    front_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/Front',f))) for f in os.listdir('Images/Healthy/Front')])\n",
    "    front_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/Front',f))) for f in os.listdir('Images/Sick/Front')])\n",
    "    front = np.concatenate((front_images_h, front_images_s))\n",
    "\n",
    "    ## Left lateral (L90) images\n",
    "    L90_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/L90',f))) for f in os.listdir('Images/Healthy/L90')])\n",
    "    L90_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/L90',f))) for f in os.listdir('Images/Sick/L90')])\n",
    "    L90 = np.concatenate((L90_images_h, L90_images_s))\n",
    "\n",
    "    ## Right lateral (R90) images\n",
    "    R90_images_h = np.asarray([np.array(Image.open(os.path.join('Images/Healthy/R90',f))) for f in os.listdir('Images/Healthy/R90')])\n",
    "    R90_images_s = np.asarray([np.array(Image.open(os.path.join('Images/Sick/R90',f))) for f in os.listdir('Images/Sick/R90')])\n",
    "    R90 = np.concatenate((R90_images_h, R90_images_s))\n",
    "    \n",
    "    ## Labels\n",
    "    labels_h = [0]*len(front_images_h)\n",
    "    labels_s = [1]*len(front_images_s)\n",
    "    labels = np.concatenate((labels_h, labels_s))\n",
    "\n",
    "    ## Split the dataset into crossval and test sets\n",
    "    seed_tuning = 13  # Seed for hyperparameter tuning\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=int(round(0.15*len(labels))), random_state = seed_tuning)\n",
    "    crossval_index, test_index = [[crossval_index, test_index] for crossval_index, test_index in splitter.split(front, labels)][0]\n",
    "    \n",
    "    front_crossval = front[crossval_index]\n",
    "    L90_crossval = L90[crossval_index]\n",
    "    R90_crossval = R90[crossval_index]\n",
    "    labels_crossval = labels[crossval_index]\n",
    "\n",
    "    #front_test = front[test_index]\n",
    "    #L90_test = L90[test_index]\n",
    "    #R90_tets = R90[test_index]\n",
    "    #labels_test = labels[test_index]\n",
    "\n",
    "    ## Segment frontal images\n",
    "    # Load segmentation model\n",
    "    path_segmentation = 'Segmentation'\n",
    "    segmentation_results = pd.read_csv(os.path.join(path_segmentation, 'Results.csv'), index_col=0)\n",
    "    best_segmentation_model = segmentation_results.iloc[np.argmax(segmentation_results['testDice'].values)].name\n",
    "    segmentation_model = tf.keras.models.load_model(os.path.join(path_segmentation, 'Models', best_segmentation_model+'.h5'), compile = False)\n",
    "\n",
    "    # Apply segmentation\n",
    "    front_crossval = apply_segmentation(front_crossval, segmentation_model)\n",
    "    #front_test = apply_segmentation(front_test, segmentation_model)\n",
    "    \n",
    "    ## Remove background\n",
    "    background_threshold = 0.4\n",
    "    front_crossval = np.multiply(front_crossval, front_crossval > background_threshold)\n",
    "    L90_crossval = np.multiply(L90_crossval, L90_crossval > background_threshold)\n",
    "    R90_crossval = np.multiply(R90_crossval, R90_crossval > background_threshold)\n",
    "\n",
    "    #front_test = np.multiply(front_test, front_test > background_threshold)\n",
    "    #L90_test = np.multiply(L90_test, L90_test > background_threshold)\n",
    "    #R90_tets = np.multiply(R90_tets, R90_tets > background_threshold)\n",
    "\n",
    "    ## Add the channels dimension\n",
    "    front_crossval = np.expand_dims(front_crossval,-1)\n",
    "    L90_crossval = np.expand_dims(L90_crossval,-1)\n",
    "    R90_crossval = np.expand_dims(R90_crossval,-1)\n",
    "\n",
    "    #front_test = np.expand_dims(front_test,-1)\n",
    "    #L90_test = np.expand_dims(L90_test,-1)\n",
    "    #R90_tets = np.expand_dims(R90_tets,-1)\n",
    "\n",
    "    return front_crossval, L90_crossval, R90_crossval, labels_crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a745c-1ef1-4f55-b23e-a746b6e51c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obective function to minimize\n",
    "def objective_multiInput_segmented(params):\n",
    "\n",
    "    # Load the data\n",
    "    front, L90, R90, labels = data_segmentation()\n",
    "    _,h,w,_ = front.shape\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    epochs = 30\n",
    "    batch_size = 8\n",
    "    lr = params['lr']\n",
    "    folds = 5\n",
    "    seed = 13\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Store results from each fold\n",
    "    fold_metrics = []\n",
    "\n",
    "    # Initialize the model\n",
    "    with open(\"Image_classifiers/Parameters_multiInputCNN.json\", \"r\") as f:\n",
    "        model_params = json.load(f)\n",
    "    model_params.pop('lr')\n",
    "    model_params = {\n",
    "        key: (tuple(value) if isinstance(value, list) else value)\n",
    "        for key, value in model_params.items()\n",
    "    }\n",
    "    model = create_multiInput_model(model_params, h, w)\n",
    "\n",
    "    # Save the initial weights of the model\n",
    "    initial_weights = model.get_weights()\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(skf.split(front, labels)):\n",
    "        \n",
    "        print(f\"Fold {i+1}/{folds}\")\n",
    "        \n",
    "        # Split the data into training and validation sets\n",
    "        train_front, val_front = front[train_idx], front[val_idx]\n",
    "        train_L90, val_L90 = L90[train_idx], L90[val_idx]\n",
    "        train_R90, val_R90 = R90[train_idx], R90[val_idx]\n",
    "        train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
    "\n",
    "        rate_train = sum(train_labels==0)/sum(train_labels)\n",
    "\n",
    "        # Stack views along the channel axis\n",
    "        train_images = [train_front, train_L90, train_R90]\n",
    "        val_images = [val_front, val_L90, val_R90]\n",
    "\n",
    "        # Reset the model to initial weights\n",
    "        model.set_weights(initial_weights)\n",
    "\n",
    "        # Train the model\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "        history = model.fit(\n",
    "            train_images, train_labels,\n",
    "            class_weight={0: 1, 1: rate_train},\n",
    "            validation_data=(val_images, val_labels),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[NaNCheckCallback()],\n",
    "            verbose=0  # Suppress training output\n",
    "        )\n",
    "\n",
    "        # Record the best validation AUC for this fold\n",
    "        best_auc = max(history.history['val_AUC'])\n",
    "        fold_metrics.append(best_auc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Clear TensorFlow session to release GPU memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    # Return the negative mean AUC across all folds as the loss\n",
    "    print(f'Max test ROC AUC in each fold: {fold_metrics}')\n",
    "    mean_auc = np.mean(fold_metrics)\n",
    "    return -mean_auc  # Maximize AUC by minimizing its negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f93adb9-fb9f-4fe6-bf89-9342ce769ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter search space\n",
    "space = {\n",
    "    'lr': hp.loguniform('lr', np.log(1e-5), np.log(1e-2))\n",
    "}\n",
    "\n",
    "# Create trials object to store optimization results\n",
    "trials = Trials()\n",
    "\n",
    "# Run optimization\n",
    "ti = time.time()\n",
    "best_multiInput_segmented = fmin(\n",
    "    fn=objective_multiInput_segmented,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=max_evals,\n",
    "    trials=trials\n",
    ")\n",
    "tuningTime = time.time() - ti\n",
    "\n",
    "hours, remainder = divmod(tuningTime, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print(f'Hyperparameter tuning took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "lr = best_multiInput_segmented['lr']\n",
    "print(f'Best learning rate: {lr}')\n",
    "\n",
    "# Save the tuned learning rate\n",
    "np.save('Image_classifiers/lr_'+selected_model+'_segmented', lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39710d-25d0-4f3d-869f-96b1781d4b20",
   "metadata": {},
   "source": [
    "## Train N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5fde67-b775-43de-850b-3bfab52cabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read tuned hyperparameters\n",
    "# Model architecture\n",
    "with open(\"Image_classifiers/Parameters_multiInputCNN.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "params.pop('lr')\n",
    "\n",
    "# Convert lists back to tuples if needed\n",
    "params = {\n",
    "    key: (tuple(value) if isinstance(value, list) else value)\n",
    "    for key, value in params.items()\n",
    "}\n",
    "\n",
    "# learning rate\n",
    "lr = np.load('Image_classifiers/lr_'+selected_model+'_segmented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef0205-50ec-45dc-b994-df1dad4a6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_results = pd.read_csv('Image_classifiers/Results_'+str(N)+'trials.csv', index_col = 0).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5b19b-8b04-4dc9-80c1-1b593d34d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedShuffleSplit(n_splits=N, test_size=int(round(0.15*len(labels))), random_state = seed)\n",
    "\n",
    "#best_test_auc = 0\n",
    "for trial, (train_index, test_index) in enumerate(splitter.split(front, labels)):\n",
    "\n",
    "    print(f'Trial {trial + 1}'), print()\n",
    "\n",
    "    ## Split the dataset\n",
    "    train_front = front[train_index]\n",
    "    train_L90 = L90[train_index]\n",
    "    train_R90 = R90[train_index]\n",
    "    train_labels = labels[train_index]\n",
    "    \n",
    "    test_front = front[test_index]\n",
    "    test_L90 = L90[test_index]\n",
    "    test_R90 = R90[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "\n",
    "    ## Segment frontal images\n",
    "    train_front = apply_segmentation(train_front, segmentation_model)\n",
    "    test_front = apply_segmentation(test_front, segmentation_model)\n",
    "    \n",
    "    ## Remove background\n",
    "    train_front = np.multiply(train_front, train_front > background_threshold)\n",
    "    train_L90 = np.multiply(train_L90, train_L90 > background_threshold)\n",
    "    train_R90 = np.multiply(train_R90, train_R90 > background_threshold)\n",
    "\n",
    "    test_front = np.multiply(test_front, test_front > background_threshold)\n",
    "    test_L90 = np.multiply(test_L90, test_L90 > background_threshold)\n",
    "    test_R90 = np.multiply(test_R90, test_R90 > background_threshold)\n",
    "\n",
    "    ## Add the channel dimension\n",
    "    train_front = np.expand_dims(train_front, -1)\n",
    "    train_L90 = np.expand_dims(train_L90, -1)\n",
    "    train_R90 = np.expand_dims(train_R90, -1)\n",
    "\n",
    "    test_front = np.expand_dims(test_front, -1)\n",
    "    test_L90 = np.expand_dims(test_L90, -1)\n",
    "    test_R90 = np.expand_dims(test_R90, -1)\n",
    "\n",
    "    ## Compute the class weight difference\n",
    "    rate_train = sum(train_labels == 0) / sum(train_labels)\n",
    "\n",
    "    ## Initialize and compile the model\n",
    "    model = create_multiInput_model(params, h, w)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'AUC', 'TruePositives', 'FalseNegatives', 'TrueNegatives', 'FalsePositives', 'Precision', 'Recall', Specificity(), WeightedError(rate=20)], \n",
    "                  optimizer=SGD(learning_rate=lr, clipvalue=1.0))\n",
    "    \n",
    "    ## Train the model\n",
    "    ti = time.time()\n",
    "    history = model.fit([train_front, train_L90, train_R90], train_labels, class_weight = {0: 1, 1: rate_train}, \n",
    "                        validation_data = ([test_front, test_L90, test_R90], test_labels)\n",
    "                        batch_size=batch_size, epochs=epochs, verbose=0, callbacks=[NaNCheckCallback()]) \n",
    "    trainTime = time.time() - ti\n",
    "\n",
    "    hours, remainder = divmod(trainTime, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f'Training took {hours} hours, {minutes} minutes, and {seconds} seconds.')\n",
    "\n",
    "    # Save learning curves\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(range(epochs), history.history['loss'], label='Train Loss')\n",
    "    plt.plot(range(epochs), history.history['val_loss'], label='Test Loss')\n",
    "    plt.legend(loc='upper right',fontsize=10)\n",
    "    plt.title('Train and Test Loss',fontsize=12)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(epochs), history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(range(epochs), history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right',fontsize=10)\n",
    "    plt.title('Train and Test Accuracy',fontsize=12)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(range(epochs), history.history['AUC'], label='Train AUC')\n",
    "    plt.plot(range(epochs), history.history['val_AUC'], label='Test AUC')\n",
    "    plt.legend(loc='lower right',fontsize=10)\n",
    "    plt.title('Train and Test ROC AUC',fontsize=12)\n",
    "    \n",
    "    plt.savefig('Image_classifiers/Learning_curves/MultiInputCNN_segmented_'+str(trial+1)+'.png'), plt.show()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the model\n",
    "    model.save('Image_classifiers/Models/MultiInputCNN_segmented_'+str(trial+1)+'.h5')\n",
    "\n",
    "    # Print the number of parameters in the model\n",
    "    total_params = model.count_params()  # total_params = np.sum([np.prod(v.shape.as_list()) for v in model.variables])\n",
    "    trainable_params = np.sum([np.prod(v.shape.as_list()) for v in model.trainable_variables])\n",
    "    print(f'Classifier has {total_params} total parameters, {trainable_params} of which are trainable.'), print()\n",
    "    \n",
    "    # Predict\n",
    "    train_preds = model.predict([train_front, train_L90, train_R90])\n",
    "    np.save('Image_classifiers/Predictions/MultiInputCNN_segmented_train_'+str(trial+1)+'.npy',train_preds)\n",
    "    test_preds = model.predict([test_front, test_L90, test_R90])\n",
    "    np.save('Image_classifiers/Predictions/MultiInputCNN_segmented_test_'+str(trial+1)+'.npy',test_preds)\n",
    "\n",
    "    # Evaluate the model\n",
    "    results_train = evaluate_model_skl(train_preds, train_labels)\n",
    "    print('TRAIN results:')\n",
    "    for metric, value in results_train.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    results_test = evaluate_model_skl(test_preds, test_labels)\n",
    "    print('TEST results:')\n",
    "    for metric, value in results_test.items():\n",
    "        print(f'{metric}: {value:.4f}' if isinstance(value, (float, int)) else f'{metric}: {value}')\n",
    "    print()\n",
    "\n",
    "    ## Store results\n",
    "    trials_results.append({**{'classifier':'multiInputCNN_segmented'}, **{'trial':trial+1}, \n",
    "                            **store_results(trainable_params, trainTime, results_train, results_test)})\n",
    "\n",
    "    # Clear TensorFlow session to release GPU memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    print(), print(100*'#'), print()\n",
    "\n",
    "pd.DataFrame(trials_results).to_csv('Image_classifiers/Results_'+str(N)+'trials.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa1562-913a-47f2-92fc-93efe0d13843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deed4ba-e3b4-46eb-8309-8cfd3d985d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
